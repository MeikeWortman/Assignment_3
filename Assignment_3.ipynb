{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36640b00",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12eb4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed packages \n",
    "from mesa import Agent, Model\n",
    "from mesa.time import SimultaneousActivation\n",
    "from mesa.space import NetworkGrid\n",
    "from mesa.datacollection import DataCollector\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Callable, Iterable, List, Dict, Optional, Tuple\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c241833b",
   "metadata": {},
   "source": [
    "# Strategy update rules #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de309ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_strategy_logit(agent, neighbors, a_I, b, tau):\n",
    "    \"\"\"Choose strategy using logit / softmax choice.\n",
    "\n",
    "    Parameters\n",
    "    - agent: the agent choosing a strategy\n",
    "    - neighbors: list of neighbour agents\n",
    "    - a_I: effective coordination payoff given current infrastructure\n",
    "    - b: defection payoff\n",
    "    - tau: temperature parameter for softmax\n",
    "    \"\"\"\n",
    "    # compute expected payoffs for C and D\n",
    "    pi_C = 0.0\n",
    "    pi_D = 0.0\n",
    "    for other in neighbors:\n",
    "        s_j = other.strategy\n",
    "        if s_j == \"C\":\n",
    "            pi_C += a_I\n",
    "            pi_D += b\n",
    "        else:\n",
    "            pi_C += 0.0\n",
    "            pi_D += b\n",
    "\n",
    "    # softmax choice\n",
    "    denom = np.exp(pi_C / tau) + np.exp(pi_D / tau)\n",
    "    P_C = np.exp(pi_C / tau) / denom if denom > 0 else 0.5\n",
    "    return \"C\" if random.random() < P_C else \"D\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e943f0",
   "metadata": {},
   "source": [
    "# The Agent Class #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc0be1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EVAgent(Agent):\n",
    "    \"\"\"Single agent at a graph node.\n",
    "\n",
    "    Attributes\n",
    "    - strategy: \"C\" (adopt EV) or \"D\" (defect / ICE)\n",
    "    - payoff: accumulated payoff from interactions with neighbours\n",
    "    - next_strategy: strategy chosen for the next time step\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, unique_id, model, init_strategy=\"D\"):\n",
    "        super().__init__(unique_id, model)\n",
    "        self.strategy = init_strategy\n",
    "        self.payoff = 0.0\n",
    "        self.next_strategy = init_strategy\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Compute payoff from interactions with neighbours.\n",
    "\n",
    "        Stag Hunt payoff rules:\n",
    "        - C vs C: `a_I` (coordination enhanced by infrastructure)\n",
    "        - C vs D: 0\n",
    "        - D vs C: `b`\n",
    "        - D vs D: `b`\n",
    "        \"\"\"\n",
    "        I = self.model.infrastructure\n",
    "        a0 = self.model.a0\n",
    "        beta_I = self.model.beta_I\n",
    "        b = self.model.b\n",
    "        a_I = a0 + beta_I * I\n",
    "\n",
    "        neighbor_agents = []\n",
    "        for nbr in self.model.G.neighbors(self.pos):\n",
    "            neighbor_agents.extend(self.model.grid.get_cell_list_contents([nbr]))\n",
    "        if not neighbor_agents:\n",
    "            self.payoff = 0.0\n",
    "            return\n",
    "\n",
    "        payoff = 0.0\n",
    "        for other in neighbor_agents:\n",
    "            s_i = self.strategy\n",
    "            s_j = other.strategy\n",
    "            if s_i == \"C\" and s_j == \"C\":\n",
    "                payoff += a_I\n",
    "            elif s_i == \"C\" and s_j == \"D\":\n",
    "                payoff += 0.0\n",
    "            elif s_i == \"D\" and s_j == \"C\":\n",
    "                payoff += b\n",
    "            else:\n",
    "                payoff += b\n",
    "        self.payoff = payoff\n",
    "\n",
    "    ####################################\n",
    "    # Advance method\n",
    "    #\n",
    "    # The advance method updates the agent's strategy based on the selected rule.\n",
    "    #\n",
    "    # Parameters\n",
    "    # - strategy_choice_func: the strategy selection function to use (\"logit\")\n",
    "    ####################################\n",
    "    def advance(self, strategy_choice_func=\"logit\"):\n",
    "        \"\"\"Update next_strategy using the selected rule.\n",
    "\n",
    "        If called without an explicit rule, read `self.model.strategy_choice_func`.\n",
    "        Commit `self.strategy = self.next_strategy` for synchronous updates.\n",
    "        \"\"\"\n",
    "        # Default to logit, and treat any value (\"imitate\" or \"logit\") as logit.\n",
    "        func = strategy_choice_func if strategy_choice_func is not None else getattr(\n",
    "            self.model,\n",
    "            \"strategy_choice_func\",\n",
    "            \"logit\"\n",
    "        )\n",
    "\n",
    "        neighbor_agents = []\n",
    "        for nbr in self.model.G.neighbors(self.pos):\n",
    "            neighbor_agents.extend(self.model.grid.get_cell_list_contents([nbr]))\n",
    "\n",
    "        if func in (\"imitate\", \"logit\"):\n",
    "            # Always use logit as the actual update rule\n",
    "            a_I = self.model.a0 + self.model.beta_I * self.model.infrastructure\n",
    "            self.next_strategy = choose_strategy_logit(\n",
    "                self,\n",
    "                neighbor_agents,\n",
    "                a_I,\n",
    "                self.model.b,\n",
    "                getattr(self.model, \"tau\", 1.0),\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy choice function: {func}\")\n",
    "\n",
    "        # Commit the update for SimultaneousActivation\n",
    "        self.strategy = self.next_strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa036705",
   "metadata": {},
   "source": [
    "# EV Stag Hunt Model Implementation # \n",
    "with Different Network Types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bf2d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EVStagHuntModel(Model):\n",
    "    \"\"\"Mesa model for EV Stag Hunt on a network.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        initial_ev=10,\n",
    "        a0=2.0,\n",
    "        beta_I=3.0,\n",
    "        b=1.0,\n",
    "        g_I=0.1,\n",
    "        I0=0.05,\n",
    "        seed=None,\n",
    "        network_type=\"random\",   # if nothing is selected, this default -> random network\n",
    "        n_nodes=100,\n",
    "        p=0.05,\n",
    "        m=2,\n",
    "        collect=True,\n",
    "        strategy_choice_func: str = \"imitate\",\n",
    "        tau: float = 1.0,\n",
    "    ):\n",
    "        super().__init__(seed=seed)\n",
    "\n",
    "        # Build graph\n",
    "        # Note: if network_type is not \"BA\" or \"SW\", an Erdős–Rényi random network is used.\n",
    "        if network_type == \"BA\":\n",
    "            # Barabási–Albert scale-free network\n",
    "            G = nx.barabasi_albert_graph(n_nodes, m, seed=seed)\n",
    "        elif network_type == \"WS\":\n",
    "            G = nx.watts_strogatz_graph(n_nodes, k=k, p=p, seed=seed)\n",
    "        else:\n",
    "            # Default: Erdős–Rényi random network\n",
    "            G = nx.erdos_renyi_graph(n_nodes, p, seed=seed)\n",
    "\n",
    "        self.G = G\n",
    "        self.grid = NetworkGrid(G)\n",
    "        self.schedule = SimultaneousActivation(self)\n",
    "\n",
    "        # parameters\n",
    "        self.a0 = a0\n",
    "        self.beta_I = beta_I\n",
    "        self.b = b\n",
    "        self.g_I = g_I\n",
    "        self.infrastructure = I0\n",
    "        self.step_count = 0\n",
    "        self.strategy_choice_func = strategy_choice_func\n",
    "        self.tau = tau\n",
    "\n",
    "        # initialize node attribute for agent reference\n",
    "        for n in self.G.nodes:\n",
    "            self.G.nodes[n][\"agent\"] = []\n",
    "\n",
    "        # choose initial EV nodes\n",
    "        total_nodes = self.G.number_of_nodes()\n",
    "        k_ev = max(0, min(initial_ev, total_nodes))\n",
    "        ev_nodes = set(self.random.sample(list(self.G.nodes), k_ev))\n",
    "\n",
    "        # create one agent per node\n",
    "        uid = 0\n",
    "        for node in self.G.nodes:\n",
    "            init_strategy = \"C\" if node in ev_nodes else \"D\"\n",
    "            agent = EVAgent(uid, self, init_strategy)\n",
    "            uid += 1\n",
    "            self.schedule.add(agent)\n",
    "            self.grid.place_agent(agent, node)\n",
    "\n",
    "        self.datacollector = None\n",
    "        if collect:\n",
    "            self.datacollector = DataCollector(\n",
    "                model_reporters={\n",
    "                    \"X\": self.get_adoption_fraction,\n",
    "                    \"I\": lambda m: m.infrastructure,\n",
    "                },\n",
    "                agent_reporters={\"strategy\": \"strategy\", \"payoff\": \"payoff\"},\n",
    "            )\n",
    "\n",
    "    def get_adoption_fraction(self):\n",
    "        agents = self.schedule.agents\n",
    "        if not agents:\n",
    "            return 0.0\n",
    "        return sum(1 for a in agents if a.strategy == \"C\") / len(agents)\n",
    "\n",
    "    #######################\n",
    "    # Model step function\n",
    "    #\n",
    "    # The step function advances the model by one time step.\n",
    "    # It first advances all agents, then computes the adoption fraction and infrastructure level.\n",
    "    # The infrastructure level is updated based on the adoption fraction and the infrastructure growth rate.\n",
    "    # The updated infrastructure level is clipped to the interval [0, 1].\n",
    "    # Finally, if data collection is enabled, the model and agent data are collected.\n",
    "    #######################\n",
    "    def step(self): \n",
    "        self.schedule.step()  # advance all agents\n",
    "        X = self.get_adoption_fraction()  # compute adoption fraction after all agents have advanced\n",
    "        I = self.infrastructure  # infrastructure level before this step\n",
    "        dI = self.g_I * (X - I)  # infrastructure growth rate, impacted by adoption fraction\n",
    "        self.infrastructure = float(min(1.0, max(0.0, I + dI)))  # clip infrastructure level to [0, 1]\n",
    "        if self.datacollector is not None:\n",
    "            self.datacollector.collect(self)  # collect data at the end of each step\n",
    "        self.step_count += 1  # increment step count after data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8360ed74",
   "metadata": {},
   "source": [
    "# Initial Adopter Selection: Random #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a1b4df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# Set initial adopters \n",
    "#\n",
    "# Parameters\n",
    "# - model: the EVStagHuntModel instance\n",
    "# - X0_frac: fraction of agents to initially choose EV adoption\n",
    "# - seed: random seed for reproducibility\n",
    "###########################\n",
    "def set_initial_adopters(model, X0_frac, method=\"random\", seed=None):\n",
    "    import numpy as np\n",
    "    rng = np.random.default_rng(seed)\n",
    "    agents = model.schedule.agents\n",
    "    n = len(agents)\n",
    "# number of adopters\n",
    "    k = int(round(X0_frac * n))\n",
    "  # reset all to D\n",
    "    for a in agents:\n",
    "        a.strategy = \"D\"\n",
    "    if k <= 0:\n",
    "        return\n",
    "# choose k random agents\n",
    "    idx = rng.choice(n, size=k, replace=False)\n",
    "    for i in idx:\n",
    "        agents[i].strategy = \"C\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21624f81",
   "metadata": {},
   "source": [
    "# Run Network #\n",
    "Runs network and returns final adoption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f2976ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Ratio sweep helpers (computation-only)\n",
    "# -----------------------------\n",
    "#########################\n",
    "#\n",
    "# Run a single network trial\n",
    "# \n",
    "# Parameters\n",
    "# - X0_frac: fraction of agents to initially choose EV adoption\n",
    "# - ratio: payoff ratio between EV and DC agents (a0 = ratio*b - beta_I*I0)\n",
    "# - I0: initial infrastructure level\n",
    "# - beta_I: cost of EV adoption relative to DC (beta_I*I0)\n",
    "# - b: payoff of EV (b)\n",
    "# - g_I: infrastructure growth rate (g_I)\n",
    "# - T: number of time steps to run\n",
    "# - network_type: type of network to generate (\"random\", \"BA\", or \"SW\")\n",
    "# - n_nodes: number of nodes in the network\n",
    "# - p: probability of edge creation in random network / rewiring prob in SW\n",
    "# - m: number of edges to attach from a new node to existing nodes in BA network /\n",
    "#      k (number of nearest neighbours) in SW network\n",
    "# - seed: random seed for reproducibility\n",
    "# - tol: tolerance for convergence check (default: 1e-3)\n",
    "# - patience: number of steps to wait for convergence (default: 30)\n",
    "def run_network_trial(\n",
    "    X0_frac: float,\n",
    "    ratio: float,\n",
    "    *,\n",
    "    I0: float = 0.05,\n",
    "    beta_I: float = 2.0,\n",
    "    b: float = 1.0,\n",
    "    g_I: float = 0.05,\n",
    "    T: int = 200,\n",
    "    network_type: str = \"random\", #choose network type here\n",
    "    n_nodes: int = 120,\n",
    "    p: float = 0.05,\n",
    "    m: int = 2,\n",
    "    k: int = 30,\n",
    "    seed: int | None = None,\n",
    "    tol: float = 1e-3,\n",
    "    patience: int = 30,\n",
    "    collect: bool = False,\n",
    "    strategy_choice_func: str = \"logit\",  \n",
    "    tau: float = 1.0,\n",
    ") -> float:\n",
    "    \"\"\"Run a single realisation and return final adoption fraction.\n",
    "\n",
    "    Preserves the intended initial payoff ratio via a0 = ratio*b - beta_I*I0.\n",
    "    Includes basic stability-based early stopping.\n",
    "    \"\"\"\n",
    "    initial_ev = int(round(X0_frac * n_nodes))\n",
    "    a0 = ratio * b - beta_I * I0\n",
    "\n",
    "    model = EVStagHuntModel(\n",
    "        initial_ev=initial_ev,\n",
    "        a0=a0,\n",
    "        beta_I=beta_I,\n",
    "        b=b,\n",
    "        g_I=g_I,\n",
    "        I0=I0,\n",
    "        seed=seed,\n",
    "        network_type=network_type,\n",
    "        n_nodes=n_nodes,\n",
    "        p=p,\n",
    "        m=m,\n",
    "        collect=collect,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "\n",
    "    stable_steps = 0\n",
    "    prev_X = None\n",
    "    prev_I = None\n",
    "    for _ in range(T):\n",
    "        model.step()\n",
    "        X = model.get_adoption_fraction()\n",
    "        I = model.infrastructure\n",
    "        if prev_X is not None and prev_I is not None:\n",
    "            if abs(X - prev_X) < tol and abs(I - prev_I) < tol:\n",
    "                stable_steps += 1\n",
    "            else:\n",
    "                stable_steps = 0\n",
    "        prev_X, prev_I = X, I\n",
    "        if X in (0.0, 1.0) and stable_steps >= 10:\n",
    "            break\n",
    "        if stable_steps >= patience:\n",
    "            break\n",
    "\n",
    "    return model.get_adoption_fraction()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54d3f70",
   "metadata": {},
   "source": [
    "# Final adoption vs payoff ratio #\n",
    "This function runs multiple simulations for each payoff ratio, computes the final EV adoption in each run, and returns the mean adoption level for every ratio. This allows us to estimate how EV uptake depends on incentive strength and to detect tipping points in adoption dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a50463ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "# Compute final mean adoption fraction vs ratio\n",
    "# \n",
    "##########################\n",
    "def final_mean_adoption_vs_ratio(\n",
    "    X0_frac: float,\n",
    "    ratio_values: Iterable[float],\n",
    "    *,\n",
    "    I0: float = 0.05,\n",
    "    beta_I: float = 2.0,\n",
    "    b: float = 1.0,\n",
    "    g_I: float = 0.05,\n",
    "    T: int = 200,\n",
    "    network_type: str = \"random\",  # \"random\", \"BA\", or \"WS\"\n",
    "    n_nodes: int = 120,\n",
    "    p: float = 0.05,\n",
    "    m: int = 2,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    strategy_choice_func: str = \"logit\",  # default changed from \"imitate\" to \"logit\"\n",
    "    tau: float = 1.0,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute mean final adoption across a sweep of ratio values.\n",
    "\n",
    "    For each ratio, average over `batch_size` trials with jittered `I0` and seeds.\n",
    "    Returns a numpy array of means aligned with `ratio_values` order.\n",
    "    \"\"\"\n",
    "    ratios = list(ratio_values)\n",
    "    means: List[float] = []\n",
    "    for ratio in ratios:\n",
    "        finals: List[float] = []\n",
    "        for _ in range(batch_size):\n",
    "            I0_j = float(np.clip(np.random.normal(loc=I0, scale=init_noise_I), 0.0, 1.0))\n",
    "            seed_j = np.random.randint(0, 2**31 - 1)\n",
    "            x_star = run_network_trial(\n",
    "                X0_frac,\n",
    "                ratio,\n",
    "                I0=I0_j,\n",
    "                beta_I=beta_I,\n",
    "                b=b,\n",
    "                g_I=g_I,\n",
    "                T=T,\n",
    "                network_type=network_type,\n",
    "                n_nodes=n_nodes,\n",
    "                p=p,\n",
    "                m=m,\n",
    "                k=k,\n",
    "                seed=seed_j,\n",
    "                collect=False,\n",
    "                strategy_choice_func=strategy_choice_func,\n",
    "                tau=tau,\n",
    "            )\n",
    "            finals.append(x_star)\n",
    "        means.append(float(np.mean(finals)))\n",
    "    return np.asarray(means, dtype=float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a161f5",
   "metadata": {},
   "source": [
    "# Generating Heatmap Rows #\n",
    "Adoption vs Initial Conditions for a Fixed Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaebe8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "# Compute final mean adoption fraction vs ratio\n",
    "# \n",
    "##########################\n",
    "def final_mean_adoption_vs_ratio(\n",
    "    X0_frac: float,\n",
    "    ratio_values: Iterable[float],\n",
    "    *,\n",
    "    I0: float = 0.05,\n",
    "    beta_I: float = 2.0,\n",
    "    b: float = 1.0,\n",
    "    g_I: float = 0.05,\n",
    "    T: int = 200,\n",
    "    network_type: str = \"random\",   # supports \"random\", \"BA\", \"SW\"\n",
    "    n_nodes: int = 120,\n",
    "    p: float = 0.05,\n",
    "    m: int = 2,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    strategy_choice_func: str = \"logit\",   # <-- changed from \"imitate\"\n",
    "    tau: float = 1.0,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute mean final EV adoption across a sweep of payoff ratios.\n",
    "\n",
    "    For each ratio in ratio_values:\n",
    "        - Run `batch_size` independent simulations\n",
    "        - Each simulation randomly perturbs initial infrastructure I0 (noise)\n",
    "        - Each uses a random seed for agent decisions and network processes\n",
    "        - Record the final adoption fraction\n",
    "    Return a numpy array of mean adoption values (same ordering as ratio_values).\n",
    "    \"\"\"\n",
    "    ratios = list(ratio_values)\n",
    "    means: List[float] = []\n",
    "\n",
    "    for ratio in ratios:\n",
    "        finals: List[float] = []\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            # Add small noise to initial infrastructure for robustness\n",
    "            I0_j = float(np.clip(np.random.normal(loc=I0, scale=init_noise_I), 0.0, 1.0))\n",
    "\n",
    "            # Fresh random seed for each run\n",
    "            seed_j = np.random.randint(0, 2**31 - 1)\n",
    "\n",
    "            # Run a single simulation\n",
    "            x_star = run_network_trial(\n",
    "                X0_frac,\n",
    "                ratio,\n",
    "                I0=I0_j,\n",
    "                beta_I=beta_I,\n",
    "                b=b,\n",
    "                g_I=g_I,\n",
    "                T=T,\n",
    "                network_type=network_type,\n",
    "                n_nodes=n_nodes,\n",
    "                p=p,\n",
    "                m=m,\n",
    "                k=k,\n",
    "                seed=seed_j,\n",
    "                collect=False,\n",
    "                strategy_choice_func=strategy_choice_func,\n",
    "                tau=tau,\n",
    "            )\n",
    "            finals.append(x_star)\n",
    "\n",
    "        # Average final adoption across runs\n",
    "        means.append(float(np.mean(finals)))\n",
    "\n",
    "    return np.asarray(means, dtype=float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899b11a7",
   "metadata": {},
   "source": [
    "# Whole Heatmap #\n",
    "Adoption vs Initial Conditions for a Fixed Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "392cd27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "# Compute heatmap matrix for phase sweep\n",
    "# \n",
    "##########################\n",
    "def phase_sweep_X0_vs_ratio(\n",
    "    X0_values: Iterable[float],\n",
    "    ratio_values: Iterable[float],\n",
    "    *,\n",
    "    I0: float = 0.05,\n",
    "    beta_I: float = 2.0,\n",
    "    b: float = 1.0,\n",
    "    g_I: float = 0.05,\n",
    "    T: int = 250,\n",
    "    network_type: str = \"BA\",   # \"random\", \"BA\", or \"SW\"\n",
    "    n_nodes: int = 120,\n",
    "    p: float = 0.05,\n",
    "    m: int = 2,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    "    max_workers: int | None = None,\n",
    "    backend: str = \"process\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute a heatmap matrix of mean final adoption X* over (X0, ratio).\n",
    "\n",
    "    Returns an array of shape (len(ratio_values), len(X0_values)) aligned with\n",
    "    the provided orders. Rows correspond to ratios; columns to X0 values.\n",
    "    \"\"\"\n",
    "    X0_values = list(X0_values)\n",
    "    ratio_values = list(ratio_values)\n",
    "    X_final = np.zeros((len(ratio_values), len(X0_values)), dtype=float)\n",
    "\n",
    "    # Prepare tasks per ratio\n",
    "    tasks: List[Dict] = []\n",
    "    for ratio in ratio_values:\n",
    "        tasks.append({\n",
    "            \"ratio\": ratio,\n",
    "            \"X0_values\": X0_values,\n",
    "            \"I0\": I0,\n",
    "            \"beta_I\": beta_I,\n",
    "            \"b\": b,\n",
    "            \"g_I\": g_I,\n",
    "            \"T\": T,\n",
    "            \"network_type\": network_type,\n",
    "            \"n_nodes\": n_nodes,\n",
    "            \"p\": p,\n",
    "            \"m\": m,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"init_noise_I\": init_noise_I,\n",
    "            \"strategy_choice_func\": strategy_choice_func,\n",
    "            \"tau\": tau,\n",
    "        })\n",
    "\n",
    "    if max_workers is None:\n",
    "        try:\n",
    "            max_workers = os.cpu_count() or 1\n",
    "        except Exception:\n",
    "            max_workers = 1\n",
    "\n",
    "    Executor = ProcessPoolExecutor if backend == \"process\" and max_workers > 1 else ThreadPoolExecutor\n",
    "\n",
    "    if max_workers > 1:\n",
    "        with Executor(max_workers=max_workers) as ex:\n",
    "            futures = [ex.submit(_row_for_ratio_task, args) for args in tasks]\n",
    "            for i, fut in enumerate(futures):\n",
    "                row = fut.result()\n",
    "                X_final[i, :] = row\n",
    "    else:\n",
    "        for i, args in enumerate(tasks):\n",
    "            row = _row_for_ratio_task(args)\n",
    "            X_final[i, :] = row\n",
    "\n",
    "    return X_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc1c5f8",
   "metadata": {},
   "source": [
    "## Plotting Functions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c5a865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b644be7f",
   "metadata": {},
   "source": [
    "Helper: Automatic File Path for Saving Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e75a748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _default_plot_path(filename: str) -> str:\n",
    "    plots_dir = os.path.join(os.getcwd(), \"plots\")\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    return os.path.join(plots_dir, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aea6f74",
   "metadata": {},
   "source": [
    "Fan chart plot for baseline vs policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8aa6d476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fanchart(traces_df: pd.DataFrame, out_path: Optional[str] = None) -> str:\n",
    "    \"\"\"Plot fan charts (quantile bands) for baseline vs subsidy using traces DF.\n",
    "\n",
    "    traces_df columns: ['group', 'trial', 'time', 'X'] where group in {'baseline','subsidy'}.\n",
    "    \"\"\"\n",
    "    if traces_df.empty:\n",
    "        raise ValueError(\"traces_df is empty\")\n",
    "\n",
    "    groups = [\"baseline\", \"subsidy\"]\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(11, 8), constrained_layout=True)\n",
    "\n",
    "    for j, group in enumerate(groups):\n",
    "        gdf = traces_df[traces_df[\"group\"] == group]\n",
    "\n",
    "        # Compute quantiles by time across trials\n",
    "        q = gdf.groupby(\"time\")[\"X\"].quantile([0.10, 0.25, 0.75, 0.90]).unstack(level=1)\n",
    "        mean = gdf.groupby(\"time\")[\"X\"].mean()\n",
    "        t = mean.index.to_numpy()\n",
    "\n",
    "        ax = axes[0, j]\n",
    "        ax.fill_between(t, q[0.10], q[0.90], color=(\"steelblue\" if group == \"baseline\" else \"darkorange\"), alpha=0.15, label=\"10–90%\")\n",
    "        ax.fill_between(t, q[0.25], q[0.75], color=(\"steelblue\" if group == \"baseline\" else \"darkorange\"), alpha=0.30, label=\"25–75%\")\n",
    "\n",
    "        # Overlay some traces for context (sample up to 100 trials)\n",
    "        trial_ids = gdf[\"trial\"].unique()\n",
    "        rng = np.random.default_rng(123)\n",
    "        sample = rng.choice(trial_ids, size=min(100, len(trial_ids)), replace=False)\n",
    "        for tr in sample:\n",
    "            tr_df = gdf[gdf[\"trial\"] == tr]\n",
    "            ax.plot(tr_df[\"time\"], tr_df[\"X\"], color=(\"steelblue\" if group == \"baseline\" else \"darkorange\"), alpha=0.1, linewidth=0.8)\n",
    "\n",
    "        ax.plot(t, mean, color=(\"steelblue\" if group == \"baseline\" else \"darkorange\"), linewidth=2, label=\"mean\")\n",
    "        ax.set_title(f\"{group.capitalize()} adoption\")\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.set_ylabel(\"X(t)\")\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.legend(loc=\"lower right\")\n",
    "\n",
    "        # Final X(T) histogram\n",
    "        t_max = int(gdf[\"time\"].max())\n",
    "        final_vals = gdf[gdf[\"time\"] == t_max].groupby(\"trial\")[\"X\"].mean().to_numpy()\n",
    "        axes[1, j].hist(final_vals, bins=20, color=(\"steelblue\" if group == \"baseline\" else \"darkorange\"), alpha=0.8)\n",
    "        axes[1, j].set_title(f\"{group.capitalize()} final X(T)\")\n",
    "        axes[1, j].set_xlabel(\"X(T)\")\n",
    "        axes[1, j].set_ylabel(\"Count\")\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = _default_plot_path(\"ev_intervention_fanchart.png\")\n",
    "    fig.savefig(out_path, dpi=140)\n",
    "    plt.close(fig)\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78be906b",
   "metadata": {},
   "source": [
    "Spaghetti Plot of Individual Adoption Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2af0d785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spaghetti(traces_df: pd.DataFrame, *, max_traces: int = 100, alpha: float = 0.15, out_path: Optional[str] = None) -> str:\n",
    "    \"\"\"Spaghetti plot from traces DF for baseline vs subsidy.\"\"\"\n",
    "    groups = [\"baseline\", \"subsidy\"]\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(11, 4.5), constrained_layout=True)\n",
    "    rng = np.random.default_rng(123)\n",
    "\n",
    "    for j, group in enumerate(groups):\n",
    "        gdf = traces_df[traces_df[\"group\"] == group]\n",
    "        trial_ids = gdf[\"trial\"].unique()\n",
    "        sample = rng.choice(trial_ids, size=min(max_traces, len(trial_ids)), replace=False)\n",
    "        ax = axes[j]\n",
    "        for tr in sample:\n",
    "            tr_df = gdf[gdf[\"trial\"] == tr]\n",
    "            ax.plot(tr_df[\"time\"], tr_df[\"X\"], color=(\"steelblue\" if group == \"baseline\" else \"darkorange\"), alpha=alpha, linewidth=0.8)\n",
    "        ax.set_title(f\"{group.capitalize()} traces\")\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.set_ylabel(\"X(t)\")\n",
    "        ax.set_ylim(0, 1)\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = _default_plot_path(\"ev_spaghetti.png\")\n",
    "    fig.savefig(out_path, dpi=140)\n",
    "    plt.close(fig)\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba10de5a",
   "metadata": {},
   "source": [
    "Density Plot of Adoption Dynamics (Baseline vs Subsidy) over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f28eb039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_density(traces_df: pd.DataFrame, *, x_bins: int = 50, time_bins: Optional[int] = None, out_path: Optional[str] = None) -> str:\n",
    "    \"\"\"Time-evolving density plot (2D histogram) from traces DF.\"\"\"\n",
    "    groups = [\"baseline\", \"subsidy\"]\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4.8), constrained_layout=True)\n",
    "\n",
    "    for j, group in enumerate(groups):\n",
    "        gdf = traces_df[traces_df[\"group\"] == group]\n",
    "        T = int(gdf[\"time\"].max()) + 1\n",
    "        if time_bins is None:\n",
    "            bins_time = T\n",
    "        else:\n",
    "            bins_time = time_bins\n",
    "        hb = axes[j].hist2d(gdf[\"time\"].to_numpy(), gdf[\"X\"].to_numpy(), bins=[bins_time, x_bins], range=[[0, T - 1], [0.0, 1.0]], cmap=\"magma\")\n",
    "        axes[j].set_title(f\"{group.capitalize()} density: time vs X(t)\")\n",
    "        axes[j].set_xlabel(\"Time\")\n",
    "        axes[j].set_ylabel(\"X(t)\")\n",
    "        fig.colorbar(hb[3], ax=axes[j], label=\"count\")\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = _default_plot_path(\"ev_density.png\")\n",
    "    fig.savefig(out_path, dpi=140)\n",
    "    plt.close(fig)\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc842ca",
   "metadata": {},
   "source": [
    "Density of Adoption Trajectories Over Time (Baseline vs Subsidy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b10eb8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_density(traces_df: pd.DataFrame, *, x_bins: int = 50, time_bins: Optional[int] = None, out_path: Optional[str] = None) -> str:\n",
    "    \"\"\"Time-evolving density plot (2D histogram) from traces DF.\"\"\"\n",
    "    groups = [\"baseline\", \"subsidy\"]\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4.8), constrained_layout=True)\n",
    "\n",
    "    for j, group in enumerate(groups):\n",
    "        gdf = traces_df[traces_df[\"group\"] == group]\n",
    "        T = int(gdf[\"time\"].max()) + 1\n",
    "        if time_bins is None:\n",
    "            bins_time = T\n",
    "        else:\n",
    "            bins_time = time_bins\n",
    "        hb = axes[j].hist2d(gdf[\"time\"].to_numpy(), gdf[\"X\"].to_numpy(), bins=[bins_time, x_bins], range=[[0, T - 1], [0.0, 1.0]], cmap=\"magma\")\n",
    "        axes[j].set_title(f\"{group.capitalize()} density: time vs X(t)\")\n",
    "        axes[j].set_xlabel(\"Time\")\n",
    "        axes[j].set_ylabel(\"X(t)\")\n",
    "        fig.colorbar(hb[3], ax=axes[j], label=\"count\")\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = _default_plot_path(\"ev_density.png\")\n",
    "    fig.savefig(out_path, dpi=140)\n",
    "    plt.close(fig)\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d42f29",
   "metadata": {},
   "source": [
    "Phase Plot: Long-Run Adoption as a Function of X0 and Payoff Ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35d4909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_phase_plot(phase_df: pd.DataFrame, out_path: Optional[str] = None) -> str:\n",
    "    \"\"\"Plot heatmap from tidy DataFrame with columns ['X0','ratio','X_final'].\"\"\"\n",
    "    # Pivot to matrix for imshow\n",
    "    pivot = phase_df.pivot(index=\"ratio\", columns=\"X0\", values=\"X_final\").sort_index().sort_index(axis=1)\n",
    "    ratios = pivot.index.to_numpy()\n",
    "    X0s = pivot.columns.to_numpy()\n",
    "\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    im = plt.imshow(\n",
    "        pivot.to_numpy(),\n",
    "        origin=\"lower\",\n",
    "        extent=[X0s[0], X0s[-1], ratios[0], ratios[-1]],\n",
    "        aspect=\"auto\",\n",
    "        vmin=0.0,\n",
    "        vmax=1.0,\n",
    "        cmap=\"plasma\",\n",
    "    )\n",
    "    plt.colorbar(im, label=\"Final adopters X*\")\n",
    "    plt.xlabel(\"X0 (initial adoption)\")\n",
    "    plt.ylabel(\"a_I / b (initial payoff ratio)\")\n",
    "    plt.title(\"Network phase plot: X* over X0 and a_I/b\")\n",
    "\n",
    "    # Overlay threshold X = 1/ratio\n",
    "    X_thresh = 1.0 / ratios\n",
    "    X_thresh_clipped = np.clip(X_thresh, 0.0, 1.0)\n",
    "    plt.plot(X_thresh_clipped, ratios, color=\"white\", linestyle=\"--\", linewidth=1.5, label=\"X = b / a_I (initial)\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = _default_plot_path(\"ev_phase_plot.png\")\n",
    "    plt.savefig(out_path, dpi=140, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0828da",
   "metadata": {},
   "source": [
    "# Subsidy Policy #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6df794f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Experiment utilities for the EV Stag Hunt model.\n",
    "\n",
    "Contains policy factory functions, trial runners, multiprocessing helpers,\n",
    "and standalone plotting routines. Depends on `ev_core` for the model.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from typing import Callable, Dict, Optional, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "\n",
    "# -----------------------------\n",
    "# Policy factories (subsidy)\n",
    "# -----------------------------\n",
    "\n",
    "def policy_subsidy_factory(start: int, end: int, delta_a0: float = 0.3, delta_beta_I: float = 0.0) -> Callable:\n",
    "    \"\"\"Create a policy that temporarily boosts coordination payoffs.\n",
    "\n",
    "    Raises `a0` and/or `beta_I` during `[start, end)` and reverts after.\n",
    "    Returns a closure `policy(model, step)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def policy(model, step):\n",
    "        if not hasattr(policy, \"base_a0\"):\n",
    "            policy.base_a0 = model.a0\n",
    "        if not hasattr(policy, \"base_beta_I\"):\n",
    "            policy.base_beta_I = model.beta_I\n",
    "\n",
    "        if start <= step < end:\n",
    "            model.a0 = policy.base_a0 + delta_a0\n",
    "            model.beta_I = policy.base_beta_I + delta_beta_I\n",
    "        else:\n",
    "            model.a0 = policy.base_a0\n",
    "            model.beta_I = policy.base_beta_I\n",
    "\n",
    "    return policy\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Trial runner\n",
    "# -----------------------------\n",
    "\n",
    "def run_timeseries_trial(\n",
    "    T: int = 200,\n",
    "    scenario_kwargs: Optional[Dict] = None,\n",
    "    seed: Optional[int] = None,\n",
    "    policy: Optional[Callable] = None,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    ") -> Tuple[np.ndarray, np.ndarray, pd.DataFrame]:\n",
    "    \"\"\"Run one simulation; return X(t), I(t), and model vars dataframe.\"\"\"\n",
    "\n",
    "    scenario = {\n",
    "        \"a0\": 2.0,\n",
    "        \"ratio\": None,\n",
    "        \"beta_I\": 3.0,\n",
    "        \"b\": 1.0,\n",
    "        \"g_I\": 0.1,\n",
    "        \"I0\": 0.05,\n",
    "        \"network_type\": \"random\",\n",
    "        \"n_nodes\": 100,\n",
    "        \"p\": 0.05,\n",
    "        \"m\": 2,\n",
    "        \"collect\": True,\n",
    "        \"X0_frac\": 0.0,\n",
    "        \"init_method\": \"random\",\n",
    "    }\n",
    "    if scenario_kwargs:\n",
    "        scenario.update(scenario_kwargs)\n",
    "\n",
    "    # Compute a0 from ratio if provided\n",
    "    a0_for_model = scenario[\"a0\"]\n",
    "    if scenario.get(\"ratio\") is not None:\n",
    "        a0_for_model = (\n",
    "            float(scenario[\"ratio\"]) * float(scenario[\"b\"])\n",
    "            - float(scenario[\"beta_I\"]) * float(scenario[\"I0\"])\n",
    "        )\n",
    "    model = EVStagHuntModel(\n",
    "        initial_ev=0,                     \n",
    "        a0=a0_for_model,\n",
    "        beta_I=scenario[\"beta_I\"],\n",
    "        b=scenario[\"b\"],\n",
    "        g_I=scenario[\"g_I\"],\n",
    "        I0=scenario[\"I0\"],\n",
    "        seed=seed,\n",
    "        network_type=scenario[\"network_type\"],\n",
    "        n_nodes=scenario[\"n_nodes\"],\n",
    "        p=scenario[\"p\"],\n",
    "        m=scenario[\"m\"],\n",
    "        collect=True,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "\n",
    "    # Set initial adopters AFTER model creation\n",
    "    if scenario.get(\"X0_frac\", 0.0) > 0.0:\n",
    "        set_initial_adopters(\n",
    "            model,\n",
    "            scenario[\"X0_frac\"],\n",
    "            method=scenario.get(\"init_method\", \"random\"),\n",
    "            seed=seed,\n",
    "        )\n",
    "\n",
    "    for t in range(T):\n",
    "        if policy is not None:\n",
    "            policy(model, t)\n",
    "        model.step()\n",
    "\n",
    "    df = model.datacollector.get_model_vars_dataframe().copy()\n",
    "    return df[\"X\"].to_numpy(), df[\"I\"].to_numpy(), df\n",
    "\n",
    "\n",
    "def _timeseries_trial_worker(args_dict: Dict) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Worker for parallel trials that reconstructs closures for policies.\"\"\"\n",
    "    T = args_dict[\"T\"]\n",
    "    scenario_kwargs = args_dict.get(\"scenario_kwargs\", {})\n",
    "    seed = args_dict.get(\"seed\", None)\n",
    "    policy_spec = args_dict.get(\"policy\", None)\n",
    "    strategy_choice_func = args_dict.get(\"strategy_choice_func\", \"logit\")\n",
    "    tau = args_dict.get(\"tau\", 1.0)\n",
    "\n",
    "    policy = None\n",
    "    if isinstance(policy_spec, dict):\n",
    "        ptype = policy_spec.get(\"type\")\n",
    "        if ptype == \"subsidy\":\n",
    "            policy = policy_subsidy_factory(**policy_spec[\"params\"])\n",
    "        # no infrastructure policy anymore\n",
    "\n",
    "    X, I, _df = run_timeseries_trial(\n",
    "        T=T,\n",
    "        scenario_kwargs=scenario_kwargs,\n",
    "        seed=seed,\n",
    "        policy=policy,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "    return X, I\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Experiment: Intervention trials + plotting\n",
    "# -----------------------------\n",
    "\n",
    "def collect_intervention_trials(\n",
    "    n_trials: int = 10,\n",
    "    T: int = 200,\n",
    "    scenario_kwargs: Optional[Dict] = None,\n",
    "    subsidy_params: Optional[Dict] = None,\n",
    "    max_workers: int = 1,\n",
    "    seed_base: int = 42,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    ") -> Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray], pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Run baseline and subsidy trials; return raw trajectories and summary dataframes.\"\"\"\n",
    "\n",
    "    scenario = scenario_kwargs or {}\n",
    "    subsidy = subsidy_params or {\"start\": 30, \"end\": 80, \"delta_a0\": 0.3, \"delta_beta_I\": 0.0}\n",
    "\n",
    "    baseline_args = []\n",
    "    subsidy_args = []\n",
    "    for i in range(n_trials):\n",
    "        seed = seed_base + i\n",
    "        baseline_args.append(\n",
    "            {\n",
    "                \"T\": T,\n",
    "                \"scenario_kwargs\": scenario,\n",
    "                \"seed\": seed,\n",
    "                \"policy\": None,\n",
    "                \"strategy_choice_func\": strategy_choice_func,\n",
    "                \"tau\": tau,\n",
    "            }\n",
    "        )\n",
    "        subsidy_args.append(\n",
    "            {\n",
    "                \"T\": T,\n",
    "                \"scenario_kwargs\": scenario,\n",
    "                \"seed\": seed,\n",
    "                \"policy\": {\"type\": \"subsidy\", \"params\": subsidy},\n",
    "                \"strategy_choice_func\": strategy_choice_func,\n",
    "                \"tau\": tau,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    baseline_X, baseline_I = [], []\n",
    "    subsidy_X, subsidy_I = [], []\n",
    "\n",
    "    # Run sequentially or concurrently\n",
    "    Executor = ThreadPoolExecutor if max_workers == 1 else ProcessPoolExecutor\n",
    "    with Executor(max_workers=max_workers) as ex:\n",
    "        baseline_futs = [ex.submit(_timeseries_trial_worker, args) for args in baseline_args]\n",
    "        subsidy_futs = [ex.submit(_timeseries_trial_worker, args) for args in subsidy_args]\n",
    "        for fut in as_completed(baseline_futs):\n",
    "            X, I = fut.result()\n",
    "            baseline_X.append(X)\n",
    "            baseline_I.append(I)\n",
    "        for fut in as_completed(subsidy_futs):\n",
    "            X, I = fut.result()\n",
    "            subsidy_X.append(X)\n",
    "            subsidy_I.append(I)\n",
    "\n",
    "    # Align order by seed (as_completed may scramble)\n",
    "    baseline_X = sorted(baseline_X, key=lambda arr: tuple(arr))\n",
    "    subsidy_X = sorted(subsidy_X, key=lambda arr: tuple(arr))\n",
    "\n",
    "    # Summary stats\n",
    "    def summarize(X_list: List[np.ndarray]) -> pd.DataFrame:\n",
    "        mat = np.vstack(X_list)\n",
    "        df = pd.DataFrame({\n",
    "            \"X_mean\": mat.mean(axis=0),\n",
    "            \"X_med\": np.median(mat, axis=0),\n",
    "            \"X_q10\": np.quantile(mat, 0.10, axis=0),\n",
    "            \"X_q25\": np.quantile(mat, 0.25, axis=0),\n",
    "            \"X_q75\": np.quantile(mat, 0.75, axis=0),\n",
    "            \"X_q90\": np.quantile(mat, 0.90, axis=0),\n",
    "        })\n",
    "        return df\n",
    "\n",
    "    baseline_df = summarize(baseline_X)\n",
    "    subsidy_df = summarize(subsidy_X)\n",
    "\n",
    "    return baseline_X, baseline_I, subsidy_X, subsidy_I, baseline_df, subsidy_df\n",
    "\n",
    "\n",
    "def traces_to_long_df(baseline_X: List[np.ndarray], subsidy_X: List[np.ndarray]) -> pd.DataFrame:\n",
    "    \"\"\"Convert trajectory lists to a tidy DataFrame: [group, trial, time, X].\"\"\"\n",
    "    rows = []\n",
    "    for trial, X in enumerate(baseline_X):\n",
    "        for t, x in enumerate(X):\n",
    "            rows.append((\"baseline\", trial, t, float(x)))\n",
    "    for trial, X in enumerate(subsidy_X):\n",
    "        for t, x in enumerate(X):\n",
    "            rows.append((\"subsidy\", trial, t, float(x)))\n",
    "    return pd.DataFrame(rows, columns=[\"group\", \"trial\", \"time\", \"X\"])\n",
    "\n",
    "\n",
    "def ratio_sweep_df(\n",
    "    X0_frac: float = 0.40,\n",
    "    ratio_values: Optional[np.ndarray] = None,\n",
    "    scenario_kwargs: Optional[Dict] = None,\n",
    "    T: int = 250,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute X* vs ratio and return as a DataFrame.\"\"\"\n",
    "    scenario = {\n",
    "        \"beta_I\": 2.0,\n",
    "        \"b\": 1.0,\n",
    "        \"g_I\": 0.05,\n",
    "        \"I0\": 0.05,\n",
    "        \"network_type\": \"BA\",\n",
    "        \"n_nodes\": 120,\n",
    "        \"p\": 0.05,\n",
    "        \"m\": 2,\n",
    "    }\n",
    "    if scenario_kwargs:\n",
    "        scenario.update(scenario_kwargs)\n",
    "\n",
    "    if ratio_values is None:\n",
    "        ratio_values = np.linspace(0.8, 3.5, 41)\n",
    "\n",
    "    X_means = final_mean_adoption_vs_ratio(\n",
    "        X0_frac,\n",
    "        ratio_values,\n",
    "        I0=scenario[\"I0\"],\n",
    "        beta_I=scenario[\"beta_I\"],\n",
    "        b=scenario[\"b\"],\n",
    "        g_I=scenario[\"g_I\"],\n",
    "        T=T,\n",
    "        network_type=scenario[\"network_type\"],\n",
    "        n_nodes=scenario[\"n_nodes\"],\n",
    "        p=scenario[\"p\"],\n",
    "        m=scenario[\"m\"],\n",
    "        batch_size=batch_size,\n",
    "        init_noise_I=init_noise_I,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "\n",
    "    return pd.DataFrame({\"ratio\": ratio_values, \"X_mean\": X_means})\n",
    "\n",
    "\n",
    "def phase_sweep_df(\n",
    "    max_workers: int | None = None,\n",
    "    backend: str = \"process\",\n",
    "    X0_values: Optional[np.ndarray] = None,\n",
    "    ratio_values: Optional[np.ndarray] = None,\n",
    "    scenario_kwargs: Optional[Dict] = None,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    T: int = 250,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute tidy DataFrame of X* over (X0, ratio).\"\"\"\n",
    "    if X0_values is None:\n",
    "        X0_values = np.linspace(0.0, 1.0, 21)\n",
    "    if ratio_values is None:\n",
    "        ratio_values = np.linspace(0.8, 3.5, 41)\n",
    "\n",
    "    scenario = {\n",
    "        \"I0\": 0.05,\n",
    "        \"beta_I\": 2.0,\n",
    "        \"b\": 1.0,\n",
    "        \"g_I\": 0.05,\n",
    "        \"network_type\": \"BA\",\n",
    "        \"n_nodes\": 120,\n",
    "        \"p\": 0.05,\n",
    "        \"m\": 2,\n",
    "    }\n",
    "    if scenario_kwargs:\n",
    "        scenario.update(scenario_kwargs)\n",
    "\n",
    "    X_final = phase_sweep_X0_vs_ratio(\n",
    "        X0_values,\n",
    "        ratio_values,\n",
    "        I0=scenario[\"I0\"],\n",
    "        beta_I=scenario[\"beta_I\"],\n",
    "        b=scenario[\"b\"],\n",
    "        g_I=scenario[\"g_I\"],\n",
    "        T=T,\n",
    "        network_type=scenario[\"network_type\"],\n",
    "        n_nodes=scenario[\"n_nodes\"],\n",
    "        p=scenario[\"p\"],\n",
    "        m=scenario[\"m\"],\n",
    "        batch_size=batch_size,\n",
    "        init_noise_I=init_noise_I,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "        max_workers=max_workers or 1,\n",
    "        backend=backend,\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "    for i, X0 in enumerate(X0_values):\n",
    "        for j, ratio in enumerate(ratio_values):\n",
    "            rows.append((float(X0), float(ratio), float(X_final[j, i])))\n",
    "    return pd.DataFrame(rows, columns=[\"X0\", \"ratio\", \"X_final\"])\n",
    "\n",
    "\n",
    "def run_ratio_sweep_plot(\n",
    "    X0_frac: float = 0.40,\n",
    "    ratio_values: Optional[np.ndarray] = None,\n",
    "    scenario_kwargs: Optional[Dict] = None,\n",
    "    T: int = 250,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    "    out_path: Optional[str] = None,\n",
    ") -> str:\n",
    "    \"\"\"Sweep ratio values and plot final adoption X* vs a_I/b for a fixed X0.\n",
    "\n",
    "    Calls the core computation helper and saves a simple line plot.\n",
    "    Returns the path to the saved image.\n",
    "    \"\"\"\n",
    "    scenario = {\n",
    "        \"beta_I\": 2.0,\n",
    "        \"b\": 1.0,\n",
    "        \"g_I\": 0.05,\n",
    "        \"I0\": 0.05,\n",
    "        \"network_type\": \"BA\",\n",
    "        \"n_nodes\": 120,\n",
    "        \"p\": 0.05,\n",
    "        \"m\": 2,\n",
    "    }\n",
    "    if scenario_kwargs:\n",
    "        scenario.update(scenario_kwargs)\n",
    "\n",
    "    if ratio_values is None:\n",
    "        ratio_values = np.linspace(0.8, 3.5, 41)\n",
    "\n",
    "    X_means = final_mean_adoption_vs_ratio(\n",
    "        X0_frac,\n",
    "        ratio_values,\n",
    "        I0=scenario[\"I0\"],\n",
    "        beta_I=scenario[\"beta_I\"],\n",
    "        b=scenario[\"b\"],\n",
    "        g_I=scenario[\"g_I\"],\n",
    "        T=T,\n",
    "        network_type=scenario[\"network_type\"],\n",
    "        n_nodes=scenario[\"n_nodes\"],\n",
    "        p=scenario[\"p\"],\n",
    "        m=scenario[\"m\"],\n",
    "        batch_size=batch_size,\n",
    "        init_noise_I=init_noise_I,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    ax.plot(ratio_values, X_means, color=\"C0\", lw=2)\n",
    "    ax.set_xlabel(\"a_I / b (ratio)\")\n",
    "    ax.set_ylabel(\"Final adoption X*\")\n",
    "    ax.set_title(f\"X* vs ratio for X0={X0_frac:.2f}\")\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "    ax.grid(True, alpha=0.25)\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = os.path.join(os.getcwd(), \"ev_ratio_sweep.png\")\n",
    "    fig.savefig(out_path, dpi=140, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def run_phase_plot_X0_vs_ratio_network(\n",
    "    max_workers: int | None = None,\n",
    "    backend: str = \"process\",\n",
    "    X0_values: Optional[np.ndarray] = None,\n",
    "    ratio_values: Optional[np.ndarray] = None,\n",
    "    scenario_kwargs: Optional[Dict] = None,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    T: int = 250,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    "    out_path: Optional[str] = None,\n",
    ") -> str:\n",
    "    \"\"\"Produce a heatmap of X* over (X0, a_I/b) using core sweep helper.\n",
    "\n",
    "    Saves a figure similar to the original model script and returns the path.\n",
    "    \"\"\"\n",
    "    # Defaults aligned with the original phase plot\n",
    "    if X0_values is None:\n",
    "        X0_values = np.linspace(0.0, 1.0, 21)\n",
    "    if ratio_values is None:\n",
    "        ratio_values = np.linspace(0.8, 3.5, 41)\n",
    "\n",
    "    scenario = {\n",
    "        \"I0\": 0.05,\n",
    "        \"beta_I\": 2.0,\n",
    "        \"b\": 1.0,\n",
    "        \"g_I\": 0.05,\n",
    "        \"network_type\": \"BA\",\n",
    "        \"n_nodes\": 120,\n",
    "        \"p\": 0.05,\n",
    "        \"m\": 2,\n",
    "    }\n",
    "    if scenario_kwargs:\n",
    "        scenario.update(scenario_kwargs)\n",
    "\n",
    "    X_final = phase_sweep_X0_vs_ratio(\n",
    "        X0_values,\n",
    "        ratio_values,\n",
    "        I0=scenario[\"I0\"],\n",
    "        beta_I=scenario[\"beta_I\"],\n",
    "        b=scenario[\"b\"],\n",
    "        g_I=scenario[\"g_I\"],\n",
    "        T=T,\n",
    "        network_type=scenario[\"network_type\"],\n",
    "        n_nodes=scenario[\"n_nodes\"],\n",
    "        p=scenario[\"p\"],\n",
    "        m=scenario[\"m\"],\n",
    "        batch_size=batch_size,\n",
    "        init_noise_I=init_noise_I,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "        max_workers=max_workers or 1,\n",
    "        backend=backend,\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    im = plt.imshow(\n",
    "        X_final,\n",
    "        origin=\"lower\",\n",
    "        extent=[X0_values[0], X0_values[-1], ratio_values[0], ratio_values[-1]],\n",
    "        aspect=\"auto\",\n",
    "        vmin=0.0,\n",
    "        vmax=1.0,\n",
    "        cmap=\"plasma\",\n",
    "    )\n",
    "    plt.colorbar(im, label=\"Final adopters X*\")\n",
    "    plt.xlabel(\"X0 (initial adoption)\")\n",
    "    plt.ylabel(\"a_I / b (initial payoff ratio)\")\n",
    "    plt.title(\"Network phase plot: X* over X0 and a_I/b\")\n",
    "\n",
    "    # Overlay initial threshold X = b/a_I => X = 1/ratio\n",
    "    X_thresh = 1.0 / ratio_values\n",
    "    X_thresh_clipped = np.clip(X_thresh, 0.0, 1.0)\n",
    "    plt.plot(\n",
    "        X_thresh_clipped,\n",
    "        ratio_values,\n",
    "        color=\"white\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=1.5,\n",
    "        label=\"X = b / a_I (initial)\",\n",
    "    )\n",
    "\n",
    "    plt.legend(loc=\"upper right\")\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = os.path.join(os.getcwd(), \"ev_phase_plot.png\")\n",
    "    plt.savefig(out_path, dpi=140, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def run_intervention_example(\n",
    "    n_trials: int = 10,\n",
    "    T: int = 200,\n",
    "    scenario_kwargs: Optional[Dict] = None,\n",
    "    subsidy_params: Optional[Dict] = None,\n",
    "    max_workers: int = 1,\n",
    "    seed_base: int = 42,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, str]:\n",
    "    \"\"\"Convenience: collect trials, plot, and return summary + image path.\"\"\"\n",
    "\n",
    "    baseline_X, baseline_I, subsidy_X, subsidy_I, baseline_df, subsidy_df = collect_intervention_trials(\n",
    "        n_trials=n_trials,\n",
    "        T=T,\n",
    "        scenario_kwargs=scenario_kwargs,\n",
    "        subsidy_params=subsidy_params,\n",
    "        max_workers=max_workers,\n",
    "        seed_base=seed_base,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "    # Use DataFrame-based plotting to ensure outputs go to plots/\n",
    "    traces_df = traces_to_long_df(baseline_X, subsidy_X)\n",
    "    img_path = plot_fanchart(traces_df)\n",
    "    return baseline_df, subsidy_df, img_path\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# CLI Entrypoint (optional)\n",
    "# -----------------------------\n",
    "\n",
    "def main():\n",
    "    # Defaults aligned with original ev_stag_mesa_model.run_intervention_example\n",
    "    n_trials = 30  # use fewer than 500 for speed while keeping shape\n",
    "    T = 200\n",
    "    strategy_choice_func = \"logit\"\n",
    "    tau = 1.0\n",
    "    max_workers = 1\n",
    "    seed_base = 100\n",
    "\n",
    "    scenario = dict(\n",
    "        # Preserve initial ratio by computing a0 from ratio, matching the original\n",
    "        ratio=2.3,\n",
    "        beta_I=2.0,\n",
    "        b=1.0,\n",
    "        g_I=0.10,\n",
    "        I0=0.05,\n",
    "        network_type=\"BA\",\n",
    "        n_nodes=300,\n",
    "        m=2,\n",
    "        collect=True,\n",
    "        X0_frac=0.40,\n",
    "        init_method=\"random\",\n",
    "        # ER-specific `p` ignored for BA but kept for completeness\n",
    "        p=0.05,\n",
    "    )\n",
    "    subsidy = dict(start=10, end=60, delta_a0=0.4, delta_beta_I=0.0)\n",
    "\n",
    "    baseline_df, subsidy_df, img_path = run_intervention_example(\n",
    "        n_trials=n_trials,\n",
    "        T=T,\n",
    "        scenario_kwargs=scenario,\n",
    "        subsidy_params=subsidy,\n",
    "        max_workers=max_workers,\n",
    "        seed_base=seed_base,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "\n",
    "    print(\"Baseline DF shape:\", baseline_df.shape)\n",
    "    print(\"Subsidy DF shape:\", subsidy_df.shape)\n",
    "    print(\"Baseline final X_mean:\", float(baseline_df[\"X_mean\"].iloc[-1]))\n",
    "    print(\"Subsidy  final X_mean:\", float(subsidy_df[\"X_mean\"].iloc[-1]))\n",
    "\n",
    "    # Also run the phase plot of X* over (X0, a_I/b) and save it\n",
    "    phase_df = phase_sweep_df(\n",
    "        max_workers=1,\n",
    "        backend=\"thread\",\n",
    "        X0_values=np.linspace(0.0, 1.0, 21),\n",
    "        ratio_values=np.linspace(0.8, 3.5, 31),\n",
    "        batch_size=8,\n",
    "        T=200,\n",
    "        strategy_choice_func=\"logit\",\n",
    "        tau=1.0,\n",
    "    )\n",
    "    phase_path = plot_phase_plot(phase_df)\n",
    "    print(\"Saved phase plot:\", phase_path)\n",
    "\n",
    "    # Spaghetti and time-evolving density plots\n",
    "    n_trials_spaghetti = 100\n",
    "    T_spaghetti = 200\n",
    "\n",
    "    baseline_X, baseline_I, subsidy_X, subsidy_I, baseline_df2, subsidy_df2 = collect_intervention_trials(\n",
    "        n_trials=n_trials_spaghetti,\n",
    "        T=T_spaghetti,\n",
    "        scenario_kwargs=scenario,\n",
    "        subsidy_params=subsidy,\n",
    "        max_workers=max_workers,\n",
    "        seed_base=seed_base,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "    traces_df = traces_to_long_df(baseline_X, subsidy_X)\n",
    "    spaghetti_path = plot_spaghetti(traces_df, max_traces=100, alpha=0.15)\n",
    "    print(\"Saved spaghetti plot:\", spaghetti_path)\n",
    "\n",
    "    density_path = plot_density(traces_df, x_bins=50, time_bins=T_spaghetti)\n",
    "    print(\"Saved time-evolving density plot:\", density_path)\n",
    "\n",
    "    # Ratio sweep computed to DF then plotted\n",
    "    sweep_df = ratio_sweep_df(\n",
    "        X0_frac=scenario.get(\"X0_frac\", 0.40),\n",
    "        ratio_values=np.linspace(0.8, 3.5, 31),\n",
    "        scenario_kwargs=scenario,\n",
    "        T=200,\n",
    "        batch_size=8,\n",
    "        strategy_choice_func=\"logit\",\n",
    "        tau=1.0,\n",
    "    )\n",
    "    sweep_path = plot_ratio_sweep(sweep_df)\n",
    "    print(\"Saved ratio sweep plot:\", sweep_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6704a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# 1) State-based subsidy policy factory\n",
    "# -------------------------------------------------\n",
    "def policy_subsidy_state_factory(\n",
    "    *,\n",
    "    x_low: float,\n",
    "    x_high: float,\n",
    "    delta_a0: float = 0.2,\n",
    "    delta_beta_I: float = 0.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Subsidy is active only when the aggregate adoption X(t)\n",
    "    lies within a predefined interval [x_low, x_high).\n",
    "    This makes the policy state-dependent rather than time-based.\n",
    "    \"\"\"\n",
    "    def policy(model, step):\n",
    "        # Store baseline parameters once to ensure reversibility\n",
    "        if not hasattr(policy, \"base_a0\"):\n",
    "            policy.base_a0 = model.a0\n",
    "        if not hasattr(policy, \"base_beta_I\"):\n",
    "            policy.base_beta_I = model.beta_I\n",
    "\n",
    "        # Read current adoption level\n",
    "        if hasattr(model, \"get_adoption_fraction\"):\n",
    "            x = float(model.get_adoption_fraction())\n",
    "        else:\n",
    "            # Fallback: try to read last recorded X from datacollector\n",
    "            try:\n",
    "                df = model.datacollector.get_model_vars_dataframe()\n",
    "                x = float(df[\"X\"].iloc[-1])\n",
    "            except Exception:\n",
    "                x = 0.0\n",
    "\n",
    "        # Apply subsidy only inside the specified adoption window\n",
    "        if x_low <= x < x_high:\n",
    "            model.a0 = policy.base_a0 + delta_a0\n",
    "            model.beta_I = policy.base_beta_I + delta_beta_I\n",
    "        else:\n",
    "            model.a0 = policy.base_a0\n",
    "            model.beta_I = policy.base_beta_I\n",
    "\n",
    "    return policy\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2) Helper: run multiple trials with a given policy\n",
    "# -------------------------------------------------\n",
    "def collect_trials_with_custom_policy(\n",
    "    *,\n",
    "    n_trials: int,\n",
    "    T: int,\n",
    "    scenario_kwargs: dict,\n",
    "    policy_func,\n",
    "    seed_base: int = 100,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 2.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs multiple stochastic trials and returns\n",
    "    the mean adoption trajectory X(t).\n",
    "    \"\"\"\n",
    "    X_list = []\n",
    "    for i in range(n_trials):\n",
    "        seed = seed_base + i\n",
    "        X, I, _ = run_timeseries_trial(\n",
    "            T=T,\n",
    "            scenario_kwargs=scenario_kwargs,\n",
    "            seed=seed,\n",
    "            policy=policy_func,\n",
    "            strategy_choice_func=strategy_choice_func,\n",
    "            tau=tau,\n",
    "        )\n",
    "        X_list.append(X)\n",
    "\n",
    "    mat = np.vstack(X_list)\n",
    "    return mat.mean(axis=0)\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3) Scenario close to tipping (to make timing matter)\n",
    "# -------------------------------------------------\n",
    "T = 200\n",
    "n_trials = 30\n",
    "seed_base = 100\n",
    "strategy_choice_func = \"logit\"\n",
    "tau = 2.0  # smoother response to payoff differences\n",
    "\n",
    "scenario = dict(\n",
    "    ratio=1.6,          # close to tipping\n",
    "    beta_I=2.0,\n",
    "    b=1.0,\n",
    "    g_I=0.10,\n",
    "    I0=0.05,\n",
    "    network_type=\"BA\",\n",
    "    n_nodes=300,\n",
    "    m=2,\n",
    "    p=0.05,\n",
    "    X0_frac=0.10,       # lower initial adoption so baseline does not reach full adoption\n",
    "    init_method=\"random\",\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4) Define state-based subsidy windows and intensities\n",
    "# -------------------------------------------------\n",
    "# Adoption intervals are chosen so that they do not overlap\n",
    "# with the initial condition and meaningfully separate\n",
    "# early, intermediate, and late diffusion phases.\n",
    "policies = {\n",
    "    \"Early (0.10≤X<0.30)\": (0.10, 0.30),\n",
    "    \"Early–mid (0.30≤X<0.50)\": (0.30, 0.50),\n",
    "    \"Mid–late (0.50≤X<0.70)\": (0.50, 0.70),\n",
    "    \"Late (0.70≤X<0.90)\": (0.70, 0.90),\n",
    "}\n",
    "\n",
    "# Two subsidy strengths to compare timing versus intensity\n",
    "intensities = [0.15, 0.30]\n",
    "\n",
    "colors = {\n",
    "    \"Early (0.10≤X<0.30)\": \"C0\",\n",
    "    \"Early–mid (0.30≤X<0.50)\": \"C1\",\n",
    "    \"Mid–late (0.50≤X<0.70)\": \"C2\",\n",
    "    \"Late (0.70≤X<0.90)\": \"C3\",\n",
    "}\n",
    "linestyles = {0.15: \"--\", 0.30: \"-\"}\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5) Run baseline and policy scenarios\n",
    "# -------------------------------------------------\n",
    "t = np.arange(T)\n",
    "\n",
    "plt.figure(figsize=(9, 5))\n",
    "\n",
    "# Baseline: no subsidy applied\n",
    "baseline_mean = collect_trials_with_custom_policy(\n",
    "    n_trials=n_trials,\n",
    "    T=T,\n",
    "    scenario_kwargs=scenario,\n",
    "    policy_func=None,\n",
    "    seed_base=seed_base,\n",
    "    strategy_choice_func=strategy_choice_func,\n",
    "    tau=tau,\n",
    ")\n",
    "plt.plot(\n",
    "    t,\n",
    "    baseline_mean,\n",
    "    color=\"black\",\n",
    "    linewidth=2.8,\n",
    "    label=\"Baseline (no subsidy)\",\n",
    ")\n",
    "\n",
    "# State-dependent subsidy policies\n",
    "for name, (x_low, x_high) in policies.items():\n",
    "    for da0 in intensities:\n",
    "        pol = policy_subsidy_state_factory(\n",
    "            x_low=x_low,\n",
    "            x_high=x_high,\n",
    "            delta_a0=da0,\n",
    "        )\n",
    "        mean_curve = collect_trials_with_custom_policy(\n",
    "            n_trials=n_trials,\n",
    "            T=T,\n",
    "            scenario_kwargs=scenario,\n",
    "            policy_func=pol,\n",
    "            seed_base=seed_base,\n",
    "            strategy_choice_func=strategy_choice_func,\n",
    "            tau=tau,\n",
    "        )\n",
    "        plt.plot(\n",
    "            t,\n",
    "            mean_curve,\n",
    "            color=colors[name],\n",
    "            linestyle=linestyles[da0],\n",
    "            linewidth=2.0,\n",
    "            label=f\"{name}, Δa0={da0}\",\n",
    "        )\n",
    "\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Adoption X(t)\")\n",
    "plt.title(\"State-dependent subsidy applied during different adoption stages\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(alpha=0.25)\n",
    "plt.legend(ncol=2, fontsize=9)\n",
    "\n",
    "out_path = \"plots/state_based_subsidy_lines_corrected.png\"\n",
    "plt.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved to:\", out_path)\n",
    "print(\"Baseline final X:\", float(baseline_mean[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56bc463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# 0) Common settings (same as you had)\n",
    "# -------------------------------------------------\n",
    "T = 200\n",
    "n_trials = 30\n",
    "seed_base = 100\n",
    "strategy_choice_func = \"logit\"\n",
    "tau = 2.0\n",
    "\n",
    "common = dict(\n",
    "    ratio=1.6,          # close to tipping\n",
    "    beta_I=2.0,\n",
    "    b=1.0,\n",
    "    g_I=0.10,\n",
    "    I0=0.05,\n",
    "    n_nodes=300,\n",
    "    X0_frac=0.10,\n",
    "    init_method=\"random\",\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Network-specific scenarios\n",
    "# -------------------------------------------------\n",
    "scenarios = {\n",
    "    \"BA\": dict(**common, network_type=\"BA\", m=2),\n",
    "    # ER: choose p so expected degree ~ 2m = 4 (to be roughly comparable to BA with m=2)\n",
    "    # expected degree ≈ p*(N-1) => p ≈ 4/299 ≈ 0.0134\n",
    "    \"ER\": dict(**common, network_type=\"ER\", p=4/(300-1)),\n",
    "}\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Policy windows and intensities (same as you had)\n",
    "# -------------------------------------------------\n",
    "policies = {\n",
    "    \"Early (0.10≤X<0.30)\": (0.10, 0.30),\n",
    "    \"Early–mid (0.30≤X<0.50)\": (0.30, 0.50),\n",
    "    \"Mid–late (0.50≤X<0.70)\": (0.50, 0.70),\n",
    "    \"Late (0.70≤X<0.90)\": (0.70, 0.90),\n",
    "}\n",
    "intensities = [0.15, 0.30]\n",
    "\n",
    "colors = {\n",
    "    \"Early (0.10≤X<0.30)\": \"C0\",\n",
    "    \"Early–mid (0.30≤X<0.50)\": \"C1\",\n",
    "    \"Mid–late (0.50≤X<0.70)\": \"C2\",\n",
    "    \"Late (0.70≤X<0.90)\": \"C3\",\n",
    "}\n",
    "linestyles = {0.15: \"--\", 0.30: \"-\"}\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Run + plot per network\n",
    "# -------------------------------------------------\n",
    "for net_name, scenario in scenarios.items():\n",
    "    t = np.arange(T)\n",
    "    plt.figure(figsize=(9, 5))\n",
    "\n",
    "    # Baseline\n",
    "    baseline_mean = collect_trials_with_custom_policy(\n",
    "        n_trials=n_trials,\n",
    "        T=T,\n",
    "        scenario_kwargs=scenario,\n",
    "        policy_func=None,\n",
    "        seed_base=seed_base,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "    plt.plot(t, baseline_mean, color=\"black\", linewidth=2.8, label=\"Baseline (no subsidy)\")\n",
    "\n",
    "    # Policies\n",
    "    for name, (x_low, x_high) in policies.items():\n",
    "        for da0 in intensities:\n",
    "            pol = policy_subsidy_state_factory(\n",
    "                x_low=x_low,\n",
    "                x_high=x_high,\n",
    "                delta_a0=da0,\n",
    "            )\n",
    "            mean_curve = collect_trials_with_custom_policy(\n",
    "                n_trials=n_trials,\n",
    "                T=T,\n",
    "                scenario_kwargs=scenario,\n",
    "                policy_func=pol,\n",
    "                seed_base=seed_base,\n",
    "                strategy_choice_func=strategy_choice_func,\n",
    "                tau=tau,\n",
    "            )\n",
    "            plt.plot(\n",
    "                t, mean_curve,\n",
    "                color=colors[name],\n",
    "                linestyle=linestyles[da0],\n",
    "                linewidth=2.0,\n",
    "                label=f\"{name}, Δa0={da0}\",\n",
    "            )\n",
    "\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Adoption X(t)\")\n",
    "    plt.title(f\"State-dependent subsidy during adoption stages — {net_name}\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(alpha=0.25)\n",
    "    plt.legend(ncol=2, fontsize=9)\n",
    "\n",
    "    out_path = f\"plots/state_based_subsidy_{net_name}.png\"\n",
    "    plt.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"[{net_name}] Saved to: {out_path}\")\n",
    "    print(f\"[{net_name}] Baseline final X:\", float(baseline_mean[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c432e114",
   "metadata": {},
   "source": [
    "# !!! Refreshing Core Codes for the Next Section to Avoid Error Spilling Over !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02c8343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_strategy_imitate(agent, neighbors):\n",
    "    \"\"\"Choose strategy of the highest-payoff neighbour (including self).\"\"\"\n",
    "    candidates = neighbors + [agent]\n",
    "    best = max(candidates, key=lambda a: a.payoff)\n",
    "    return best.strategy\n",
    "\n",
    "def choose_strategy_logit(agent, neighbors, a_I, b, tau):\n",
    "    \"\"\"Choose strategy using logit / softmax choice.\n",
    "\n",
    "    Parameters\n",
    "    - agent: the agent choosing a strategy\n",
    "    - neighbors: list of neighbour agents\n",
    "    - a_I: effective coordination payoff given current infrastructure\n",
    "    - b: defection payoff\n",
    "    - tau: temperature parameter for softmax\n",
    "    \"\"\"\n",
    "    # compute expected payoffs for C and D \n",
    "    \"\"\" Still have to do determine C and D pi! \"\"\"\n",
    "    pi_C = 0.0\n",
    "    pi_D = 0.0\n",
    "    for other in neighbors:\n",
    "        s_j = other.strategy\n",
    "        if s_j == \"C\":\n",
    "            pi_C += a_I\n",
    "            pi_D += b\n",
    "        else:\n",
    "            pi_C += 0.0\n",
    "            pi_D += b\n",
    "\n",
    "    # softmax choice\n",
    "    denom = np.exp(pi_C / tau) + np.exp(pi_D / tau)\n",
    "    P_C = np.exp(pi_C / tau) / denom if denom > 0 else 0.5\n",
    "    return \"C\" if random.random() < P_C else \"D\"\n",
    "\n",
    "####################################\n",
    "# Agent class\n",
    "#\n",
    "# The EVAgent class implements the single agent at a graph node.\n",
    "#\n",
    "# Attributes\n",
    "# - strategy: \"C\" (adopt EV) or \"D\" (defect / ICE)\n",
    "# - payoff: accumulated payoff from interactions with neighbours\n",
    "# - next_strategy: strategy chosen for the next time step\n",
    "####################################\n",
    "class EVAgent(Agent):\n",
    "    \"\"\"Single agent at a graph node.\n",
    "\n",
    "    Attributes\n",
    "    - strategy: \"C\" (adopt EV) or \"D\" (defect / ICE)\n",
    "    - payoff: accumulated payoff from interactions with neighbours\n",
    "    - next_strategy: strategy chosen for the next time step\n",
    "    \"\"\"\n",
    "# Initial conditions, runs when a new agent is created. Each agent starts with: a strategy (C/D), zero payoff and a next strategy based on neighbours\n",
    "    def __init__(self, unique_id, model, init_strategy=\"D\"):\n",
    "        super().__init__(unique_id, model)\n",
    "        self.strategy = init_strategy\n",
    "        self.payoff = 0.0\n",
    "        self.next_strategy = init_strategy\n",
    "# Calculate payoff of current strategy \n",
    "    def step(self):\n",
    "        \"\"\"Compute payoff from interactions with neighbours.\n",
    "\n",
    "        Stag Hunt payoff rules:\n",
    "        - C vs C: `a_I` (coordination enhanced by infrastructure)\n",
    "        - C vs D: 0\n",
    "        - D vs C: `b`\n",
    "        - D vs D: `b`\n",
    "        \"\"\"\n",
    "        I = self.model.infrastructure           # Current infrastructure level\n",
    "        a0 = self.model.a0                      # Base coordination payoff for EV adoption\n",
    "        beta_I = self.model.beta_I              # Strength of infrastructure feedback\n",
    "        b = self.model.b                        # payoff from sticking with ICE\n",
    "        a_I = a0 + beta_I * I                   # effective payoff for EV–EV interaction\n",
    "\n",
    "        neighbor_agents = []\n",
    "        for nbr in self.model.G.neighbors(self.pos):\n",
    "            neighbor_agents.extend(self.model.grid.get_cell_list_contents([nbr]))\n",
    "        if not neighbor_agents:\n",
    "            self.payoff = 0.0\n",
    "            return\n",
    "\n",
    "        payoff = 0.0\n",
    "        for other in neighbor_agents:\n",
    "            s_i = self.strategy\n",
    "            s_j = other.strategy\n",
    "            if s_i == \"C\" and s_j == \"C\":\n",
    "                payoff += a_I\n",
    "            elif s_i == \"C\" and s_j == \"D\":\n",
    "                payoff += 0.0\n",
    "            elif s_i == \"D\" and s_j == \"C\":\n",
    "                payoff += b\n",
    "            else:\n",
    "                payoff += b\n",
    "        self.payoff = payoff\n",
    "\n",
    "    ####################################\n",
    "    # Advance method\n",
    "    #\n",
    "    # The advance method updates the agent's strategy based on the selected rule.\n",
    "    #\n",
    "    # Parameters\n",
    "    # - strategy_choice_func: the strategy selection function to use (\"imitate\" or \"logit\")\n",
    "    ####################################\n",
    "\n",
    "    def advance(self, strategy_choice_func=\"imitate\"):\n",
    "        \"\"\"Update next_strategy using the selected rule.\n",
    "\n",
    "        If called without an explicit rule, read `self.model.strategy_choice_func`.\n",
    "        Commit `self.strategy = self.next_strategy` for synchronous updates.\n",
    "        \"\"\"\n",
    "        func = strategy_choice_func if strategy_choice_func is not None else getattr(self.model, \"strategy_choice_func\", \"imitate\")\n",
    "\n",
    "        neighbor_agents = []\n",
    "        for nbr in self.model.G.neighbors(self.pos):\n",
    "            neighbor_agents.extend(self.model.grid.get_cell_list_contents([nbr]))\n",
    "\n",
    "        if func == \"imitate\":\n",
    "            self.next_strategy = choose_strategy_imitate(self, neighbor_agents)\n",
    "        elif func == \"logit\":\n",
    "            a_I = self.model.a0 + self.model.beta_I * self.model.infrastructure\n",
    "            self.next_strategy = choose_strategy_logit(self, neighbor_agents, a_I, self.model.b, getattr(self.model, \"tau\", 1.0))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy choice function: {func}\")\n",
    "\n",
    "        self.strategy = self.next_strategy\n",
    "\n",
    "class EVStagHuntModel(Model):\n",
    "    \"\"\"Mesa model for EV Stag Hunt on a network.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        initial_ev=10, # - initial_ev: number of initial EV nodes\n",
    "        a0=2.0, # - a0: base payoff for EV adoption\n",
    "        beta_I=3.0, # - beta_I: payoff enhancement factor for EV adoption\n",
    "        b=1.0, # - b: payoff for ICE defection\n",
    "        g_I=0.1, # - g_I: infrastructure growth rate\n",
    "        I0=0.05, # - I0: initial infrastructure level\n",
    "        seed=42,\n",
    "        network_type=\"random\",\n",
    "        n_nodes=100,\n",
    "        p=0.05,\n",
    "        m=2,\n",
    "        k=30,\n",
    "        collect=True,\n",
    "        strategy_choice_func: str = \"imitate\",\n",
    "        tau: float = 1.0,\n",
    "    ):\n",
    "        super().__init__(seed=seed)\n",
    "\n",
    "        # Build graph\n",
    "        if network_type == \"BA\":\n",
    "            G = nx.barabasi_albert_graph(n_nodes, 15, seed=seed)\n",
    "\n",
    "        elif network_type == \"ER\":\n",
    "            G = nx.erdos_renyi_graph(n_nodes, p, seed=seed)\n",
    "\n",
    "        elif network_type == \"WS\":\n",
    "            k = k if k % 2 == 0 else k + 1  # Ensure even for WS\n",
    "            G = nx.watts_strogatz_graph(n=n_nodes, k=k, p=p, seed=seed)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown network_type: {network_type}\")\n",
    "\n",
    "        self.G = G\n",
    "        self.grid = NetworkGrid(G)\n",
    "        self.schedule = SimultaneousActivation(self)\n",
    "\n",
    "        # parameters\n",
    "        self.a0 = a0\n",
    "        self.beta_I = beta_I\n",
    "        self.b = b\n",
    "        self.g_I = g_I\n",
    "        self.infrastructure = I0\n",
    "        self.step_count = 0\n",
    "        self.strategy_choice_func = strategy_choice_func\n",
    "        self.tau = tau\n",
    "\n",
    "        # initialize node attribute for agent reference\n",
    "        for n in self.G.nodes:\n",
    "            self.G.nodes[n][\"agent\"] = []\n",
    "\n",
    "        # choose initial EV nodes\n",
    "        total_nodes = self.G.number_of_nodes()\n",
    "        k_ev = max(0, min(initial_ev, total_nodes))\n",
    "        ev_nodes = set(self.random.sample(list(self.G.nodes), k_ev))\n",
    "\n",
    "        # create one agent per node\n",
    "        uid = 0\n",
    "        for node in self.G.nodes:\n",
    "            init_strategy = \"C\" if node in ev_nodes else \"D\"\n",
    "            agent = EVAgent(uid, self, init_strategy)\n",
    "            uid += 1\n",
    "            self.schedule.add(agent)\n",
    "            self.grid.place_agent(agent, node)\n",
    "\n",
    "        self.datacollector = None\n",
    "        if collect:\n",
    "            self.datacollector = DataCollector(\n",
    "                model_reporters={\n",
    "                    \"X\": self.get_adoption_fraction,\n",
    "                    \"I\": lambda m: m.infrastructure,\n",
    "                },\n",
    "                agent_reporters={\"strategy\": \"strategy\", \"payoff\": \"payoff\"},\n",
    "            )\n",
    "\n",
    "    def get_adoption_fraction(self):\n",
    "        agents = self.schedule.agents\n",
    "        if not agents:\n",
    "            return 0.0\n",
    "        return sum(1 for a in agents if a.strategy == \"C\") / len(agents)\n",
    "\n",
    "    # ####################\n",
    "    # Model step function\n",
    "    #\n",
    "    # The step function advances the model by one time step.\n",
    "    # It first advances all agents, then computes the adoption fraction and infrastructure level.\n",
    "    # The infrastructure level is updated based on the adoption fraction and the infrastructure growth rate.\n",
    "    # The updated infrastructure level is clipped to the interval [0, 1].\n",
    "    # Finally, if data collection is enabled, the model and agent data are collected.\n",
    "    #######################\n",
    "    def step(self): \n",
    "        self.schedule.step() # advance all agents\n",
    "        X = self.get_adoption_fraction() # compute adoption fraction after all agents have advanced\n",
    "        I = self.infrastructure # infrastructure level before this step\n",
    "        dI = self.g_I * (X - I) # infrastructure growth rate, impacted by adoption fraction\n",
    "        self.infrastructure = float(min(1.0, max(0.0, I + dI))) # clip infrastructure level to [0, 1]\n",
    "        if self.datacollector is not None:\n",
    "            self.datacollector.collect(self) # collect data at the end of each step\n",
    "        self.step_count += 1 # increment step count after data collection\n",
    "\n",
    "#########################\n",
    "#\n",
    "# Set initial adopters\n",
    "# \n",
    "# Parameters\n",
    "# - model: the EVStagHuntModel instance\n",
    "# - X0_frac: fraction of agents to initially choose EV adoption\n",
    "# - method: method to choose initial adopters (\"random\" or \"degree\")\n",
    "# - seed: random seed for reproducibility\n",
    "# - high: whether to choose high or low degree nodes for \"degree\" method\n",
    "###########################\n",
    "def set_initial_adopters(model, X0_frac, method=\"random\", seed=None, high=True):\n",
    "    \"\"\"Set a fraction of agents to EV adopters using different heuristics.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    agents = model.schedule.agents\n",
    "    n = len(agents)\n",
    "    k = int(round(X0_frac * n))\n",
    "\n",
    "    for a in agents:\n",
    "        a.strategy = \"D\"\n",
    "\n",
    "    if k <= 0:\n",
    "        return\n",
    "\n",
    "    if method == \"random\":\n",
    "        idx = rng.choice(n, size=k, replace=False)\n",
    "        for i in idx:\n",
    "            agents[i].strategy = \"C\"\n",
    "        return\n",
    "\n",
    "    if method == \"degree\":\n",
    "        deg = dict(model.G.degree())\n",
    "        ordered_nodes = sorted(deg.keys(), key=lambda u: deg[u], reverse=high)\n",
    "        chosen = set(ordered_nodes[:k])\n",
    "        for a in agents:\n",
    "            if a.unique_id in chosen:\n",
    "                a.strategy = \"C\"\n",
    "        return\n",
    "\n",
    "    raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Ratio sweep helpers (computation-only)\n",
    "# -----------------------------\n",
    "#########################\n",
    "#\n",
    "# Run a single network trial\n",
    "# \n",
    "# Parameters\n",
    "# - X0_frac: fraction of agents to initially choose EV adoption\n",
    "# - ratio: payoff ratio between EV and DC agents (a0 = ratio*b - beta_I*I0)\n",
    "# - I0: initial infrastructure level\n",
    "# - beta_I: cost of EV adoption relative to DC (beta_I*I0)\n",
    "# - b: payoff of EV (b)\n",
    "# - g_I: infrastructure growth rate (g_I)\n",
    "# - T: number of time steps to run\n",
    "# - network_type: type of network to generate (\"random\" or \"BA\")\n",
    "# - n_nodes: number of nodes in the network\n",
    "# - p: probability of edge creation in random network\n",
    "# - m: number of edges to attach from a new node to existing nodes in BA network\n",
    "# - seed: random seed for reproducibility\n",
    "# - tol: tolerance for convergence check (default: 1e-3)\n",
    "# - patience: number of steps to wait for convergence (default: 30)\n",
    "\n",
    "def run_network_trial(\n",
    "    X0_frac: float,\n",
    "    ratio: float = 2.0,\n",
    "    *,\n",
    "    I0: float = 0.05,\n",
    "    beta_I: float = 2.0,\n",
    "    b: float = 1.0,\n",
    "    g_I: float = 0.05,\n",
    "    T: int = 200,\n",
    "    network_type: str = \"ER\",\n",
    "    n_nodes: int = 120,\n",
    "    p: float = 0.05,\n",
    "    m: int = 2,\n",
    "    k: int = 30,\n",
    "    seed: int | None = None,\n",
    "    tol: float = 1e-3,\n",
    "    patience: int = 30,\n",
    "    collect: bool = True,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    ") -> float:\n",
    "    \"\"\"Run a single realisation and return final adoption fraction.\n",
    "\n",
    "    Preserves the intended initial payoff ratio via a0 = ratio*b - beta_I*I0.\n",
    "    Includes basic stability-based early stopping.\n",
    "    \"\"\"\n",
    "    initial_ev = int(round(X0_frac * n_nodes))\n",
    "    a0 = ratio * b - beta_I * I0\n",
    "\n",
    "    model = EVStagHuntModel(\n",
    "        initial_ev=initial_ev,\n",
    "        a0=a0,\n",
    "        beta_I=beta_I,\n",
    "        b=b,\n",
    "        g_I=g_I,\n",
    "        I0=I0,\n",
    "        seed=seed,\n",
    "        network_type=network_type,\n",
    "        n_nodes=n_nodes,\n",
    "        p=p,\n",
    "        m=m,\n",
    "        k=k,\n",
    "        collect=collect,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "\n",
    "    stable_steps = 0\n",
    "    prev_X = None\n",
    "    prev_I = None\n",
    "    for _ in range(T):\n",
    "        model.step()\n",
    "        X = model.get_adoption_fraction()\n",
    "        I = model.infrastructure\n",
    "        if prev_X is not None and prev_I is not None:\n",
    "            if abs(X - prev_X) < tol and abs(I - prev_I) < tol:\n",
    "                stable_steps += 1\n",
    "            else:\n",
    "                stable_steps = 0\n",
    "        prev_X, prev_I = X, I\n",
    "        if X in (0.0, 1.0) and stable_steps >= 10:\n",
    "            break\n",
    "        if stable_steps >= patience:\n",
    "            break\n",
    "\n",
    "    return model.get_adoption_fraction()\n",
    "\n",
    "#########################\n",
    "#\n",
    "# Compute final mean adoption fraction vs ratio\n",
    "# \n",
    "##########################\n",
    "def final_mean_adoption_vs_ratio(\n",
    "    X0_frac: float,\n",
    "    ratio_values: Iterable[float],\n",
    "    *,\n",
    "    I0: float = 0.05,\n",
    "    beta_I: float = 2.0,\n",
    "    b: float = 1.0,\n",
    "    g_I: float = 0.05,\n",
    "    T: int = 200,\n",
    "    network_type: str = \"random\",\n",
    "    n_nodes: int = 120,\n",
    "    p: float = 0.05,\n",
    "    m: int = 2,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    strategy_choice_func: str = \"imitate\",\n",
    "    tau: float = 1.0,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute mean final adoption across a sweep of ratio values.\n",
    "\n",
    "    For each ratio, average over `batch_size` trials with jittered `I0` and seeds.\n",
    "    Returns a numpy array of means aligned with `ratio_values` order.\n",
    "    \"\"\"\n",
    "    ratios = list(ratio_values)\n",
    "    means: List[float] = []\n",
    "    for ratio in ratios:\n",
    "        finals: List[float] = []\n",
    "        for _ in range(batch_size):\n",
    "            I0_j = float(np.clip(np.random.normal(loc=I0, scale=init_noise_I), 0.0, 1.0))\n",
    "            seed_j = np.random.randint(0, 2**31 - 1)\n",
    "            x_star = run_network_trial(\n",
    "                X0_frac,\n",
    "                ratio,\n",
    "                I0=I0_j,\n",
    "                beta_I=beta_I,\n",
    "                b=b,\n",
    "                g_I=g_I,\n",
    "                T=T,\n",
    "                network_type=network_type,\n",
    "                n_nodes=n_nodes,\n",
    "                p=p,\n",
    "                m=m,\n",
    "                seed=seed_j,\n",
    "                collect=False,\n",
    "                strategy_choice_func=strategy_choice_func,\n",
    "                tau=tau,\n",
    "            )\n",
    "            finals.append(x_star)\n",
    "        means.append(float(np.mean(finals)))\n",
    "    return np.asarray(means, dtype=float)\n",
    "\n",
    "#########################\n",
    "#\n",
    "# Compute heatmap row for a fixed ratio\n",
    "# \n",
    "##########################\n",
    "def _row_for_I0_task(args: Dict) -> np.ndarray:\n",
    "    \"\"\"Top-level worker to compute one heatmap row for a fixed I0.\n",
    "\n",
    "    Returns an array of mean final adoption across provided X0_values.\n",
    "    \"\"\"\n",
    "    I0 = args[\"I0\"]\n",
    "    X0_values = args[\"X0_values\"]\n",
    "    beta_I = args[\"beta_I\"]\n",
    "    b = args[\"b\"]\n",
    "    g_I = args[\"g_I\"]\n",
    "    T = args[\"T\"]\n",
    "    network_type = args[\"network_type\"]\n",
    "    n_nodes = args[\"n_nodes\"]\n",
    "    p = args[\"p\"]\n",
    "    m = args[\"m\"]\n",
    "    batch_size = args[\"batch_size\"]\n",
    "    init_noise_I = args[\"init_noise_I\"]\n",
    "    strategy_choice_func = args[\"strategy_choice_func\"]\n",
    "    tau = args[\"tau\"]\n",
    "\n",
    "    row = np.empty(len(X0_values), dtype=float)\n",
    "    for j, X0 in enumerate(X0_values):\n",
    "        finals: List[float] = []\n",
    "        for _ in range(batch_size):\n",
    "            # Add small noise to I0 if desired\n",
    "            I0_j = float(np.clip(np.random.normal(loc=I0, scale=init_noise_I), 0.0, 1.0))\n",
    "            seed_j = np.random.randint(0, 2**31 - 1)\n",
    "            x_star = run_network_trial(\n",
    "                X0_frac=X0,\n",
    "                I0=I0_j,\n",
    "                beta_I=beta_I,\n",
    "                b=b,\n",
    "                g_I=g_I,\n",
    "                T=T,\n",
    "                network_type=network_type,\n",
    "                n_nodes=n_nodes,\n",
    "                p=p,\n",
    "                m=m,\n",
    "                seed=seed_j,\n",
    "                collect=False,\n",
    "                strategy_choice_func=strategy_choice_func,\n",
    "                tau=tau,\n",
    "            )\n",
    "            finals.append(x_star)\n",
    "        row[j] = float(np.mean(finals))\n",
    "    return row\n",
    "\n",
    "    \n",
    "#########################\n",
    "#\n",
    "# Compute heatmap matrix for phase sweep\n",
    "# \n",
    "##########################\n",
    "def phase_sweep_X0_vs_I0(\n",
    "    X0_values: Iterable[float],\n",
    "    I0_values: Iterable[float],\n",
    "    *,\n",
    "    beta_I: float = 2.0,\n",
    "    b: float = 1.0,\n",
    "    g_I: float = 0.05,\n",
    "    T: int = 250,\n",
    "    network_type: str = \"BA\",\n",
    "    n_nodes: int = 120,\n",
    "    p: float = 0.05,\n",
    "    m: int = 2,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    "    max_workers: int | None = None,\n",
    "    backend: str = \"process\",\n",
    "    seed: int = 42,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute a heatmap of mean final adoption X* over (X0, I0).\n",
    "\n",
    "    Returns an array of shape (len(I0_values), len(X0_values)), \n",
    "    rows = I0_values, columns = X0_values.\n",
    "    \"\"\"\n",
    "    X0_values = list(X0_values)\n",
    "    I0_values = list(I0_values)\n",
    "    X_final = np.zeros((len(I0_values), len(X0_values)), dtype=float)\n",
    "\n",
    "    # Prepare tasks per I0\n",
    "    tasks: List[Dict] = []\n",
    "    for I0 in I0_values:\n",
    "        tasks.append({\n",
    "            \"I0\": I0,\n",
    "            \"X0_values\": X0_values,\n",
    "            \"beta_I\": beta_I,\n",
    "            \"b\": b,\n",
    "            \"g_I\": g_I,\n",
    "            \"T\": T,\n",
    "            \"network_type\": network_type,\n",
    "            \"n_nodes\": n_nodes,\n",
    "            \"p\": p,\n",
    "            \"m\": m,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"init_noise_I\": init_noise_I,\n",
    "            \"strategy_choice_func\": strategy_choice_func,\n",
    "            \"tau\": tau,\n",
    "            \"seed\": seed,\n",
    "        })\n",
    "\n",
    "    if max_workers is None:\n",
    "        try:\n",
    "            max_workers = os.cpu_count() or 1\n",
    "        except Exception:\n",
    "            max_workers = 1\n",
    "\n",
    "    Executor = ProcessPoolExecutor if backend == \"process\" and max_workers > 1 else ThreadPoolExecutor\n",
    "    if max_workers > 1:\n",
    "        with Executor(max_workers=max_workers) as ex:\n",
    "            futures = [ex.submit(_row_for_I0_task, args) for args in tasks]\n",
    "            for i, fut in enumerate(futures):\n",
    "                row = fut.result()\n",
    "                X_final[i, :] = row\n",
    "    else:\n",
    "        for i, args in enumerate(tasks):\n",
    "            row = _row_for_I0_task(args)\n",
    "            X_final[i, :] = row\n",
    "\n",
    "    return X_final\n",
    "\n",
    "def _default_plot_path(filename: str) -> str:\n",
    "    plots_dir = os.path.join(os.getcwd(), \"plots\")\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    return os.path.join(plots_dir, filename)\n",
    "\n",
    "\n",
    "def plot_fanchart(traces_df: pd.DataFrame, out_path: Optional[str] = None) -> str:\n",
    "    \"\"\"Plot fan charts (quantile bands) for baseline vs subsidy using traces DF.\n",
    "\n",
    "    traces_df columns: ['group', 'trial', 'time', 'X'] where group in {'baseline','subsidy'}.\n",
    "    \"\"\"\n",
    "    if traces_df.empty:\n",
    "        raise ValueError(\"traces_df is empty\")\n",
    "\n",
    "    groups = [\"baseline\", \"subsidy\"]\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(11, 8), constrained_layout=True)\n",
    "\n",
    "    for j, group in enumerate(groups):\n",
    "        gdf = traces_df[traces_df[\"group\"] == group]\n",
    "\n",
    "        # Compute quantiles by time across trials\n",
    "        q = gdf.groupby(\"time\")[\"X\"].quantile([0.10, 0.25, 0.75, 0.90]).unstack(level=1)\n",
    "        mean = gdf.groupby(\"time\")[\"X\"].mean()\n",
    "        t = mean.index.to_numpy()\n",
    "\n",
    "        ax = axes[0, j]\n",
    "        ax.fill_between(t, q[0.10], q[0.90], color=(\"steelblue\" if group == \"baseline\" else \"darkorange\"), alpha=0.15, label=\"10–90%\")\n",
    "        ax.fill_between(t, q[0.25], q[0.75], color=(\"steelblue\" if group == \"baseline\" else \"darkorange\"), alpha=0.30, label=\"25–75%\")\n",
    "\n",
    "        # Overlay some traces for context (sample up to 100 trials)\n",
    "        trial_ids = gdf[\"trial\"].unique()\n",
    "        rng = np.random.default_rng(123)\n",
    "        sample = rng.choice(trial_ids, size=min(100, len(trial_ids)), replace=False)\n",
    "        for tr in sample:\n",
    "            tr_df = gdf[gdf[\"trial\"] == tr]\n",
    "            ax.plot(tr_df[\"time\"], tr_df[\"X\"], color=(\"steelblue\" if group == \"baseline\" else \"darkorange\"), alpha=0.1, linewidth=0.8)\n",
    "\n",
    "        ax.plot(t, mean, color=(\"steelblue\" if group == \"baseline\" else \"darkorange\"), linewidth=2, label=\"mean\")\n",
    "        ax.set_title(f\"{group.capitalize()} adoption\")\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.set_ylabel(\"X(t)\")\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.legend(loc=\"lower right\")\n",
    "\n",
    "        # Final X(T) histogram\n",
    "        t_max = int(gdf[\"time\"].max())\n",
    "        final_vals = gdf[gdf[\"time\"] == t_max].groupby(\"trial\")[\"X\"].mean().to_numpy()\n",
    "        axes[1, j].hist(final_vals, bins=20, color=(\"steelblue\" if group == \"baseline\" else \"darkorange\"), alpha=0.8)\n",
    "        axes[1, j].set_title(f\"{group.capitalize()} final X(T)\")\n",
    "        axes[1, j].set_xlabel(\"X(T)\")\n",
    "        axes[1, j].set_ylabel(\"Count\")\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = _default_plot_path(\"ev_intervention_fanchart.png\")\n",
    "    fig.savefig(out_path, dpi=140)\n",
    "    plt.close(fig)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def plot_spaghetti(traces_df: pd.DataFrame, *, max_traces: int = 100, alpha: float = 0.15, out_path: Optional[str] = None) -> str:\n",
    "    \"\"\"Spaghetti plot from traces DF for baseline vs subsidy.\"\"\"\n",
    "    groups = [\"baseline\", \"subsidy\"]\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(11, 4.5), constrained_layout=True)\n",
    "    rng = np.random.default_rng(123)\n",
    "\n",
    "    for j, group in enumerate(groups):\n",
    "        gdf = traces_df[traces_df[\"group\"] == group]\n",
    "        trial_ids = gdf[\"trial\"].unique()\n",
    "        sample = rng.choice(trial_ids, size=min(max_traces, len(trial_ids)), replace=False)\n",
    "        ax = axes[j]\n",
    "        for tr in sample:\n",
    "            tr_df = gdf[gdf[\"trial\"] == tr]\n",
    "            ax.plot(tr_df[\"time\"], tr_df[\"X\"], color=(\"steelblue\" if group == \"baseline\" else \"darkorange\"), alpha=alpha, linewidth=0.8)\n",
    "        ax.set_title(f\"{group.capitalize()} traces\")\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.set_ylabel(\"X(t)\")\n",
    "        ax.set_ylim(0, 1)\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = _default_plot_path(\"ev_spaghetti.png\")\n",
    "    fig.savefig(out_path, dpi=140)\n",
    "    plt.close(fig)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def plot_density(traces_df: pd.DataFrame, *, x_bins: int = 50, time_bins: Optional[int] = None, out_path: Optional[str] = None) -> str:\n",
    "    \"\"\"Time-evolving density plot (2D histogram) from traces DF.\"\"\"\n",
    "    groups = [\"baseline\", \"subsidy\"]\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4.8), constrained_layout=True)\n",
    "\n",
    "    for j, group in enumerate(groups):\n",
    "        gdf = traces_df[traces_df[\"group\"] == group]\n",
    "        T = int(gdf[\"time\"].max()) + 1\n",
    "        if time_bins is None:\n",
    "            bins_time = T\n",
    "        else:\n",
    "            bins_time = time_bins\n",
    "        hb = axes[j].hist2d(gdf[\"time\"].to_numpy(), gdf[\"X\"].to_numpy(), bins=[bins_time, x_bins], range=[[0, T - 1], [0.0, 1.0]], cmap=\"magma\")\n",
    "        axes[j].set_title(f\"{group.capitalize()} density: time vs X(t)\")\n",
    "        axes[j].set_xlabel(\"Time\")\n",
    "        axes[j].set_ylabel(\"X(t)\")\n",
    "        fig.colorbar(hb[3], ax=axes[j], label=\"count\")\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = _default_plot_path(\"ev_density.png\")\n",
    "    fig.savefig(out_path, dpi=140)\n",
    "    plt.close(fig)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def plot_ratio_sweep(sweep_df: pd.DataFrame, out_path: Optional[str] = None) -> str:\n",
    "    \"\"\"Plot X* vs ratio from a DataFrame with columns ['ratio','X_mean'].\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    ax.plot(sweep_df[\"ratio\"], sweep_df[\"X_mean\"], color=\"C0\", lw=2)\n",
    "    ax.set_xlabel(\"a_I / b (ratio)\")\n",
    "    ax.set_ylabel(\"Final adoption X*\")\n",
    "    ax.set_title(\"X* vs ratio\")\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "    ax.grid(True, alpha=0.25)\n",
    "    if out_path is None:\n",
    "        out_path = _default_plot_path(\"ev_ratio_sweep.png\")\n",
    "    fig.savefig(out_path, dpi=140, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def plot_phase_plot(phase_df: pd.DataFrame, out_path: Optional[str] = None) -> str:\n",
    "    \"\"\"Plot heatmap from tidy DataFrame with columns ['X0','ratio','X_final'].\"\"\"\n",
    "    # Pivot to matrix for imshow\n",
    "    pivot = phase_df.pivot(index=\"ratio\", columns=\"X0\", values=\"X_final\").sort_index().sort_index(axis=1)\n",
    "    ratios = pivot.index.to_numpy()\n",
    "    X0s = pivot.columns.to_numpy()\n",
    "\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    im = plt.imshow(\n",
    "        pivot.to_numpy(),\n",
    "        origin=\"lower\",\n",
    "        extent=[X0s[0], X0s[-1], ratios[0], ratios[-1]],\n",
    "        aspect=\"auto\",\n",
    "        vmin=0.0,\n",
    "        vmax=1.0,\n",
    "        cmap=\"plasma\",\n",
    "    )\n",
    "    plt.colorbar(im, label=\"Final adopters X*\")\n",
    "    plt.xlabel(\"X0 (initial adoption)\")\n",
    "    plt.ylabel(\"a_I / b (initial payoff ratio)\")\n",
    "    plt.title(\"Network phase plot: X* over X0 and a_I/b\")\n",
    "\n",
    "    # Overlay threshold X = 1/ratio\n",
    "    X_thresh = 1.0 / ratios\n",
    "    X_thresh_clipped = np.clip(X_thresh, 0.0, 1.0)\n",
    "    plt.plot(X_thresh_clipped, ratios, color=\"white\", linestyle=\"--\", linewidth=1.5, label=\"X = b / a_I (initial)\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = _default_plot_path(\"ev_phase_plot.png\")\n",
    "    plt.savefig(out_path, dpi=140, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41deea3e",
   "metadata": {},
   "source": [
    "# Sweeping X0 & Plotting X0 Against Final Adoption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "65486d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_mean_adoption_vs_X0(\n",
    "    X0_frac_values: Iterable[float],\n",
    "    *,\n",
    "    ratio: float = 2.0,\n",
    "    I0: float = 0.05,\n",
    "    beta_I: float = 2.0,\n",
    "    b: float = 1.0,\n",
    "    g_I: float = 0.05,\n",
    "    T: int = 200,\n",
    "    network_type: str = \"random\",\n",
    "    n_nodes: int = 120,\n",
    "    p: float = 0.05,\n",
    "    m: int = 2,\n",
    "    k: int = 30,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    strategy_choice_func: str = \"imitate\",\n",
    "    tau: float = 1.0,\n",
    "    seed: int = 42,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute mean final adoption across a sweep of initial X0 fractions.\n",
    "\n",
    "    For each X0_frac, average over `batch_size` trials with jittered `I0` and seeds.\n",
    "    Returns a numpy array of means aligned with `X0_frac_values` order.\n",
    "    \"\"\"\n",
    "    X0_fracs = list(X0_frac_values)\n",
    "    means: list[float] = []\n",
    "    for X0_frac in X0_fracs:\n",
    "        finals: list[float] = []\n",
    "        for _ in range(batch_size):\n",
    "            I0_j = float(np.clip(np.random.normal(loc=I0, scale=init_noise_I), 0.0, 1.0))\n",
    "            seed_j = np.random.randint(0, 2**31 - 1)\n",
    "            x_star = run_network_trial(\n",
    "                X0_frac,\n",
    "                ratio,\n",
    "                I0=I0_j,\n",
    "                beta_I=beta_I,\n",
    "                b=b,\n",
    "                g_I=g_I,\n",
    "                T=T,\n",
    "                network_type=network_type,\n",
    "                n_nodes=n_nodes,\n",
    "                p=p,\n",
    "                m=m,\n",
    "                k=k,\n",
    "                seed=seed_j,\n",
    "                collect=False,\n",
    "                strategy_choice_func=strategy_choice_func,\n",
    "                tau=tau,\n",
    "            )\n",
    "            finals.append(x_star)\n",
    "        means.append(float(np.mean(finals)))\n",
    "    return np.asarray(means, dtype=float)\n",
    "\n",
    "def X0_sweep_df(\n",
    "    X0_frac: Optional[np.ndarray] = None,\n",
    "    scenario_kwargs: Optional[Dict] = None,\n",
    "    T: int = 250,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    "    seed: int = 42,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute X* vs ratio and return as a DataFrame.\"\"\"\n",
    "    scenario = {\n",
    "        \"ratio\": 2.0,\n",
    "        \"beta_I\": 2.0,\n",
    "        \"b\": 1.0,\n",
    "        \"g_I\": 0.05,\n",
    "        \"I0\": 0.05,\n",
    "        \"network_type\": \"random\",\n",
    "        \"n_nodes\": 200,\n",
    "        \"p\": 0.05,\n",
    "        \"m\": 2,\n",
    "        \"k\": 30,\n",
    "        \"seed\": 42,\n",
    "    }\n",
    "    if scenario_kwargs:\n",
    "        scenario.update(scenario_kwargs)\n",
    "\n",
    "    X_means = final_mean_adoption_vs_X0(\n",
    "        X0_frac,\n",
    "        ratio=scenario[\"ratio\"],\n",
    "        I0=scenario[\"I0\"],\n",
    "        beta_I=scenario[\"beta_I\"],\n",
    "        b=scenario[\"b\"],\n",
    "        g_I=scenario[\"g_I\"],\n",
    "        T=T,\n",
    "        network_type=scenario[\"network_type\"],\n",
    "        n_nodes=scenario[\"n_nodes\"],\n",
    "        p=scenario[\"p\"],\n",
    "        m=scenario[\"m\"],\n",
    "        batch_size=batch_size,\n",
    "        init_noise_I=init_noise_I,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    return pd.DataFrame({\"X0\": X0_frac, \"X_mean\": X_means})\n",
    "\n",
    "def plot_X0_sweep(df, out_path=None):\n",
    "    X0_values = df[\"X0\"]\n",
    "    X_means = df[\"X_mean\"]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    ax.plot(X0_values, X_means, lw=2)\n",
    "\n",
    "    ax.set_xlabel(\"Initial adoption X0\")\n",
    "    ax.set_ylabel(\"Final adoption X*\")\n",
    "    ax.set_title(\"X* vs initial adoption X0\")\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "    ax.grid(True, alpha=0.25)\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = \"ev_X0_sweep.png\"\n",
    "\n",
    "    fig.savefig(out_path, dpi=140, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    return out_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cde14d",
   "metadata": {},
   "source": [
    "Defining Parameters for X0 Sweep for Each Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c55e786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.236842</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.273684</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.310526</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.347368</td>\n",
       "      <td>0.025417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.384211</td>\n",
       "      <td>0.366875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.502500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.457895</td>\n",
       "      <td>0.783958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.494737</td>\n",
       "      <td>0.956875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.531579</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.568421</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.605263</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.642105</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.678947</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.715789</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.752632</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.789474</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.826316</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.863158</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          X0    X_mean\n",
       "0   0.200000  0.000000\n",
       "1   0.236842  0.000000\n",
       "2   0.273684  0.000000\n",
       "3   0.310526  0.000000\n",
       "4   0.347368  0.025417\n",
       "5   0.384211  0.366875\n",
       "6   0.421053  0.502500\n",
       "7   0.457895  0.783958\n",
       "8   0.494737  0.956875\n",
       "9   0.531579  1.000000\n",
       "10  0.568421  1.000000\n",
       "11  0.605263  1.000000\n",
       "12  0.642105  1.000000\n",
       "13  0.678947  1.000000\n",
       "14  0.715789  1.000000\n",
       "15  0.752632  1.000000\n",
       "16  0.789474  1.000000\n",
       "17  0.826316  1.000000\n",
       "18  0.863158  1.000000\n",
       "19  0.900000  1.000000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ER_X0_df = X0_sweep_df(\n",
    "    X0_frac= np.linspace(0.2, 0.9, 20, dtype= float),\n",
    "    scenario_kwargs= {\n",
    "        \"ratio\": 2.0,\n",
    "        \"beta_I\": 1.5,\n",
    "        \"b\": 2.0,\n",
    "        \"g_I\": 0.05,\n",
    "        \"I0\": 0.1,\n",
    "        \"network_type\": \"ER\",\n",
    "        \"n_nodes\": 300,\n",
    "        \"p\": 0.1,\n",
    "        \"m\": 2,},\n",
    "    T = 250,\n",
    "    batch_size = 16,\n",
    "    init_noise_I= 0.1,\n",
    "    strategy_choice_func= \"logit\",\n",
    "    tau= 2.0,\n",
    "    seed= 42,\n",
    ")\n",
    "\n",
    "ER_X0_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f9f139e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.096667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.236842</td>\n",
       "      <td>0.093542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.273684</td>\n",
       "      <td>0.097083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.310526</td>\n",
       "      <td>0.095000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.347368</td>\n",
       "      <td>0.098958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.384211</td>\n",
       "      <td>0.101250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.094792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.457895</td>\n",
       "      <td>0.197292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.494737</td>\n",
       "      <td>0.642292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.531579</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.568421</td>\n",
       "      <td>0.980208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.983542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.642105</td>\n",
       "      <td>0.977083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.678947</td>\n",
       "      <td>0.980417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.715789</td>\n",
       "      <td>0.980833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.752632</td>\n",
       "      <td>0.978958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.978958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.826316</td>\n",
       "      <td>0.973750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.978958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.980625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          X0    X_mean\n",
       "0   0.200000  0.096667\n",
       "1   0.236842  0.093542\n",
       "2   0.273684  0.097083\n",
       "3   0.310526  0.095000\n",
       "4   0.347368  0.098958\n",
       "5   0.384211  0.101250\n",
       "6   0.421053  0.094792\n",
       "7   0.457895  0.197292\n",
       "8   0.494737  0.642292\n",
       "9   0.531579  0.866667\n",
       "10  0.568421  0.980208\n",
       "11  0.605263  0.983542\n",
       "12  0.642105  0.977083\n",
       "13  0.678947  0.980417\n",
       "14  0.715789  0.980833\n",
       "15  0.752632  0.978958\n",
       "16  0.789474  0.978958\n",
       "17  0.826316  0.973750\n",
       "18  0.863158  0.978958\n",
       "19  0.900000  0.980625"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BA_X0_df = X0_sweep_df(\n",
    "    X0_frac= np.linspace(0.2, 0.9, 20, dtype= float),\n",
    "    scenario_kwargs= {\n",
    "        \"ratio\": 2.0,\n",
    "        \"beta_I\": 1.5,\n",
    "        \"b\": 2.0,\n",
    "        \"g_I\": 0.05,\n",
    "        \"I0\": 0.1,\n",
    "        \"network_type\": \"BA\",\n",
    "        \"n_nodes\": 300,\n",
    "        \"p\": 0.1,\n",
    "        \"m\": 2,},\n",
    "    T = 250,\n",
    "    batch_size = 16,\n",
    "    init_noise_I= 0.1,\n",
    "    strategy_choice_func= \"logit\",\n",
    "    tau= 2.0,\n",
    "    seed= 42,\n",
    ")\n",
    "\n",
    "BA_X0_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "49a40d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.236842</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.273684</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.310526</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.347368</td>\n",
       "      <td>0.3125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.384211</td>\n",
       "      <td>0.6250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.421053</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.457895</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.494737</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.531579</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.568421</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.605263</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.642105</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.678947</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.715789</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.752632</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.789474</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.826316</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.863158</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          X0  X_mean\n",
       "0   0.200000  0.0000\n",
       "1   0.236842  0.0000\n",
       "2   0.273684  0.0000\n",
       "3   0.310526  0.0000\n",
       "4   0.347368  0.3125\n",
       "5   0.384211  0.6250\n",
       "6   0.421053  1.0000\n",
       "7   0.457895  1.0000\n",
       "8   0.494737  1.0000\n",
       "9   0.531579  1.0000\n",
       "10  0.568421  1.0000\n",
       "11  0.605263  1.0000\n",
       "12  0.642105  1.0000\n",
       "13  0.678947  1.0000\n",
       "14  0.715789  1.0000\n",
       "15  0.752632  1.0000\n",
       "16  0.789474  1.0000\n",
       "17  0.826316  1.0000\n",
       "18  0.863158  1.0000\n",
       "19  0.900000  1.0000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WS_X0_df = X0_sweep_df(\n",
    "    X0_frac= np.linspace(0.2, 0.9, 20, dtype= float),\n",
    "    scenario_kwargs= {\n",
    "        \"ratio\": 2.0,\n",
    "        \"beta_I\": 1.5,\n",
    "        \"b\": 2.0,\n",
    "        \"g_I\": 0.05,\n",
    "        \"I0\": 0.1,\n",
    "        \"network_type\": \"WS\",\n",
    "        \"n_nodes\": 300,\n",
    "        \"p\": 0.1,\n",
    "        \"m\": 2,\n",
    "        \"k\": 30,},\n",
    "    T = 250,\n",
    "    batch_size = 16,\n",
    "    init_noise_I= 0.1,\n",
    "    strategy_choice_func= \"logit\",\n",
    "    tau= 2.0,\n",
    "    seed= 42,\n",
    ")\n",
    "\n",
    "WS_X0_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac383a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BA1_X0_sweep.png'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Plotting ER_X0_df\"\n",
    "plot_X0_sweep(ER_X0_df, out_path=\"ER1_X0_sweep.png\")\n",
    "\"Plotting BA_X0_df\"\n",
    "plot_X0_sweep(BA_X0_df, out_path=\"BA1_X0_sweep.png\")\n",
    "\"Plotting WS_X0_df\"\n",
    "plot_X0_sweep(WS_X0_df, out_path=\"WS1_X0_sweep.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278d4403",
   "metadata": {},
   "source": [
    "# Sweeping I0 & Plotting I0 Against Final Adoption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3cca898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_mean_adoption_vs_I0(\n",
    "    X0_frac: float = 0.05,\n",
    "    I0_values: Optional[Iterable[float]] = None,\n",
    "    *,\n",
    "    ratio: float = 1.0,\n",
    "    beta_I: float = 2.0,\n",
    "    b: float = 1.0,\n",
    "    g_I: float = 0.05,\n",
    "    T: int = 200,\n",
    "    network_type: str = \"ER\",\n",
    "    n_nodes: int = 120,\n",
    "    p: float = 0.05,\n",
    "    m: int = 2,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    strategy_choice_func: str = \"imitate\",\n",
    "    tau: float = 1.0,\n",
    "    seed: int = 42,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute mean final adoption across a sweep of initial infrastructure I0 values.\n",
    "\n",
    "    For each I0, average over `batch_size` trials with jittered I0 and seeds.\n",
    "    Returns a numpy array of means aligned with `I0_values` order.\n",
    "    \"\"\"\n",
    "    I0_list = list(I0_values)\n",
    "    means: list[float] = []\n",
    "    for I0_val in I0_list:\n",
    "        finals: list[float] = []\n",
    "        for _ in range(batch_size):\n",
    "            I0_j = float(np.clip(np.random.normal(loc=I0_val, scale=init_noise_I), 0.0, 1.0))\n",
    "            seed_j = np.random.randint(0, 2**31 - 1)\n",
    "            x_star = run_network_trial(\n",
    "                X0_frac,\n",
    "                ratio,\n",
    "                I0=I0_j,\n",
    "                beta_I=beta_I,\n",
    "                b=b,\n",
    "                g_I=g_I,\n",
    "                T=T,\n",
    "                network_type=network_type,\n",
    "                n_nodes=n_nodes,\n",
    "                p=p,\n",
    "                m=m,\n",
    "                seed=seed_j,\n",
    "                collect=False,\n",
    "                strategy_choice_func=strategy_choice_func,\n",
    "                tau=tau,\n",
    "            )\n",
    "            finals.append(x_star)\n",
    "        means.append(float(np.mean(finals)))\n",
    "    return np.asarray(means, dtype=float)\n",
    "\n",
    "def I0_sweep_df(\n",
    "    X0_frac: float = 0.40,\n",
    "    I0_values: Optional[np.ndarray] = None,\n",
    "    scenario_kwargs: Optional[Dict] = None,\n",
    "    T: int = 250,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    "    seed: int= 42,\n",
    "):\n",
    "    \"\"\"Compute X* vs I0 and return DataFrame.\"\"\"\n",
    "    scenario = {\n",
    "        \"ratio\": 2.0,\n",
    "        \"beta_I\": 2.0,\n",
    "        \"b\": 1.0,\n",
    "        \"g_I\": 0.05,\n",
    "        \"network_type\": \"BA\",\n",
    "        \"n_nodes\": 120,\n",
    "        \"p\": 0.05,\n",
    "        \"m\": 2,\n",
    "        \"seed\": 42,\n",
    "    }\n",
    "    if scenario_kwargs:\n",
    "        scenario.update(scenario_kwargs)\n",
    "\n",
    "    if I0_values is None:\n",
    "        I0_values = np.linspace(0.0, 0.6, 31)\n",
    "\n",
    "    X_means = final_mean_adoption_vs_I0(\n",
    "        X0_frac,\n",
    "        I0_values,\n",
    "        ratio=scenario[\"ratio\"],\n",
    "        beta_I=scenario[\"beta_I\"],\n",
    "        b=scenario[\"b\"],\n",
    "        g_I=scenario[\"g_I\"],\n",
    "        T=T,\n",
    "        network_type=scenario[\"network_type\"],\n",
    "        n_nodes=scenario[\"n_nodes\"],\n",
    "        p=scenario[\"p\"],\n",
    "        m=scenario[\"m\"],\n",
    "        batch_size=batch_size,\n",
    "        init_noise_I=init_noise_I,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    return pd.DataFrame({\"I0\": I0_values, \"X_mean\": X_means})\n",
    "\n",
    "def plot_I0_sweep(df, out_path=None):\n",
    "    I0_values = df[\"I0\"]\n",
    "    X_means = df[\"X_mean\"]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    ax.plot(I0_values, X_means, lw=2)\n",
    "\n",
    "    ax.set_xlabel(\"Initial infrastructure I0\")\n",
    "    ax.set_ylabel(\"Final adoption X*\")\n",
    "    ax.set_title(\"X* vs infrastructure I0 \")\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "    ax.grid(True, alpha=0.25)\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = \"ev_I0_sweep.png\"\n",
    "\n",
    "    fig.savefig(out_path, dpi=140, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80b117",
   "metadata": {},
   "source": [
    "Defining Parameters for I0 Sweep for Each Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c1ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ER_I0_df = I0_sweep_df(\n",
    "    X0_frac = 0.40,\n",
    "    I0_values = np.linspace(0.2, 0.9, 20, dtype= float),\n",
    "    scenario_kwargs= {\n",
    "        \"ratio\": 2.0,\n",
    "        \"beta_I\": 1.5,\n",
    "        \"b\": 2.0,\n",
    "        \"g_I\": 0.05,\n",
    "        \"network_type\": \"ER\",\n",
    "        \"n_nodes\": 300,\n",
    "        \"p\": 0.1,\n",
    "        \"m\": 2,\n",
    "        \"seed\": 42,},\n",
    "    T = 250,\n",
    "    batch_size = 16,\n",
    "    init_noise_I= 0.1,\n",
    "    strategy_choice_func= \"logit\",\n",
    "    tau= 2.0,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "ER_I0_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83c98e4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m BA_I0_df \u001b[38;5;241m=\u001b[39m I0_sweep_df(\n\u001b[0;32m      2\u001b[0m     X0_frac \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.40\u001b[39m,\n\u001b[0;32m      3\u001b[0m     I0_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m20\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m),\n\u001b[0;32m      4\u001b[0m     scenario_kwargs\u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mratio\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2.0\u001b[39m,\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta_I\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1.5\u001b[39m,\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2.0\u001b[39m,\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mg_I\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.05\u001b[39m,\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnetwork_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBA\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m300\u001b[39m,\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m,},\n\u001b[0;32m     13\u001b[0m     T \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m250\u001b[39m,\n\u001b[0;32m     14\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m     15\u001b[0m     init_noise_I\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m     16\u001b[0m     strategy_choice_func\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     tau\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.0\u001b[39m,\n\u001b[0;32m     18\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     21\u001b[0m BA_I0_df\n",
      "Cell \u001b[1;32mIn[20], line 82\u001b[0m, in \u001b[0;36mI0_sweep_df\u001b[1;34m(X0_frac, I0_values, scenario_kwargs, T, batch_size, init_noise_I, strategy_choice_func, tau, seed)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m I0_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     I0_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.6\u001b[39m, \u001b[38;5;241m31\u001b[39m)\n\u001b[1;32m---> 82\u001b[0m X_means \u001b[38;5;241m=\u001b[39m final_mean_adoption_vs_I0(\n\u001b[0;32m     83\u001b[0m     X0_frac,\n\u001b[0;32m     84\u001b[0m     I0_values,\n\u001b[0;32m     85\u001b[0m     ratio\u001b[38;5;241m=\u001b[39mscenario[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mratio\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     86\u001b[0m     beta_I\u001b[38;5;241m=\u001b[39mscenario[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta_I\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     87\u001b[0m     b\u001b[38;5;241m=\u001b[39mscenario[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     88\u001b[0m     g_I\u001b[38;5;241m=\u001b[39mscenario[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mg_I\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     89\u001b[0m     T\u001b[38;5;241m=\u001b[39mT,\n\u001b[0;32m     90\u001b[0m     network_type\u001b[38;5;241m=\u001b[39mscenario[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnetwork_type\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     91\u001b[0m     n_nodes\u001b[38;5;241m=\u001b[39mscenario[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     92\u001b[0m     p\u001b[38;5;241m=\u001b[39mscenario[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     93\u001b[0m     m\u001b[38;5;241m=\u001b[39mscenario[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     94\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m     95\u001b[0m     init_noise_I\u001b[38;5;241m=\u001b[39minit_noise_I,\n\u001b[0;32m     96\u001b[0m     strategy_choice_func\u001b[38;5;241m=\u001b[39mstrategy_choice_func,\n\u001b[0;32m     97\u001b[0m     tau\u001b[38;5;241m=\u001b[39mtau,\n\u001b[0;32m     98\u001b[0m     seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[0;32m     99\u001b[0m )\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI0\u001b[39m\u001b[38;5;124m\"\u001b[39m: I0_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m: X_means})\n",
      "Cell \u001b[1;32mIn[20], line 32\u001b[0m, in \u001b[0;36mfinal_mean_adoption_vs_I0\u001b[1;34m(X0_frac, I0_values, ratio, beta_I, b, g_I, T, network_type, n_nodes, p, m, batch_size, init_noise_I, strategy_choice_func, tau, seed)\u001b[0m\n\u001b[0;32m     30\u001b[0m     I0_j \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(np\u001b[38;5;241m.\u001b[39mclip(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(loc\u001b[38;5;241m=\u001b[39mI0_val, scale\u001b[38;5;241m=\u001b[39minit_noise_I), \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m))\n\u001b[0;32m     31\u001b[0m     seed_j \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m31\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m     x_star \u001b[38;5;241m=\u001b[39m run_network_trial(\n\u001b[0;32m     33\u001b[0m         X0_frac,\n\u001b[0;32m     34\u001b[0m         ratio,\n\u001b[0;32m     35\u001b[0m         I0\u001b[38;5;241m=\u001b[39mI0_j,\n\u001b[0;32m     36\u001b[0m         beta_I\u001b[38;5;241m=\u001b[39mbeta_I,\n\u001b[0;32m     37\u001b[0m         b\u001b[38;5;241m=\u001b[39mb,\n\u001b[0;32m     38\u001b[0m         g_I\u001b[38;5;241m=\u001b[39mg_I,\n\u001b[0;32m     39\u001b[0m         T\u001b[38;5;241m=\u001b[39mT,\n\u001b[0;32m     40\u001b[0m         network_type\u001b[38;5;241m=\u001b[39mnetwork_type,\n\u001b[0;32m     41\u001b[0m         n_nodes\u001b[38;5;241m=\u001b[39mn_nodes,\n\u001b[0;32m     42\u001b[0m         p\u001b[38;5;241m=\u001b[39mp,\n\u001b[0;32m     43\u001b[0m         m\u001b[38;5;241m=\u001b[39mm,\n\u001b[0;32m     44\u001b[0m         seed\u001b[38;5;241m=\u001b[39mseed_j,\n\u001b[0;32m     45\u001b[0m         collect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     46\u001b[0m         strategy_choice_func\u001b[38;5;241m=\u001b[39mstrategy_choice_func,\n\u001b[0;32m     47\u001b[0m         tau\u001b[38;5;241m=\u001b[39mtau,\n\u001b[0;32m     48\u001b[0m     )\n\u001b[0;32m     49\u001b[0m     finals\u001b[38;5;241m.\u001b[39mappend(x_star)\n\u001b[0;32m     50\u001b[0m means\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(finals)))\n",
      "Cell \u001b[1;32mIn[6], line 74\u001b[0m, in \u001b[0;36mrun_network_trial\u001b[1;34m(X0_frac, ratio, I0, beta_I, b, g_I, T, network_type, n_nodes, p, m, k, seed, tol, patience, collect, strategy_choice_func, tau)\u001b[0m\n\u001b[0;32m     72\u001b[0m prev_I \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(T):\n\u001b[1;32m---> 74\u001b[0m     model\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     75\u001b[0m     X \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_adoption_fraction()\n\u001b[0;32m     76\u001b[0m     I \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minfrastructure\n",
      "Cell \u001b[1;32mIn[4], line 94\u001b[0m, in \u001b[0;36mEVStagHuntModel.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m): \n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschedule\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# advance all agents\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_adoption_fraction()  \u001b[38;5;66;03m# compute adoption fraction after all agents have advanced\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     I \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfrastructure  \u001b[38;5;66;03m# infrastructure level before this step\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aryew\\anaconda3\\Lib\\site-packages\\mesa\\time.py:144\u001b[0m, in \u001b[0;36mSimultaneousActivation.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    142\u001b[0m agent_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agents\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_key \u001b[38;5;129;01min\u001b[39;00m agent_keys:\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agents[agent_key]\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# We recompute the keys because some agents might have been removed in\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# the previous loop.\u001b[39;00m\n\u001b[0;32m    147\u001b[0m agent_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agents\u001b[38;5;241m.\u001b[39mkeys())\n",
      "Cell \u001b[1;32mIn[3], line 33\u001b[0m, in \u001b[0;36mEVAgent.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     31\u001b[0m neighbor_agents \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nbr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mG\u001b[38;5;241m.\u001b[39mneighbors(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos):\n\u001b[1;32m---> 33\u001b[0m     neighbor_agents\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgrid\u001b[38;5;241m.\u001b[39mget_cell_list_contents([nbr]))\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m neighbor_agents:\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpayoff \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\aryew\\anaconda3\\Lib\\site-packages\\mesa\\space.py:1078\u001b[0m, in \u001b[0;36mNetworkGrid.get_cell_list_contents\u001b[1;34m(self, cell_list)\u001b[0m\n\u001b[0;32m   1074\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_cell_list_contents\u001b[39m(\u001b[38;5;28mself\u001b[39m, cell_list: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Agent]:\n\u001b[0;32m   1075\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a list of the agents contained in the nodes identified\u001b[39;00m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;124;03m    in `cell_list`; nodes with empty content are excluded.\u001b[39;00m\n\u001b[0;32m   1077\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1078\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_cell_list_contents(cell_list))\n",
      "File \u001b[1;32mc:\\Users\\aryew\\anaconda3\\Lib\\site-packages\\mesa\\space.py:1088\u001b[0m, in \u001b[0;36mNetworkGrid.iter_cell_list_contents\u001b[1;34m(self, cell_list)\u001b[0m\n\u001b[0;32m   1084\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21miter_cell_list_contents\u001b[39m(\u001b[38;5;28mself\u001b[39m, cell_list: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Agent]:\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns an iterator of the agents contained in the nodes identified\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m \u001b[38;5;124;03m    in `cell_list`; nodes with empty content are excluded.\u001b[39;00m\n\u001b[0;32m   1087\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[0;32m   1089\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mG\u001b[38;5;241m.\u001b[39mnodes[node_id][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1090\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m node_id \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mfilterfalse(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_cell_empty, cell_list)\n\u001b[0;32m   1091\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BA_I0_df = I0_sweep_df(\n",
    "    X0_frac = 0.40,\n",
    "    I0_values = np.linspace(0.2, 0.9, 20, dtype= float),\n",
    "    scenario_kwargs= {\n",
    "        \"ratio\": 2.0,\n",
    "        \"beta_I\": 1.5,\n",
    "        \"b\": 2.0,\n",
    "        \"g_I\": 0.05,\n",
    "        \"network_type\": \"BA\",\n",
    "        \"n_nodes\": 300,\n",
    "        \"p\": 0.1,\n",
    "        \"m\": 2,},\n",
    "    T = 250,\n",
    "    batch_size = 16,\n",
    "    init_noise_I= 0.1,\n",
    "    strategy_choice_func= \"logit\",\n",
    "    tau= 2.0,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "BA_I0_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eddc7dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I0</th>\n",
       "      <th>X_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.236842</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.273684</td>\n",
       "      <td>0.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.310526</td>\n",
       "      <td>0.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.347368</td>\n",
       "      <td>0.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.384211</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.457895</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.494737</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.531579</td>\n",
       "      <td>0.1875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.568421</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.642105</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.678947</td>\n",
       "      <td>0.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.715789</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.752632</td>\n",
       "      <td>0.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.826316</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.1250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          I0  X_mean\n",
       "0   0.200000  0.0625\n",
       "1   0.236842  0.0625\n",
       "2   0.273684  0.1250\n",
       "3   0.310526  0.2500\n",
       "4   0.347368  0.1250\n",
       "5   0.384211  0.0000\n",
       "6   0.421053  0.0625\n",
       "7   0.457895  0.0625\n",
       "8   0.494737  0.0000\n",
       "9   0.531579  0.1875\n",
       "10  0.568421  0.0000\n",
       "11  0.605263  0.0625\n",
       "12  0.642105  0.0625\n",
       "13  0.678947  0.1250\n",
       "14  0.715789  0.0000\n",
       "15  0.752632  0.1250\n",
       "16  0.789474  0.1250\n",
       "17  0.826316  0.0625\n",
       "18  0.863158  0.1250\n",
       "19  0.900000  0.1250"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WS_I0_df = I0_sweep_df(\n",
    "    X0_frac = 0.40,\n",
    "    I0_values = np.linspace(0.2, 0.9, 20, dtype= float),\n",
    "    scenario_kwargs= {\n",
    "        \"ratio\": 2.0,\n",
    "        \"beta_I\": 1.5,\n",
    "        \"b\": 2.0,\n",
    "        \"g_I\": 0.05,\n",
    "        \"network_type\": \"WS\",\n",
    "        \"n_nodes\": 300,\n",
    "        \"p\": 0.1,\n",
    "        \"m\": 2,\n",
    "        \"k\": 30,},\n",
    "    T = 250,\n",
    "    batch_size = 16,\n",
    "    init_noise_I= 0.1,\n",
    "    strategy_choice_func= \"logit\",\n",
    "    tau= 2.0,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "WS_I0_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd9f3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Plotting ER_I0_df\"\n",
    "plot_I0_sweep(ER_I0_df, out_path=\"ER1_I0_sweep.png\")\n",
    "\"Plotting BA_I0_df\"\n",
    "plot_I0_sweep(BA_I0_df, out_path=\"BA1_I0_sweep.png\")\n",
    "\"Plotting WS_I0_df\"\n",
    "plot_I0_sweep(WS_I0_df, out_path=\"WS1_I0_sweep.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cdd271",
   "metadata": {},
   "source": [
    "# Sweeping BetaI & Plotting BetaI Against Final Adoption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6285cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_mean_adoption_vs_betaI(\n",
    "    X0_frac: float = 0.05,\n",
    "    I0: float = 0.05,\n",
    "    betaI_values: Iterable[float] = None,\n",
    "    *,\n",
    "    ratio: float = 1.0,\n",
    "    b: float = 1.0,\n",
    "    g_I: float = 0.05,\n",
    "    T: int = 200,\n",
    "    network_type: str = \"ER\",\n",
    "    n_nodes: int = 120,\n",
    "    p: float = 0.05,\n",
    "    m: int = 2,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    strategy_choice_func: str = \"imitate\",\n",
    "    tau: float = 1.0,\n",
    "    seed: int = 42,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute mean final adoption across a sweep of beta_I values.\"\"\"\n",
    "\n",
    "    if betaI_values is None:\n",
    "        betaI_values = np.linspace(0.0, 5.0, 21)  # default range if not provided\n",
    "\n",
    "    betaI_list = list(betaI_values)\n",
    "    means: list[float] = []\n",
    "\n",
    "    for beta_I_val in betaI_list:\n",
    "        finals: list[float] = []\n",
    "        for _ in range(batch_size):\n",
    "            I0_j = float(np.clip(np.random.normal(loc=I0, scale=init_noise_I), 0.0, 1.0))\n",
    "            seed_j = np.random.randint(0, 2**31 - 1)\n",
    "            x_star = run_network_trial(\n",
    "                X0_frac,\n",
    "                ratio,\n",
    "                I0=I0_j,\n",
    "                beta_I=beta_I_val,\n",
    "                b=b,\n",
    "                g_I=g_I,\n",
    "                T=T,\n",
    "                network_type=network_type,\n",
    "                n_nodes=n_nodes,\n",
    "                p=p,\n",
    "                m=m,\n",
    "                seed=seed_j,\n",
    "                collect=False,\n",
    "                strategy_choice_func=strategy_choice_func,\n",
    "                tau=tau,\n",
    "            )\n",
    "            finals.append(x_star)\n",
    "        means.append(float(np.mean(finals)))\n",
    "\n",
    "    return np.asarray(means, dtype=float)\n",
    "\n",
    "def betaI_sweep_df(\n",
    "    X0_frac: float = 0.40,\n",
    "    I0: float = 0.05,\n",
    "    betaI_values: Optional[np.ndarray] = None,\n",
    "    scenario_kwargs: Optional[Dict] = None,\n",
    "    T: int = 250,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"Compute X* vs beta_I and return DataFrame.\"\"\"\n",
    "\n",
    "    scenario = {\n",
    "        \"ratio\": 2.0,\n",
    "        \"b\": 1.0,\n",
    "        \"g_I\": 0.05,\n",
    "        \"network_type\": \"BA\",\n",
    "        \"n_nodes\": 120,\n",
    "        \"p\": 0.05,\n",
    "        \"m\": 2,\n",
    "    }\n",
    "    if scenario_kwargs:\n",
    "        scenario.update(scenario_kwargs)\n",
    "\n",
    "    if betaI_values is None:\n",
    "        betaI_values = np.linspace(0.0, 5.0, 21)\n",
    "\n",
    "    X_means = final_mean_adoption_vs_betaI(\n",
    "        X0_frac,\n",
    "        I0,\n",
    "        betaI_values,\n",
    "        ratio=scenario[\"ratio\"],\n",
    "        b=scenario[\"b\"],\n",
    "        g_I=scenario[\"g_I\"],\n",
    "        T=T,\n",
    "        network_type=scenario[\"network_type\"],\n",
    "        n_nodes=scenario[\"n_nodes\"],\n",
    "        p=scenario[\"p\"],\n",
    "        m=scenario[\"m\"],\n",
    "        batch_size=batch_size,\n",
    "        init_noise_I=init_noise_I,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    return pd.DataFrame({\"beta_I\": betaI_values, \"X_mean\": X_means})\n",
    "\n",
    "def plot_BetaI_sweep(df, out_path=None):\n",
    "    betaI_values = df[\"beta_I\"]\n",
    "    X_means = df[\"X_mean\"]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    ax.plot(betaI_values, X_means, lw=2)\n",
    "\n",
    "    ax.set_xlabel(\"Infrastructure feedback strength βI\")\n",
    "    ax.set_ylabel(\"Final adoption X*\")\n",
    "    ax.set_title(\"X* vs infrastructure feedback strength βI\")\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "    ax.grid(True, alpha=0.25)\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = \"ev_betaI_sweep.png\"\n",
    "\n",
    "    fig.savefig(out_path, dpi=140, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42e9258",
   "metadata": {},
   "source": [
    "Defining Parameters for BetaI Sweep for Each Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902be7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ER_BetaI_df = betaI_sweep_df(\n",
    "    X0_frac= 0.40,\n",
    "    I0= 0.1,\n",
    "    betaI_values= np.linspace(0.5, 9.0, 20, dtype= float),\n",
    "    scenario_kwargs={\n",
    "        \"ratio\": 2.0,\n",
    "        \"b\": 2.0,\n",
    "        \"g_I\": 0.05,\n",
    "        \"network_type\": \"ER\",\n",
    "        \"n_nodes\": 300,\n",
    "        \"p\": 0.1,\n",
    "        \"m\": 2,\n",
    "        \"k\": 30,},\n",
    "    T = 250,\n",
    "    batch_size = 16,\n",
    "    init_noise_I= 0.1,\n",
    "    strategy_choice_func= \"logit\",\n",
    "    tau= 2.0,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "ER_BetaI_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0ed0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BA_BetaI_df = betaI_sweep_df(\n",
    "    X0_frac= 0.40,\n",
    "    I0= 0.1,\n",
    "    betaI_values= np.linspace(0.5, 9.0, 20, dtype= float),\n",
    "    scenario_kwargs={\n",
    "        \"ratio\": 2.0,\n",
    "        \"b\": 2.0,\n",
    "        \"g_I\": 0.05,\n",
    "        \"network_type\": \"BA\",\n",
    "        \"n_nodes\": 300,\n",
    "        \"p\": 0.1,\n",
    "        \"m\": 2,\n",
    "        \"k\": 30,},\n",
    "    T = 250,\n",
    "    batch_size = 16,\n",
    "    init_noise_I= 0.1,\n",
    "    strategy_choice_func= \"logit\",\n",
    "    tau= 2.0,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "BA_BetaI_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417bf339",
   "metadata": {},
   "outputs": [],
   "source": [
    "WS_BetaI_df = betaI_sweep_df(\n",
    "    X0_frac= 0.40,\n",
    "    I0= 0.1,\n",
    "    betaI_values= np.linspace(0.5, 9.0, 20, dtype= float),\n",
    "    scenario_kwargs={\n",
    "        \"ratio\": 2.0,\n",
    "        \"b\": 2.0,\n",
    "        \"g_I\": 0.05,\n",
    "        \"network_type\": \"WS\",\n",
    "        \"n_nodes\": 300,\n",
    "        \"p\": 0.1,\n",
    "        \"m\": 2,\n",
    "        \"k\": 30,},\n",
    "    T = 250,\n",
    "    batch_size = 16,\n",
    "    init_noise_I= 0.1,\n",
    "    strategy_choice_func= \"logit\",\n",
    "    tau= 2.0,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "WS_BetaI_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a10f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Plotting ER_BetaI_df\"\n",
    "plot_BetaI_sweep(ER_BetaI_df, out_path=\"ER1_BetaI_sweep.png\")\n",
    "\"Plotting BA_BetaI_df\"\n",
    "plot_BetaI_sweep(BA_BetaI_df, out_path=\"BA1_BetaI_sweep.png\")\n",
    "\"Plotting WS_BetaI_df\"\n",
    "plot_BetaI_sweep(WS_BetaI_df, out_path=\"WS1_BetaI_sweep.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba1d17c",
   "metadata": {},
   "source": [
    "# !!! Refreshing Core Codes for the Next Section to Avoid Error Spilling Over !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8474c1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_strategy_imitate(agent, neighbors):\n",
    "    \"\"\"Choose strategy of the highest-payoff neighbour (including self).\"\"\"\n",
    "    candidates = neighbors + [agent]\n",
    "    best = max(candidates, key=lambda a: a.payoff)\n",
    "    return best.strategy\n",
    "\n",
    "\n",
    "def choose_strategy_logit(agent, neighbors, a_I, b, tau):\n",
    "    \"\"\"Choose strategy using logit / softmax choice.\n",
    "\n",
    "    Parameters\n",
    "    - agent: the agent choosing a strategy\n",
    "    - neighbors: list of neighbour agents\n",
    "    - a_I: effective coordination payoff given current infrastructure\n",
    "    - b: defection payoff\n",
    "    - tau: temperature parameter for softmax\n",
    "    \"\"\"\n",
    "    # compute expected payoffs for C and D\n",
    "    pi_C = 0.0\n",
    "    pi_D = 0.0\n",
    "    for other in neighbors:\n",
    "        s_j = other.strategy\n",
    "        if s_j == \"C\":\n",
    "            pi_C += a_I\n",
    "            pi_D += b\n",
    "        else:\n",
    "            pi_C += 0.0\n",
    "            pi_D += b\n",
    "\n",
    "    # softmax choice\n",
    "    denom = np.exp(pi_C / tau) + np.exp(pi_D / tau)\n",
    "    P_C = np.exp(pi_C / tau) / denom if denom > 0 else 0.5\n",
    "    return \"C\" if random.random() < P_C else \"D\"\n",
    "\n",
    "\n",
    "\n",
    "####################################\n",
    "# Agent class\n",
    "#\n",
    "# The EVAgent class implements the single agent at a graph node.\n",
    "#\n",
    "# Attributes\n",
    "# - strategy: \"C\" (adopt EV) or \"D\" (defect / ICE)\n",
    "# - payoff: accumulated payoff from interactions with neighbours\n",
    "# - next_strategy: strategy chosen for the next time step\n",
    "####################################\n",
    "class EVAgent(Agent):\n",
    "    \"\"\"Single agent at a graph node.\n",
    "\n",
    "    Attributes\n",
    "    - strategy: \"C\" (adopt EV) or \"D\" (defect / ICE)\n",
    "    - payoff: accumulated payoff from interactions with neighbours\n",
    "    - next_strategy: strategy chosen for the next time step\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, unique_id, model, init_strategy=\"D\"):\n",
    "        super().__init__(unique_id, model)\n",
    "        self.strategy = init_strategy\n",
    "        self.payoff = 0.0\n",
    "        self.next_strategy = init_strategy\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Compute payoff from interactions with neighbours.\n",
    "\n",
    "        Stag Hunt payoff rules:\n",
    "        - C vs C: `a_I` (coordination enhanced by infrastructure)\n",
    "        - C vs D: 0\n",
    "        - D vs C: `b`\n",
    "        - D vs D: `b`\n",
    "        \"\"\"\n",
    "        I = self.model.infrastructure\n",
    "        a0 = self.model.a0\n",
    "        beta_I = self.model.beta_I\n",
    "        b = self.model.b\n",
    "        a_I = a0 + beta_I * I\n",
    "\n",
    "        neighbor_agents = []\n",
    "        for nbr in self.model.G.neighbors(self.pos):\n",
    "            neighbor_agents.extend(self.model.grid.get_cell_list_contents([nbr]))\n",
    "        if not neighbor_agents:\n",
    "            self.payoff = 0.0\n",
    "            return\n",
    "\n",
    "        payoff = 0.0\n",
    "        for other in neighbor_agents:\n",
    "            s_i = self.strategy\n",
    "            s_j = other.strategy\n",
    "            if s_i == \"C\" and s_j == \"C\":\n",
    "                payoff += a_I\n",
    "            elif s_i == \"C\" and s_j == \"D\":\n",
    "                payoff += 0.0\n",
    "            elif s_i == \"D\" and s_j == \"C\":\n",
    "                payoff += b\n",
    "            else:\n",
    "                payoff += b\n",
    "        self.payoff = payoff\n",
    "\n",
    "    ####################################\n",
    "    # Advance method\n",
    "    #\n",
    "    # The advance method updates the agent's strategy based on the selected rule.\n",
    "    #\n",
    "    # Parameters\n",
    "    # - strategy_choice_func: the strategy selection function to use (\"imitate\" or \"logit\")\n",
    "    ####################################\n",
    "    def advance(self, strategy_choice_func=\"imitate\"):\n",
    "        \"\"\"Update next_strategy using the selected rule.\n",
    "\n",
    "        If called without an explicit rule, read `self.model.strategy_choice_func`.\n",
    "        Commit `self.strategy = self.next_strategy` for synchronous updates.\n",
    "        \"\"\"\n",
    "        func = strategy_choice_func if strategy_choice_func is not None else getattr(self.model, \"strategy_choice_func\", \"imitate\")\n",
    "\n",
    "        neighbor_agents = []\n",
    "        for nbr in self.model.G.neighbors(self.pos):\n",
    "            neighbor_agents.extend(self.model.grid.get_cell_list_contents([nbr]))\n",
    "\n",
    "        if func == \"imitate\":\n",
    "            self.next_strategy = choose_strategy_imitate(self, neighbor_agents)\n",
    "        elif func == \"logit\":\n",
    "            a_I = self.model.a0 + self.model.beta_I * self.model.infrastructure\n",
    "            self.next_strategy = choose_strategy_logit(self, neighbor_agents, a_I, self.model.b, getattr(self.model, \"tau\", 1.0))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy choice function: {func}\")\n",
    "\n",
    "        self.strategy = self.next_strategy\n",
    "\n",
    "    ####################################\n",
    "    # Model class\n",
    "    #\n",
    "    # The EVStagHuntModel class implements the Mesa model for EV Stag Hunt on a network.\n",
    "    #\n",
    "    # Parameters\n",
    "    # - initial_ev: number of initial EV nodes\n",
    "    # - a0: base payoff for EV adoption\n",
    "    # - beta_I: payoff enhancement factor for EV adoption\n",
    "    # - b: payoff for ICE defection\n",
    "    # - g_I: infrastructure growth rate\n",
    "    # - I0: initial infrastructure level\n",
    "    # - seed: random seed for reproducibility\n",
    "    # - network_type: type of network to generate (\"random\" or \"BA\")\n",
    "    # - n_nodes: number of nodes in the network\n",
    "    # - p: probability of edge creation in random network\n",
    "    # - m: number of edges to attach to new node in BA network\n",
    "    # - collect: whether to collect agent and model-level data\n",
    "    # - strategy_choice_func: strategy selection function (\"imitate\" or \"logit\")\n",
    "    # - tau: temperature parameter for softmax choice (only used with \"logit\")\n",
    "    ####################################\n",
    "#\n",
    "class EVStagHuntModel(Model):\n",
    "    \"\"\"Mesa model for EV Stag Hunt on a network.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        initial_ev=10,\n",
    "        a0=2.0,\n",
    "        beta_I=3.0,\n",
    "        b=1.0,\n",
    "        g_I=0.1,\n",
    "        I0=0.05,\n",
    "        seed=None,\n",
    "        network_type=\"random\",\n",
    "        n_nodes=100,\n",
    "        p=0.05,\n",
    "        m=2,\n",
    "        k=30,\n",
    "        collect=True,\n",
    "        strategy_choice_func: str = \"imitate\",\n",
    "        tau: float = 1.0,\n",
    "    ):\n",
    "        super().__init__(seed=seed)\n",
    "\n",
    "        # Build graph\n",
    "        if network_type == \"BA\":\n",
    "            G = nx.barabasi_albert_graph(n_nodes, m, seed=seed)\n",
    "        elif network_type == \"WS\":\n",
    "            G = nx.watts_strogatz_graph(n_nodes, k=k, p=p, seed=seed)\n",
    "        else:\n",
    "            G = nx.erdos_renyi_graph(n_nodes, p, seed=seed)\n",
    "        self.G = G\n",
    "        self.grid = NetworkGrid(G)\n",
    "        self.schedule = SimultaneousActivation(self)\n",
    "\n",
    "        # parameters\n",
    "        self.a0 = a0\n",
    "        self.beta_I = beta_I\n",
    "        self.b = b\n",
    "        self.g_I = g_I\n",
    "        self.infrastructure = I0\n",
    "        self.step_count = 0\n",
    "        self.strategy_choice_func = strategy_choice_func\n",
    "        self.tau = tau\n",
    "\n",
    "        # initialize node attribute for agent reference\n",
    "        for n in self.G.nodes:\n",
    "            self.G.nodes[n][\"agent\"] = []\n",
    "\n",
    "        # choose initial EV nodes\n",
    "        total_nodes = self.G.number_of_nodes()\n",
    "        k_ev = max(0, min(initial_ev, total_nodes))\n",
    "        ev_nodes = set(self.random.sample(list(self.G.nodes), k_ev))\n",
    "\n",
    "        # create one agent per node\n",
    "        uid = 0\n",
    "        for node in self.G.nodes:\n",
    "            init_strategy = \"C\" if node in ev_nodes else \"D\"\n",
    "            agent = EVAgent(uid, self, init_strategy)\n",
    "            uid += 1\n",
    "            self.schedule.add(agent)\n",
    "            self.grid.place_agent(agent, node)\n",
    "\n",
    "        self.datacollector = None\n",
    "        if collect:\n",
    "            self.datacollector = DataCollector(\n",
    "                model_reporters={\n",
    "                    \"X\": self.get_adoption_fraction,\n",
    "                    \"I\": lambda m: m.infrastructure,\n",
    "                },\n",
    "                agent_reporters={\"strategy\": \"strategy\", \"payoff\": \"payoff\"},\n",
    "            )\n",
    "\n",
    "    def get_adoption_fraction(self):\n",
    "        agents = self.schedule.agents\n",
    "        if not agents:\n",
    "            return 0.0\n",
    "        return sum(1 for a in agents if a.strategy == \"C\") / len(agents)\n",
    "\n",
    "    # ####################\n",
    "    # Model step function\n",
    "    #\n",
    "    # The step function advances the model by one time step.\n",
    "    # It first advances all agents, then computes the adoption fraction and infrastructure level.\n",
    "    # The infrastructure level is updated based on the adoption fraction and the infrastructure growth rate.\n",
    "    # The updated infrastructure level is clipped to the interval [0, 1].\n",
    "    # Finally, if data collection is enabled, the model and agent data are collected.\n",
    "    #######################\n",
    "    def step(self): \n",
    "        self.schedule.step() # advance all agents\n",
    "        X = self.get_adoption_fraction() # compute adoption fraction after all agents have advanced\n",
    "        I = self.infrastructure # infrastructure level before this step\n",
    "        dI = self.g_I * (X - I) # infrastructure growth rate, impacted by adoption fraction\n",
    "        self.infrastructure = float(min(1.0, max(0.0, I + dI))) # clip infrastructure level to [0, 1]\n",
    "        if self.datacollector is not None:\n",
    "            self.datacollector.collect(self) # collect data at the end of each step\n",
    "        self.step_count += 1 # increment step count after data collection\n",
    "\n",
    "#########################\n",
    "#\n",
    "# Set initial adopters\n",
    "# \n",
    "# Parameters\n",
    "# - model: the EVStagHuntModel instance\n",
    "# - X0_frac: fraction of agents to initially choose EV adoption\n",
    "# - method: method to choose initial adopters (\"random\" or \"degree\")\n",
    "# - seed: random seed for reproducibility\n",
    "# - high: whether to choose high or low degree nodes for \"degree\" method\n",
    "###########################\n",
    "def set_initial_adopters(model, X0_frac, method=\"random\", seed=None, high=True):\n",
    "    \"\"\"Set a fraction of agents to EV adopters using different heuristics.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    agents = model.schedule.agents\n",
    "    n = len(agents)\n",
    "    k = int(round(X0_frac * n))\n",
    "\n",
    "    for a in agents:\n",
    "        a.strategy = \"D\"\n",
    "\n",
    "    if k <= 0:\n",
    "        return\n",
    "\n",
    "    if method == \"random\":\n",
    "        idx = rng.choice(n, size=k, replace=False)\n",
    "        for i in idx:\n",
    "            agents[i].strategy = \"C\"\n",
    "        return\n",
    "\n",
    "    if method == \"degree\":\n",
    "        deg = dict(model.G.degree())\n",
    "        ordered_nodes = sorted(deg.keys(), key=lambda u: deg[u], reverse=high)\n",
    "        chosen = set(ordered_nodes[:k])\n",
    "        for a in agents:\n",
    "            if a.unique_id in chosen:\n",
    "                a.strategy = \"C\"\n",
    "        return\n",
    "\n",
    "    raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Ratio sweep helpers (computation-only)\n",
    "# -----------------------------\n",
    "#########################\n",
    "#\n",
    "# Run a single network trial\n",
    "# \n",
    "# Parameters\n",
    "# - X0_frac: fraction of agents to initially choose EV adoption\n",
    "# - ratio: payoff ratio between EV and DC agents (a0 = ratio*b - beta_I*I0)\n",
    "# - I0: initial infrastructure level\n",
    "# - beta_I: cost of EV adoption relative to DC (beta_I*I0)\n",
    "# - b: payoff of EV (b)\n",
    "# - g_I: infrastructure growth rate (g_I)\n",
    "# - T: number of time steps to run\n",
    "# - network_type: type of network to generate (\"random\" or \"BA\")\n",
    "# - n_nodes: number of nodes in the network\n",
    "# - p: probability of edge creation in random network\n",
    "# - m: number of edges to attach from a new node to existing nodes in BA network\n",
    "# - seed: random seed for reproducibility\n",
    "# - tol: tolerance for convergence check (default: 1e-3)\n",
    "# - patience: number of steps to wait for convergence (default: 30)\n",
    "\n",
    "def run_network_trial(\n",
    "    X0_frac: float,\n",
    "    ratio: float,\n",
    "    *,\n",
    "    I0: float = 0.05,\n",
    "    beta_I: float = 2.0,\n",
    "    b: float = 1.0,\n",
    "    g_I: float = 0.05,\n",
    "    T: int = 200,\n",
    "    network_type: str = \"random\",\n",
    "    n_nodes: int = 120,\n",
    "    p: float = 0.05,\n",
    "    m: int = 2,\n",
    "    k: int = 30,\n",
    "    seed: int | None = None,\n",
    "    tol: float = 1e-3,\n",
    "    patience: int = 30,\n",
    "    collect: bool = False,\n",
    "    strategy_choice_func: str = \"imitate\",\n",
    "    tau: float = 1.0,\n",
    ") -> float:\n",
    "    \"\"\"Run a single realisation and return final adoption fraction.\n",
    "\n",
    "    Preserves the intended initial payoff ratio via a0 = ratio*b - beta_I*I0.\n",
    "    Includes basic stability-based early stopping.\n",
    "    \"\"\"\n",
    "    initial_ev = int(round(X0_frac * n_nodes))\n",
    "    a0 = ratio * b - beta_I * I0\n",
    "\n",
    "    model = EVStagHuntModel(\n",
    "        initial_ev=initial_ev,\n",
    "        a0=a0,\n",
    "        beta_I=beta_I,\n",
    "        b=b,\n",
    "        g_I=g_I,\n",
    "        I0=I0,\n",
    "        seed=seed,\n",
    "        network_type=network_type,\n",
    "        n_nodes=n_nodes,\n",
    "        p=p,\n",
    "        m=m,\n",
    "        k=k,\n",
    "        collect=collect,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "\n",
    "    stable_steps = 0\n",
    "    prev_X = None\n",
    "    prev_I = None\n",
    "    for _ in range(T):\n",
    "        model.step()\n",
    "        X = model.get_adoption_fraction()\n",
    "        I = model.infrastructure\n",
    "        if prev_X is not None and prev_I is not None:\n",
    "            if abs(X - prev_X) < tol and abs(I - prev_I) < tol:\n",
    "                stable_steps += 1\n",
    "            else:\n",
    "                stable_steps = 0\n",
    "        prev_X, prev_I = X, I\n",
    "        if X in (0.0, 1.0) and stable_steps >= 10:\n",
    "            break\n",
    "        if stable_steps >= patience:\n",
    "            break\n",
    "\n",
    "    return model.get_adoption_fraction()\n",
    "\n",
    "#########################\n",
    "#\n",
    "# Compute final mean adoption fraction vs ratio\n",
    "# \n",
    "##########################\n",
    "def final_mean_adoption_vs_ratio(\n",
    "    X0_frac: float,\n",
    "    ratio_values: Iterable[float],\n",
    "    *,\n",
    "    I0: float = 0.05,\n",
    "    beta_I: float = 2.0,\n",
    "    b: float = 1.0,\n",
    "    g_I: float = 0.05,\n",
    "    T: int = 200,\n",
    "    network_type: str = \"random\",\n",
    "    n_nodes: int = 120,\n",
    "    p: float = 0.05,\n",
    "    m: int = 2,\n",
    "    k: int = 30,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    strategy_choice_func: str = \"imitate\",\n",
    "    tau: float = 1.0,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute mean final adoption across a sweep of ratio values.\n",
    "\n",
    "    For each ratio, average over `batch_size` trials with jittered `I0` and seeds.\n",
    "    Returns a numpy array of means aligned with `ratio_values` order.\n",
    "    \"\"\"\n",
    "    ratios = list(ratio_values)\n",
    "    means: List[float] = []\n",
    "    for ratio in ratios:\n",
    "        finals: List[float] = []\n",
    "        for _ in range(batch_size):\n",
    "            I0_j = float(np.clip(np.random.normal(loc=I0, scale=init_noise_I), 0.0, 1.0))\n",
    "            seed_j = np.random.randint(0, 2**31 - 1)\n",
    "            x_star = run_network_trial(\n",
    "                X0_frac,\n",
    "                ratio,\n",
    "                I0=I0_j,\n",
    "                beta_I=beta_I,\n",
    "                b=b,\n",
    "                g_I=g_I,\n",
    "                T=T,\n",
    "                network_type=network_type,\n",
    "                n_nodes=n_nodes,\n",
    "                p=p,\n",
    "                m=m,\n",
    "                k=k,\n",
    "                seed=seed_j,\n",
    "                collect=False,\n",
    "                strategy_choice_func=strategy_choice_func,\n",
    "                tau=tau,\n",
    "            )\n",
    "            finals.append(x_star)\n",
    "        means.append(float(np.mean(finals)))\n",
    "    return np.asarray(means, dtype=float)\n",
    "\n",
    "#########################\n",
    "#\n",
    "# Compute heatmap row for a fixed ratio\n",
    "# \n",
    "##########################\n",
    "def _row_for_ratio_task(args: Dict) -> np.ndarray:\n",
    "    \"\"\"Top-level worker to compute one heatmap row for a fixed ratio.\n",
    "\n",
    "    Returns an array of mean final adoption across provided X0_values.\n",
    "    \"\"\"\n",
    "    ratio = args[\"ratio\"]\n",
    "    X0_values = args[\"X0_values\"]\n",
    "    I0 = args[\"I0\"]\n",
    "    beta_I = args[\"beta_I\"]\n",
    "    b = args[\"b\"]\n",
    "    g_I = args[\"g_I\"]\n",
    "    T = args[\"T\"]\n",
    "    network_type = args[\"network_type\"]\n",
    "    n_nodes = args[\"n_nodes\"]\n",
    "    p = args[\"p\"]\n",
    "    m = args[\"m\"]\n",
    "    k = args[\"k\"]\n",
    "    batch_size = args[\"batch_size\"]\n",
    "    init_noise_I = args[\"init_noise_I\"]\n",
    "    strategy_choice_func = args[\"strategy_choice_func\"]\n",
    "    tau = args[\"tau\"]\n",
    "\n",
    "    row = np.empty(len(X0_values), dtype=float)\n",
    "    for j, X0 in enumerate(X0_values):\n",
    "        finals: List[float] = []\n",
    "        for _ in range(batch_size):\n",
    "            I0_j = float(np.clip(np.random.normal(loc=I0, scale=init_noise_I), 0.0, 1.0))\n",
    "            seed_j = np.random.randint(0, 2**31 - 1)\n",
    "            x_star = run_network_trial(\n",
    "                X0_frac=X0,\n",
    "                ratio=ratio,\n",
    "                I0=I0_j,\n",
    "                beta_I=beta_I,\n",
    "                b=b,\n",
    "                g_I=g_I,\n",
    "                T=T,\n",
    "                network_type=network_type,\n",
    "                n_nodes=n_nodes,\n",
    "                p=p,\n",
    "                m=m,\n",
    "                k=k,\n",
    "                seed=seed_j,\n",
    "                collect=False,\n",
    "                strategy_choice_func=strategy_choice_func,\n",
    "                tau=tau,\n",
    "            )\n",
    "            finals.append(x_star)\n",
    "        row[j] = float(np.mean(finals))\n",
    "    return row\n",
    "\n",
    "    \n",
    "#########################\n",
    "#\n",
    "# Compute heatmap matrix for phase sweep\n",
    "# \n",
    "##########################\n",
    "def phase_sweep_X0_vs_ratio(\n",
    "    X0_values: Iterable[float],\n",
    "    ratio_values: Iterable[float],\n",
    "    *,\n",
    "    I0: float = 0.05,\n",
    "    beta_I: float = 2.0,\n",
    "    b: float = 1.0,\n",
    "    g_I: float = 0.05,\n",
    "    T: int = 250,\n",
    "    network_type: str = \"BA\",\n",
    "    n_nodes: int = 120,\n",
    "    p: float = 0.05,\n",
    "    m: int = 2,\n",
    "    k: int = 30,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    "    max_workers: int | None = None,\n",
    "    backend: str = \"process\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute a heatmap matrix of mean final adoption X* over (X0, ratio).\n",
    "\n",
    "    Returns an array of shape (len(ratio_values), len(X0_values)) aligned with\n",
    "    the provided orders. Rows correspond to ratios; columns to X0 values.\n",
    "    \"\"\"\n",
    "    X0_values = list(X0_values)\n",
    "    ratio_values = list(ratio_values)\n",
    "    X_final = np.zeros((len(ratio_values), len(X0_values)), dtype=float)\n",
    "\n",
    "    # Prepare tasks per ratio\n",
    "    tasks: List[Dict] = []\n",
    "    for ratio in ratio_values:\n",
    "        tasks.append({\n",
    "            \"ratio\": ratio,\n",
    "            \"X0_values\": X0_values,\n",
    "            \"I0\": I0,\n",
    "            \"beta_I\": beta_I,\n",
    "            \"b\": b,\n",
    "            \"g_I\": g_I,\n",
    "            \"T\": T,\n",
    "            \"network_type\": network_type,\n",
    "            \"n_nodes\": n_nodes,\n",
    "            \"p\": p,\n",
    "            \"m\": m,\n",
    "            \"k\": k,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"init_noise_I\": init_noise_I,\n",
    "            \"strategy_choice_func\": strategy_choice_func,\n",
    "            \"tau\": tau,\n",
    "        })\n",
    "\n",
    "    if max_workers is None:\n",
    "        try:\n",
    "            max_workers = os.cpu_count() or 1\n",
    "        except Exception:\n",
    "            max_workers = 1\n",
    "\n",
    "    Executor = ProcessPoolExecutor if backend == \"process\" and max_workers > 1 else ThreadPoolExecutor\n",
    "    if max_workers > 1:\n",
    "        with Executor(max_workers=max_workers) as ex:\n",
    "            futures = [ex.submit(_row_for_ratio_task, args) for args in tasks]\n",
    "            for i, fut in enumerate(futures):\n",
    "                row = fut.result()\n",
    "                X_final[i, :] = row\n",
    "    else:\n",
    "        for i, args in enumerate(tasks):\n",
    "            row = _row_for_ratio_task(args)\n",
    "            X_final[i, :] = row\n",
    "\n",
    "    return X_final\n",
    "\n",
    "def _default_plot_path(filename: str) -> str:\n",
    "    plots_dir = os.path.join(os.getcwd(), \"plots\")\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    return os.path.join(plots_dir, filename)\n",
    "\n",
    "\n",
    "def plot_fanchart(traces_df: pd.DataFrame, out_path: Optional[str] = None) -> str:\n",
    "    \"\"\"Plot fan charts (quantile bands) for baseline vs subsidy using traces DF.\n",
    "\n",
    "    traces_df columns: ['group', 'trial', 'time', 'X'] where group in {'baseline','subsidy'}.\n",
    "    \"\"\"\n",
    "    if traces_df.empty:\n",
    "        raise ValueError(\"traces_df is empty\")\n",
    "\n",
    "    groups = [\"baseline\", \"subsidy\"]\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(11, 8), constrained_layout=True)\n",
    "\n",
    "    for j, group in enumerate(groups):\n",
    "        gdf = traces_df[traces_df[\"group\"] == group]\n",
    "\n",
    "        # Compute quantiles by time across trials\n",
    "        q = gdf.groupby(\"time\")[\"X\"].quantile([0.10, 0.25, 0.75, 0.90]).unstack(level=1)\n",
    "        mean = gdf.groupby(\"time\")[\"X\"].mean()\n",
    "        t = mean.index.to_numpy()\n",
    "\n",
    "        ax = axes[0, j]\n",
    "        ax.fill_between(t, q[0.10], q[0.90], color=(\"steelblue\" if group == \"baseline\" else \"darkorange\"), alpha=0.15, label=\"10–90%\")\n",
    "        ax.fill_between(t, q[0.25], q[0.75], color=(\"steelblue\" if group == \"baseline\" else \"darkorange\"), alpha=0.30, label=\"25–75%\")\n",
    "\n",
    "        # Overlay some traces for context (sample up to 100 trials)\n",
    "        trial_ids = gdf[\"trial\"].unique()\n",
    "        rng = np.random.default_rng(123)\n",
    "        sample = rng.choice(trial_ids, size=min(100, len(trial_ids)), replace=False)\n",
    "        for tr in sample:\n",
    "            tr_df = gdf[gdf[\"trial\"] == tr]\n",
    "            ax.plot(tr_df[\"time\"], tr_df[\"X\"], color=(\"steelblue\" if group == \"baseline\" else \"darkorange\"), alpha=0.1, linewidth=0.8)\n",
    "\n",
    "        ax.plot(t, mean, color=(\"steelblue\" if group == \"baseline\" else \"darkorange\"), linewidth=2, label=\"mean\")\n",
    "        ax.set_title(f\"{group.capitalize()} adoption\")\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.set_ylabel(\"X(t)\")\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.legend(loc=\"lower right\")\n",
    "\n",
    "        # Final X(T) histogram\n",
    "        t_max = int(gdf[\"time\"].max())\n",
    "        final_vals = gdf[gdf[\"time\"] == t_max].groupby(\"trial\")[\"X\"].mean().to_numpy()\n",
    "        axes[1, j].hist(final_vals, bins=20, color=(\"steelblue\" if group == \"baseline\" else \"darkorange\"), alpha=0.8)\n",
    "        axes[1, j].set_title(f\"{group.capitalize()} final X(T)\")\n",
    "        axes[1, j].set_xlabel(\"X(T)\")\n",
    "        axes[1, j].set_ylabel(\"Count\")\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = _default_plot_path(\"ev_intervention_fanchart.png\")\n",
    "    fig.savefig(out_path, dpi=140)\n",
    "    plt.close(fig)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def plot_spaghetti(traces_df: pd.DataFrame, *, max_traces: int = 100, alpha: float = 0.15, out_path: Optional[str] = None) -> str:\n",
    "    \"\"\"Spaghetti plot from traces DF for baseline vs subsidy.\"\"\"\n",
    "    groups = [\"baseline\", \"subsidy\"]\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(11, 4.5), constrained_layout=True)\n",
    "    rng = np.random.default_rng(123)\n",
    "\n",
    "    for j, group in enumerate(groups):\n",
    "        gdf = traces_df[traces_df[\"group\"] == group]\n",
    "        trial_ids = gdf[\"trial\"].unique()\n",
    "        sample = rng.choice(trial_ids, size=min(max_traces, len(trial_ids)), replace=False)\n",
    "        ax = axes[j]\n",
    "        for tr in sample:\n",
    "            tr_df = gdf[gdf[\"trial\"] == tr]\n",
    "            ax.plot(tr_df[\"time\"], tr_df[\"X\"], color=(\"steelblue\" if group == \"baseline\" else \"darkorange\"), alpha=alpha, linewidth=0.8)\n",
    "        ax.set_title(f\"{group.capitalize()} traces\")\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.set_ylabel(\"X(t)\")\n",
    "        ax.set_ylim(0, 1)\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = _default_plot_path(\"ev_spaghetti.png\")\n",
    "    fig.savefig(out_path, dpi=140)\n",
    "    plt.close(fig)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def plot_density(traces_df: pd.DataFrame, *, x_bins: int = 50, time_bins: Optional[int] = None, out_path: Optional[str] = None) -> str:\n",
    "    \"\"\"Time-evolving density plot (2D histogram) from traces DF.\"\"\"\n",
    "    groups = [\"baseline\", \"subsidy\"]\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4.8), constrained_layout=True)\n",
    "\n",
    "    for j, group in enumerate(groups):\n",
    "        gdf = traces_df[traces_df[\"group\"] == group]\n",
    "        T = int(gdf[\"time\"].max()) + 1\n",
    "        if time_bins is None:\n",
    "            bins_time = T\n",
    "        else:\n",
    "            bins_time = time_bins\n",
    "        hb = axes[j].hist2d(gdf[\"time\"].to_numpy(), gdf[\"X\"].to_numpy(), bins=[bins_time, x_bins], range=[[0, T - 1], [0.0, 1.0]], cmap=\"magma\")\n",
    "        axes[j].set_title(f\"{group.capitalize()} density: time vs X(t)\")\n",
    "        axes[j].set_xlabel(\"Time\")\n",
    "        axes[j].set_ylabel(\"X(t)\")\n",
    "        fig.colorbar(hb[3], ax=axes[j], label=\"count\")\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = _default_plot_path(\"ev_density.png\")\n",
    "    fig.savefig(out_path, dpi=140)\n",
    "    plt.close(fig)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def plot_ratio_sweep(sweep_df: pd.DataFrame, out_path: Optional[str] = None) -> str:\n",
    "    \"\"\"Plot X* vs ratio from a DataFrame with columns ['ratio','X_mean'].\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    ax.plot(sweep_df[\"ratio\"], sweep_df[\"X_mean\"], color=\"C0\", lw=2)\n",
    "    ax.set_xlabel(\"a_I / b (ratio)\")\n",
    "    ax.set_ylabel(\"Final adoption X*\")\n",
    "    ax.set_title(\"X* vs ratio\")\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "    ax.grid(True, alpha=0.25)\n",
    "    if out_path is None:\n",
    "        out_path = _default_plot_path(\"ev_ratio_sweep.png\")\n",
    "    fig.savefig(out_path, dpi=140, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def plot_phase_plot(phase_df: pd.DataFrame, out_path: Optional[str] = None) -> str:\n",
    "    \"\"\"Plot heatmap from tidy DataFrame with columns ['X0','ratio','X_final'].\"\"\"\n",
    "    # Pivot to matrix for imshow\n",
    "    pivot = phase_df.pivot(index=\"ratio\", columns=\"X0\", values=\"X_final\").sort_index().sort_index(axis=1)\n",
    "    ratios = pivot.index.to_numpy()\n",
    "    X0s = pivot.columns.to_numpy()\n",
    "\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    im = plt.imshow(\n",
    "        pivot.to_numpy(),\n",
    "        origin=\"lower\",\n",
    "        extent=[X0s[0], X0s[-1], ratios[0], ratios[-1]],\n",
    "        aspect=\"auto\",\n",
    "        vmin=0.0,\n",
    "        vmax=1.0,\n",
    "        cmap=\"plasma\",\n",
    "    )\n",
    "    plt.colorbar(im, label=\"Final adopters X*\")\n",
    "    plt.xlabel(\"X0 (initial adoption)\")\n",
    "    plt.ylabel(\"a_I / b (initial payoff ratio)\")\n",
    "    plt.title(\"Network phase plot: X* over X0 and a_I/b\")\n",
    "\n",
    "    # Overlay threshold X = 1/ratio\n",
    "    X_thresh = 1.0 / ratios\n",
    "    X_thresh_clipped = np.clip(X_thresh, 0.0, 1.0)\n",
    "    plt.plot(X_thresh_clipped, ratios, color=\"white\", linestyle=\"--\", linewidth=1.5, label=\"X = b / a_I (initial)\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = _default_plot_path(\"ev_phase_plot.png\")\n",
    "    plt.savefig(out_path, dpi=140, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    return out_path\n",
    "\n",
    "def policy_subsidy_factory(start: int, end: int, delta_a0: float = 0.3, delta_beta_I: float = 0.0) -> Callable:\n",
    "    \"\"\"Create a policy that temporarily boosts coordination payoffs.\n",
    "\n",
    "    Raises `a0` and/or `beta_I` during `[start, end)` and reverts after.\n",
    "    Returns a closure `policy(model, step)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def policy(model, step):\n",
    "        if not hasattr(policy, \"base_a0\"):\n",
    "            policy.base_a0 = model.a0\n",
    "        if not hasattr(policy, \"base_beta_I\"):\n",
    "            policy.base_beta_I = model.beta_I\n",
    "\n",
    "        if start <= step < end:\n",
    "            model.a0 = policy.base_a0 + delta_a0\n",
    "            model.beta_I = policy.base_beta_I + delta_beta_I\n",
    "        else:\n",
    "            model.a0 = policy.base_a0\n",
    "            model.beta_I = policy.base_beta_I\n",
    "\n",
    "    return policy\n",
    "\n",
    "\n",
    "def policy_infrastructure_boost_factory(start: int, boost: float = 0.2, once: bool = True) -> Callable:\n",
    "    \"\"\"Create a policy that injects infrastructure at a specific step.\"\"\"\n",
    "\n",
    "    def policy(model, step):\n",
    "        if step < start:\n",
    "            return\n",
    "        if once:\n",
    "            if not hasattr(policy, \"done\"):\n",
    "                model.infrastructure = float(np.clip(model.infrastructure + boost, 0.0, 1.0))\n",
    "                policy.done = True\n",
    "        else:\n",
    "            model.infrastructure = float(np.clip(model.infrastructure + boost, 0.0, 1.0))\n",
    "\n",
    "    return policy\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Trial runner\n",
    "# -----------------------------\n",
    "\n",
    "def run_timeseries_trial(\n",
    "    T: int = 200,\n",
    "    scenario_kwargs: Optional[Dict] = None,\n",
    "    seed: Optional[int] = None,\n",
    "    policy: Optional[Callable] = None,\n",
    "    strategy_choice_func: str = \"imitate\",\n",
    "    tau: float = 1.0,\n",
    ") -> Tuple[np.ndarray, np.ndarray, pd.DataFrame]:\n",
    "    \"\"\"Run a single simulation and return X(t), I(t), and the model dataframe.\"\"\"\n",
    "\n",
    "    scenario = {\n",
    "        # Either provide `ratio` to pin the initial a_I/b, or explicit `a0`.\n",
    "        # Defaults here mirror the classroom-friendly values.\n",
    "        # If `ratio` is present, we compute `a0 = ratio*b - beta_I*I0`.\n",
    "        \"a0\": 2.0,\n",
    "        \"ratio\": None,\n",
    "        \"beta_I\": 3.0,\n",
    "        \"b\": 1.0,\n",
    "        \"g_I\": 0.1,\n",
    "        \"I0\": 0.05,\n",
    "        \"network_type\": \"random\",\n",
    "        \"n_nodes\": 100,\n",
    "        \"p\": 0.05,\n",
    "        \"m\": 2,\n",
    "        \"k\": 30,\n",
    "        \"collect\": True,\n",
    "        \"X0_frac\": 0.0,\n",
    "        \"init_method\": \"random\",\n",
    "    }\n",
    "    if scenario_kwargs:\n",
    "        scenario.update(scenario_kwargs)\n",
    "\n",
    "    # Compute a0 from ratio if provided to preserve initial payoff ratio\n",
    "    a0_for_model = scenario[\"a0\"]\n",
    "    if scenario.get(\"ratio\") is not None:\n",
    "        a0_for_model = float(scenario[\"ratio\"]) * float(scenario[\"b\"]) - float(scenario[\"beta_I\"]) * float(scenario[\"I0\"])\n",
    "\n",
    "    model = EVStagHuntModel(\n",
    "        a0=a0_for_model,\n",
    "        beta_I=scenario[\"beta_I\"],\n",
    "        b=scenario[\"b\"],\n",
    "        g_I=scenario[\"g_I\"],\n",
    "        I0=scenario[\"I0\"],\n",
    "        seed=seed,\n",
    "        network_type=scenario[\"network_type\"],\n",
    "        n_nodes=scenario[\"n_nodes\"],\n",
    "        p=scenario[\"p\"],\n",
    "        m=scenario[\"m\"],\n",
    "        k=scenario[\"k\"],\n",
    "        collect=True,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "\n",
    "    if scenario.get(\"X0_frac\", 0.0) > 0.0:\n",
    "        set_initial_adopters(\n",
    "            model,\n",
    "            scenario[\"X0_frac\"],\n",
    "            method=scenario.get(\"init_method\", \"random\"),\n",
    "            seed=seed,\n",
    "        )\n",
    "\n",
    "    for t in range(T):\n",
    "        if policy is not None:\n",
    "            policy(model, t)\n",
    "        model.step()\n",
    "\n",
    "    df = model.datacollector.get_model_vars_dataframe().copy()\n",
    "    return df[\"X\"].to_numpy(), df[\"I\"].to_numpy(), df\n",
    "\n",
    "\n",
    "def _timeseries_trial_worker(args_dict: Dict) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Worker for parallel trials that reconstructs closures for policies.\"\"\"\n",
    "    T = args_dict[\"T\"]\n",
    "    scenario_kwargs = args_dict.get(\"scenario_kwargs\", {})\n",
    "    seed = args_dict.get(\"seed\", None)\n",
    "    policy_spec = args_dict.get(\"policy\", None)\n",
    "    strategy_choice_func = args_dict.get(\"strategy_choice_func\", \"imitate\")\n",
    "    tau = args_dict.get(\"tau\", 1.0)\n",
    "\n",
    "    policy = None\n",
    "    if isinstance(policy_spec, dict):\n",
    "        ptype = policy_spec.get(\"type\")\n",
    "        if ptype == \"subsidy\":\n",
    "            policy = policy_subsidy_factory(**policy_spec[\"params\"])\n",
    "        elif ptype == \"infrastructure\":\n",
    "            policy = policy_infrastructure_boost_factory(**policy_spec[\"params\"])\n",
    "\n",
    "    X, I, _df = run_timeseries_trial(\n",
    "        T=T,\n",
    "        scenario_kwargs=scenario_kwargs,\n",
    "        seed=seed,\n",
    "        policy=policy,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "    return X, I\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Experiment: Intervention trials + plotting\n",
    "# -----------------------------\n",
    "\n",
    "def collect_intervention_trials(\n",
    "    n_trials: int = 10,\n",
    "    T: int = 200,\n",
    "    scenario_kwargs: Optional[Dict] = None,\n",
    "    subsidy_params: Optional[Dict] = None,\n",
    "    max_workers: int = 1,\n",
    "    seed_base: int = 42,\n",
    "    strategy_choice_func: str = \"imitate\",\n",
    "    tau: float = 1.0,\n",
    ") -> Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray], pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Run baseline and subsidy trials; return raw trajectories and summary dataframes.\"\"\"\n",
    "\n",
    "    scenario = scenario_kwargs or {}\n",
    "    subsidy = subsidy_params or {\"start\": 30, \"end\": 80, \"delta_a0\": 0.3, \"delta_beta_I\": 0.0}\n",
    "\n",
    "    baseline_args = []\n",
    "    subsidy_args = []\n",
    "    for i in range(n_trials):\n",
    "        seed = seed_base + i\n",
    "        baseline_args.append(\n",
    "            {\n",
    "                \"T\": T,\n",
    "                \"scenario_kwargs\": scenario,\n",
    "                \"seed\": seed,\n",
    "                \"policy\": None,\n",
    "                \"strategy_choice_func\": strategy_choice_func,\n",
    "                \"tau\": tau,\n",
    "            }\n",
    "        )\n",
    "        subsidy_args.append(\n",
    "            {\n",
    "                \"T\": T,\n",
    "                \"scenario_kwargs\": scenario,\n",
    "                \"seed\": seed,\n",
    "                \"policy\": {\"type\": \"subsidy\", \"params\": subsidy},\n",
    "                \"strategy_choice_func\": strategy_choice_func,\n",
    "                \"tau\": tau,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    baseline_X, baseline_I = [], []\n",
    "    subsidy_X, subsidy_I = [], []\n",
    "\n",
    "    # Run sequentially or concurrently\n",
    "    Executor = ThreadPoolExecutor if max_workers == 1 else ProcessPoolExecutor\n",
    "    with Executor(max_workers=max_workers) as ex:\n",
    "        baseline_futs = [ex.submit(_timeseries_trial_worker, args) for args in baseline_args]\n",
    "        subsidy_futs = [ex.submit(_timeseries_trial_worker, args) for args in subsidy_args]\n",
    "        for fut in as_completed(baseline_futs):\n",
    "            X, I = fut.result()\n",
    "            baseline_X.append(X)\n",
    "            baseline_I.append(I)\n",
    "        for fut in as_completed(subsidy_futs):\n",
    "            X, I = fut.result()\n",
    "            subsidy_X.append(X)\n",
    "            subsidy_I.append(I)\n",
    "\n",
    "    # Align order by seed (as_completed may scramble)\n",
    "    baseline_X = sorted(baseline_X, key=lambda arr: tuple(arr))\n",
    "    subsidy_X = sorted(subsidy_X, key=lambda arr: tuple(arr))\n",
    "\n",
    "    # Summary stats\n",
    "    def summarize(X_list: List[np.ndarray]) -> pd.DataFrame:\n",
    "        mat = np.vstack(X_list)\n",
    "        df = pd.DataFrame({\n",
    "            \"X_mean\": mat.mean(axis=0),\n",
    "            \"X_med\": np.median(mat, axis=0),\n",
    "            \"X_q10\": np.quantile(mat, 0.10, axis=0),\n",
    "            \"X_q25\": np.quantile(mat, 0.25, axis=0),\n",
    "            \"X_q75\": np.quantile(mat, 0.75, axis=0),\n",
    "            \"X_q90\": np.quantile(mat, 0.90, axis=0),\n",
    "        })\n",
    "        return df\n",
    "\n",
    "    baseline_df = summarize(baseline_X)\n",
    "    subsidy_df = summarize(subsidy_X)\n",
    "\n",
    "    return baseline_X, baseline_I, subsidy_X, subsidy_I, baseline_df, subsidy_df\n",
    "\n",
    "\n",
    "def traces_to_long_df(baseline_X: List[np.ndarray], subsidy_X: List[np.ndarray]) -> pd.DataFrame:\n",
    "    \"\"\"Convert trajectory lists to a tidy DataFrame: [group, trial, time, X].\"\"\"\n",
    "    rows = []\n",
    "    for trial, X in enumerate(baseline_X):\n",
    "        for t, x in enumerate(X):\n",
    "            rows.append((\"baseline\", trial, t, float(x)))\n",
    "    for trial, X in enumerate(subsidy_X):\n",
    "        for t, x in enumerate(X):\n",
    "            rows.append((\"subsidy\", trial, t, float(x)))\n",
    "    return pd.DataFrame(rows, columns=[\"group\", \"trial\", \"time\", \"X\"])\n",
    "\n",
    "\n",
    "def ratio_sweep_df(\n",
    "    X0_frac: float = 0.40,\n",
    "    ratio_values: Optional[np.ndarray] = None,\n",
    "    scenario_kwargs: Optional[Dict] = None,\n",
    "    T: int = 250,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute X* vs ratio and return as a DataFrame.\"\"\"\n",
    "    scenario = {\n",
    "        \"beta_I\": 2.0,\n",
    "        \"b\": 1.0,\n",
    "        \"g_I\": 0.05,\n",
    "        \"I0\": 0.05,\n",
    "        \"network_type\": \"BA\",\n",
    "        \"n_nodes\": 120,\n",
    "        \"p\": 0.05,\n",
    "        \"m\": 2,\n",
    "        \"k\": 30,\n",
    "    }\n",
    "    if scenario_kwargs:\n",
    "        scenario.update(scenario_kwargs)\n",
    "\n",
    "    if ratio_values is None:\n",
    "        ratio_values = np.linspace(0.8, 3.5, 41)\n",
    "\n",
    "    X_means = final_mean_adoption_vs_ratio(\n",
    "        X0_frac,\n",
    "        ratio_values,\n",
    "        I0=scenario[\"I0\"],\n",
    "        beta_I=scenario[\"beta_I\"],\n",
    "        b=scenario[\"b\"],\n",
    "        g_I=scenario[\"g_I\"],\n",
    "        T=T,\n",
    "        network_type=scenario[\"network_type\"],\n",
    "        n_nodes=scenario[\"n_nodes\"],\n",
    "        p=scenario[\"p\"],\n",
    "        m=scenario[\"m\"],\n",
    "        k=scenario[\"k\"],\n",
    "        batch_size=batch_size,\n",
    "        init_noise_I=init_noise_I,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "\n",
    "    return pd.DataFrame({\"ratio\": ratio_values, \"X_mean\": X_means})\n",
    "\n",
    "\n",
    "def phase_sweep_df(\n",
    "    max_workers: int | None = None,\n",
    "    backend: str = \"process\",\n",
    "    X0_values: Optional[np.ndarray] = None,\n",
    "    ratio_values: Optional[np.ndarray] = None,\n",
    "    scenario_kwargs: Optional[Dict] = None,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    T: int = 250,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute tidy DataFrame of X* over (X0, ratio).\"\"\"\n",
    "    if X0_values is None:\n",
    "        X0_values = np.linspace(0.0, 1.0, 21)\n",
    "    if ratio_values is None:\n",
    "        ratio_values = np.linspace(0.8, 3.5, 41)\n",
    "\n",
    "    scenario = {\n",
    "        \"I0\": 0.05,\n",
    "        \"beta_I\": 2.0,\n",
    "        \"b\": 1.0,\n",
    "        \"g_I\": 0.05,\n",
    "        \"network_type\": \"BA\",\n",
    "        \"n_nodes\": 120,\n",
    "        \"p\": 0.05,\n",
    "        \"m\": 2,\n",
    "        \"k\": 30,\n",
    "    }\n",
    "    if scenario_kwargs:\n",
    "        scenario.update(scenario_kwargs)\n",
    "\n",
    "    X_final = phase_sweep_X0_vs_ratio(\n",
    "        X0_values,\n",
    "        ratio_values,\n",
    "        I0=scenario[\"I0\"],\n",
    "        beta_I=scenario[\"beta_I\"],\n",
    "        b=scenario[\"b\"],\n",
    "        g_I=scenario[\"g_I\"],\n",
    "        T=T,\n",
    "        network_type=scenario[\"network_type\"],\n",
    "        n_nodes=scenario[\"n_nodes\"],\n",
    "        p=scenario[\"p\"],\n",
    "        m=scenario[\"m\"],\n",
    "        k=scenario[\"k\"],\n",
    "        batch_size=batch_size,\n",
    "        init_noise_I=init_noise_I,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "        max_workers=max_workers or 1,\n",
    "        backend=backend,\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "    for i, X0 in enumerate(X0_values):\n",
    "        for j, ratio in enumerate(ratio_values):\n",
    "            rows.append((float(X0), float(ratio), float(X_final[j, i])))\n",
    "    return pd.DataFrame(rows, columns=[\"X0\", \"ratio\", \"X_final\"])\n",
    "\n",
    "\n",
    "def plot_intervention_fanchart(\n",
    "    baseline_X: List[np.ndarray],\n",
    "    subsidy_X: List[np.ndarray],\n",
    "    out_path: Optional[str] = None,\n",
    ") -> str:\n",
    "    \"\"\"Plot fan charts for baseline and subsidy trials and save to file.\n",
    "\n",
    "    Returns the file path to the saved image.\n",
    "    \"\"\"\n",
    "    T = len(baseline_X[0]) if baseline_X else 0\n",
    "    t = np.arange(T)\n",
    "\n",
    "    def quantiles(X_list: List[np.ndarray]):\n",
    "        mat = np.vstack(X_list)\n",
    "        return {\n",
    "            \"mean\": mat.mean(axis=0),\n",
    "            \"q10\": np.quantile(mat, 0.10, axis=0),\n",
    "            \"q25\": np.quantile(mat, 0.25, axis=0),\n",
    "            \"q75\": np.quantile(mat, 0.75, axis=0),\n",
    "            \"q90\": np.quantile(mat, 0.90, axis=0),\n",
    "            \"final\": mat[:, -1],\n",
    "        }\n",
    "\n",
    "    bq = quantiles(baseline_X)\n",
    "    sq = quantiles(subsidy_X)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(11, 8), constrained_layout=True)\n",
    "\n",
    "    # Baseline fan chart\n",
    "    ax = axes[0, 0]\n",
    "    ax.fill_between(t, bq[\"q10\"], bq[\"q90\"], color=\"steelblue\", alpha=0.15, label=\"10–90%\")\n",
    "    ax.fill_between(t, bq[\"q25\"], bq[\"q75\"], color=\"steelblue\", alpha=0.30, label=\"25–75%\")\n",
    "    for X in baseline_X:\n",
    "        ax.plot(t, X, color=\"steelblue\", alpha=0.10, linewidth=1)\n",
    "    ax.plot(t, bq[\"mean\"], color=\"steelblue\", linewidth=2, label=\"mean\")\n",
    "    ax.set_title(\"Baseline adoption\")\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"X(t)\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "    # Subsidy fan chart\n",
    "    ax = axes[0, 1]\n",
    "    ax.fill_between(t, sq[\"q10\"], sq[\"q90\"], color=\"darkorange\", alpha=0.15, label=\"10–90%\")\n",
    "    ax.fill_between(t, sq[\"q25\"], sq[\"q75\"], color=\"darkorange\", alpha=0.30, label=\"25–75%\")\n",
    "    for X in subsidy_X:\n",
    "        ax.plot(t, X, color=\"darkorange\", alpha=0.10, linewidth=1)\n",
    "    ax.plot(t, sq[\"mean\"], color=\"darkorange\", linewidth=2, label=\"mean\")\n",
    "    ax.set_title(\"Subsidy adoption\")\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"X(t)\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "    # Histograms of final X(T)\n",
    "    axes[1, 0].hist(bq[\"final\"], bins=20, color=\"steelblue\", alpha=0.8)\n",
    "    axes[1, 0].set_title(\"Baseline final adoption X(T)\")\n",
    "    axes[1, 0].set_xlabel(\"X(T)\")\n",
    "    axes[1, 0].set_ylabel(\"Count\")\n",
    "\n",
    "    axes[1, 1].hist(sq[\"final\"], bins=20, color=\"darkorange\", alpha=0.8)\n",
    "    axes[1, 1].set_title(\"Subsidy final adoption X(T)\")\n",
    "    axes[1, 1].set_xlabel(\"X(T)\")\n",
    "    axes[1, 1].set_ylabel(\"Count\")\n",
    "\n",
    "    # Save figure\n",
    "    if out_path is None:\n",
    "        out_path = os.path.join(os.getcwd(), \"ev_intervention_fanchart.png\")\n",
    "    fig.savefig(out_path, dpi=140)\n",
    "    plt.close(fig)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def plot_spaghetti_traces(\n",
    "    baseline_X: List[np.ndarray],\n",
    "    subsidy_X: List[np.ndarray],\n",
    "    *,\n",
    "    max_traces: int = 100,\n",
    "    alpha: float = 0.15,\n",
    "    out_path: Optional[str] = None,\n",
    ") -> str:\n",
    "    \"\"\"Plot raw trajectories as thin, transparent lines for baseline/subsidy.\n",
    "\n",
    "    Shows bifurcation visually: many lines diverging toward 0 or 1 over time.\n",
    "    \"\"\"\n",
    "    # Select random subset for visual clarity\n",
    "    rng = np.random.default_rng(123)\n",
    "    def subset(trajs: List[np.ndarray]) -> List[np.ndarray]:\n",
    "        if len(trajs) <= max_traces:\n",
    "            return trajs\n",
    "        idx = rng.choice(len(trajs), size=max_traces, replace=False)\n",
    "        return [trajs[i] for i in idx]\n",
    "\n",
    "    b_sub = subset(baseline_X)\n",
    "    s_sub = subset(subsidy_X)\n",
    "\n",
    "    T = len(b_sub[0]) if b_sub else 0\n",
    "    t = np.arange(T)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(11, 4.5), constrained_layout=True)\n",
    "\n",
    "    ax = axes[0]\n",
    "    for X in b_sub:\n",
    "        ax.plot(t, X, color=\"steelblue\", alpha=alpha, linewidth=0.8)\n",
    "    ax.set_title(\"Baseline traces\")\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"X(t)\")\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "    ax = axes[1]\n",
    "    for X in s_sub:\n",
    "        ax.plot(t, X, color=\"darkorange\", alpha=alpha, linewidth=0.8)\n",
    "    ax.set_title(\"Subsidy traces\")\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"X(t)\")\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = os.path.join(os.getcwd(), \"ev_spaghetti.png\")\n",
    "    fig.savefig(out_path, dpi=140)\n",
    "    plt.close(fig)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def plot_time_evolving_density(\n",
    "    baseline_X: List[np.ndarray],\n",
    "    subsidy_X: List[np.ndarray],\n",
    "    *,\n",
    "    x_bins: int = 50,\n",
    "    time_bins: Optional[int] = None,\n",
    "    out_path: Optional[str] = None,\n",
    ") -> str:\n",
    "    \"\"\"Plot 2D histograms of (time, X) densities for baseline and subsidy.\n",
    "\n",
    "    X-axis: time, Y-axis: adoption X(t), Color: frequency/density of passes.\n",
    "    \"\"\"\n",
    "    if not baseline_X or not subsidy_X:\n",
    "        raise ValueError(\"Need non-empty baseline and subsidy trajectories\")\n",
    "\n",
    "    T = len(baseline_X[0])\n",
    "    if time_bins is None:\n",
    "        time_bins = T\n",
    "\n",
    "    # Flatten (t, X) points across all trials\n",
    "    def flatten_points(trajs: List[np.ndarray]):\n",
    "        t = np.arange(T)\n",
    "        t_all = np.repeat(t, len(trajs))\n",
    "        x_all = np.hstack(trajs)\n",
    "        return t_all, x_all\n",
    "\n",
    "    bt, bx = flatten_points(baseline_X)\n",
    "    st, sx = flatten_points(subsidy_X)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4.8), constrained_layout=True)\n",
    "\n",
    "    hb = axes[0].hist2d(bt, bx, bins=[time_bins, x_bins], range=[[0, T - 1], [0.0, 1.0]], cmap=\"magma\")\n",
    "    axes[0].set_title(\"Baseline density: time vs X(t)\")\n",
    "    axes[0].set_xlabel(\"Time\")\n",
    "    axes[0].set_ylabel(\"X(t)\")\n",
    "    fig.colorbar(hb[3], ax=axes[0], label=\"count\")\n",
    "\n",
    "    hs = axes[1].hist2d(st, sx, bins=[time_bins, x_bins], range=[[0, T - 1], [0.0, 1.0]], cmap=\"magma\")\n",
    "    axes[1].set_title(\"Subsidy density: time vs X(t)\")\n",
    "    axes[1].set_xlabel(\"Time\")\n",
    "    axes[1].set_ylabel(\"X(t)\")\n",
    "    fig.colorbar(hs[3], ax=axes[1], label=\"count\")\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = os.path.join(os.getcwd(), \"ev_density.png\")\n",
    "    fig.savefig(out_path, dpi=140)\n",
    "    plt.close(fig)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def run_ratio_sweep_plot(\n",
    "    X0_frac: float = 0.40,\n",
    "    ratio_values: Optional[np.ndarray] = None,\n",
    "    scenario_kwargs: Optional[Dict] = None,\n",
    "    T: int = 250,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    "    out_path: Optional[str] = None,\n",
    ") -> str:\n",
    "    \"\"\"Sweep ratio values and plot final adoption X* vs a_I/b for a fixed X0.\n",
    "\n",
    "    Calls the core computation helper and saves a simple line plot.\n",
    "    Returns the path to the saved image.\n",
    "    \"\"\"\n",
    "    scenario = {\n",
    "        \"beta_I\": 2.0,\n",
    "        \"b\": 1.0,\n",
    "        \"g_I\": 0.05,\n",
    "        \"I0\": 0.05,\n",
    "        \"network_type\": \"BA\",\n",
    "        \"n_nodes\": 120,\n",
    "        \"p\": 0.05,\n",
    "        \"m\": 2,\n",
    "        \"k\": 30,\n",
    "    }\n",
    "    if scenario_kwargs:\n",
    "        scenario.update(scenario_kwargs)\n",
    "\n",
    "    if ratio_values is None:\n",
    "        ratio_values = np.linspace(0.8, 3.5, 41)\n",
    "\n",
    "    X_means = final_mean_adoption_vs_ratio(\n",
    "        X0_frac,\n",
    "        ratio_values,\n",
    "        I0=scenario[\"I0\"],\n",
    "        beta_I=scenario[\"beta_I\"],\n",
    "        b=scenario[\"b\"],\n",
    "        g_I=scenario[\"g_I\"],\n",
    "        T=T,\n",
    "        network_type=scenario[\"network_type\"],\n",
    "        n_nodes=scenario[\"n_nodes\"],\n",
    "        p=scenario[\"p\"],\n",
    "        m=scenario[\"m\"],\n",
    "        k=scenario[\"k\"],\n",
    "        batch_size=batch_size,\n",
    "        init_noise_I=init_noise_I,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    ax.plot(ratio_values, X_means, color=\"tabblue\" if hasattr(plt, \"tabblue\") else \"C0\", lw=2)\n",
    "    ax.set_xlabel(\"a_I / b (ratio)\")\n",
    "    ax.set_ylabel(\"Final adoption X*\")\n",
    "    ax.set_title(f\"X* vs ratio for X0={X0_frac:.2f}\")\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "    ax.grid(True, alpha=0.25)\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = os.path.join(os.getcwd(), \"ev_ratio_sweep.png\")\n",
    "    fig.savefig(out_path, dpi=140, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def run_phase_plot_X0_vs_ratio_network(\n",
    "    max_workers: int | None = None,\n",
    "    backend: str = \"process\",\n",
    "    X0_values: Optional[np.ndarray] = None,\n",
    "    ratio_values: Optional[np.ndarray] = None,\n",
    "    scenario_kwargs: Optional[Dict] = None,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    T: int = 250,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    "    out_path: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"Produce a heatmap of X* over (X0, a_I/b) using core sweep helper.\n",
    "\n",
    "    Saves a figure similar to the original model script and returns the path.\n",
    "    \"\"\"\n",
    "    # Defaults aligned with the original phase plot\n",
    "    if X0_values is None:\n",
    "        X0_values = np.linspace(0.0, 1.0, 21)\n",
    "    if ratio_values is None:\n",
    "        ratio_values = np.linspace(0.8, 3.5, 41)\n",
    "\n",
    "    scenario = {\n",
    "        \"I0\": 0.05,\n",
    "        \"beta_I\": 2.0,\n",
    "        \"b\": 1.0,\n",
    "        \"g_I\": 0.05,\n",
    "        \"network_type\": \"BA\",\n",
    "        \"n_nodes\": 120,\n",
    "        \"p\": 0.05,\n",
    "        \"m\": 2,\n",
    "        \"k\": 30,\n",
    "    }\n",
    "    if scenario_kwargs:\n",
    "        scenario.update(scenario_kwargs)\n",
    "\n",
    "    X_final = phase_sweep_X0_vs_ratio(\n",
    "        X0_values,\n",
    "        ratio_values,\n",
    "        I0=scenario[\"I0\"],\n",
    "        beta_I=scenario[\"beta_I\"],\n",
    "        b=scenario[\"b\"],\n",
    "        g_I=scenario[\"g_I\"],\n",
    "        T=T,\n",
    "        network_type=scenario[\"network_type\"],\n",
    "        n_nodes=scenario[\"n_nodes\"],\n",
    "        p=scenario[\"p\"],\n",
    "        m=scenario[\"m\"],\n",
    "        k=scenario[\"k\"],\n",
    "        batch_size=batch_size,\n",
    "        init_noise_I=init_noise_I,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "        max_workers=max_workers or 1,\n",
    "        backend=backend,\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    im = plt.imshow(\n",
    "        X_final,\n",
    "        origin=\"lower\",\n",
    "        extent=[X0_values[0], X0_values[-1], ratio_values[0], ratio_values[-1]],\n",
    "        aspect=\"auto\",\n",
    "        vmin=0.0,\n",
    "        vmax=1.0,\n",
    "        cmap=\"plasma\",\n",
    "    )\n",
    "    plt.colorbar(im, label=\"Final adopters X*\")\n",
    "    plt.xlabel(\"X0 (initial adoption)\")\n",
    "    plt.ylabel(\"a_I / b (initial payoff ratio)\")\n",
    "    plt.title(\"Network phase plot: X* over X0 and a_I/b\")\n",
    "\n",
    "    # Overlay initial threshold X = b/a_I => X = 1/ratio\n",
    "    X_thresh = 1.0 / ratio_values\n",
    "    X_thresh_clipped = np.clip(X_thresh, 0.0, 1.0)\n",
    "    plt.plot(\n",
    "        X_thresh_clipped,\n",
    "        ratio_values,\n",
    "        color=\"white\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=1.5,\n",
    "        label=\"X = b / a_I (initial)\",\n",
    "    )\n",
    "\n",
    "    plt.legend(loc=\"upper right\")\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = os.path.join(os.getcwd(), \"ev_phase_plot.png\")\n",
    "    plt.savefig(out_path, dpi=140, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def run_intervention_example(\n",
    "    n_trials: int = 10,\n",
    "    T: int = 200,\n",
    "    scenario_kwargs: Optional[Dict] = None,\n",
    "    subsidy_params: Optional[Dict] = None,\n",
    "    max_workers: int = 1,\n",
    "    seed_base: int = 42,\n",
    "    strategy_choice_func: str = \"imitate\",\n",
    "    tau: float = 1.0,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, str]:\n",
    "    \"\"\"Convenience: collect trials, plot, and return summary + image path.\"\"\"\n",
    "\n",
    "    baseline_X, baseline_I, subsidy_X, subsidy_I, baseline_df, subsidy_df = collect_intervention_trials(\n",
    "        n_trials=n_trials,\n",
    "        T=T,\n",
    "        scenario_kwargs=scenario_kwargs,\n",
    "        subsidy_params=subsidy_params,\n",
    "        max_workers=max_workers,\n",
    "        seed_base=seed_base,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "    # Use DataFrame-based plotting to ensure outputs go to plots/\n",
    "    traces_df = traces_to_long_df(baseline_X, subsidy_X)\n",
    "    img_path = plot_fanchart(traces_df)\n",
    "    return baseline_df, subsidy_df, img_path\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# CLI Entrypoint\n",
    "# -----------------------------\n",
    "\n",
    "def main():\n",
    "    # Defaults aligned with original ev_stag_mesa_model.run_intervention_example\n",
    "    n_trials = 30  # use fewer than 500 for speed while keeping shape\n",
    "    T = 200\n",
    "    strategy_choice_func = \"logit\"\n",
    "    tau = 1.0\n",
    "    max_workers = 1\n",
    "    seed_base = 100\n",
    "\n",
    "    scenario = dict(\n",
    "        # Preserve initial ratio by computing a0 from ratio, matching the original\n",
    "        ratio=2.3,\n",
    "        beta_I=2.0,\n",
    "        b=1.0,\n",
    "        g_I=0.10,\n",
    "        I0=0.05,\n",
    "        network_type=\"BA\",\n",
    "        n_nodes=300,\n",
    "        m=2,\n",
    "        collect=True,\n",
    "        X0_frac=0.40,\n",
    "        init_method=\"random\",\n",
    "        # ER-specific `p` ignored for BA but kept for completeness\n",
    "        p=0.05,\n",
    "        k=30,\n",
    "    )\n",
    "    subsidy = dict(start=10, end=60, delta_a0=0.4, delta_beta_I=0.0)\n",
    "\n",
    "    baseline_df, subsidy_df, img_path = run_intervention_example(\n",
    "        n_trials=n_trials,\n",
    "        T=T,\n",
    "        scenario_kwargs=scenario,\n",
    "        subsidy_params=subsidy,\n",
    "        max_workers=max_workers,\n",
    "        seed_base=seed_base,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "\n",
    "    print(\"Baseline DF shape:\", baseline_df.shape)\n",
    "    print(\"Subsidy DF shape:\", subsidy_df.shape)\n",
    "    print(\"Saved image:\", img_path)\n",
    "    print(\"Baseline final X_mean:\", float(baseline_df[\"X_mean\"].iloc[-1]))\n",
    "    print(\"Subsidy  final X_mean:\", float(subsidy_df[\"X_mean\"].iloc[-1]))\n",
    "\n",
    "    # Also run the phase plot of X* over (X0, a_I/b) and save it\n",
    "    phase_df = phase_sweep_df(\n",
    "        max_workers=1,\n",
    "        backend=\"thread\",\n",
    "        X0_values=np.linspace(0.0, 1.0, 21),\n",
    "        ratio_values=np.linspace(0.8, 3.5, 31),\n",
    "        batch_size=8,\n",
    "        T=200,\n",
    "        strategy_choice_func=\"logit\",\n",
    "        tau=1.0,\n",
    "    )\n",
    "    phase_path = plot_phase_plot(phase_df)\n",
    "    print(\"Saved phase plot:\", phase_path)\n",
    "\n",
    "    # Spaghetti and time-evolving density plots\n",
    "    # Use a larger trial count for clearer trace/density visuals\n",
    "    n_trials_spaghetti = 100\n",
    "    T_spaghetti = 200\n",
    "\n",
    "    baseline_X, baseline_I, subsidy_X, subsidy_I, baseline_df2, subsidy_df2 = collect_intervention_trials(\n",
    "        n_trials=n_trials_spaghetti,\n",
    "        T=T_spaghetti,\n",
    "        scenario_kwargs=scenario,\n",
    "        subsidy_params=subsidy,\n",
    "        max_workers=max_workers,\n",
    "        seed_base=seed_base,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "    traces_df = traces_to_long_df(baseline_X, subsidy_X)\n",
    "    spaghetti_path = plot_spaghetti(traces_df, max_traces=100, alpha=0.15)\n",
    "    print(\"Saved spaghetti plot:\", spaghetti_path)\n",
    "\n",
    "    density_path = plot_density(traces_df, x_bins=50, time_bins=T_spaghetti)\n",
    "    print(\"Saved time-evolving density plot:\", density_path)\n",
    "\n",
    "    # Ratio sweep computed to DF then plotted\n",
    "    sweep_df = ratio_sweep_df(\n",
    "        X0_frac=scenario.get(\"X0_frac\", 0.40),\n",
    "        ratio_values=np.linspace(0.8, 3.5, 31),\n",
    "        scenario_kwargs=scenario,\n",
    "        T=200,\n",
    "        batch_size=8,\n",
    "        strategy_choice_func=\"logit\",\n",
    "        tau=1.0,\n",
    "    )\n",
    "    sweep_path = plot_ratio_sweep(sweep_df)\n",
    "    print(\"Saved ratio sweep plot:\", sweep_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807667b3",
   "metadata": {},
   "source": [
    "# Generating X0-Payoff Ratio Heatmaps & Spaghetti Plots for Each Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6796c11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline DF shape: (200, 6)\n",
      "Subsidy DF shape: (200, 6)\n",
      "Saved image: c:\\Users\\aryew\\OneDrive\\文档\\Master's Complex Systems & Policy\\Model-Based Decision Making\\Lab-Model-Based-Decision-Making\\Assignment_3\\plots\\ev_intervention_fanchart.png\n",
      "Baseline final X_mean: 0.23655555555555557\n",
      "Subsidy  final X_mean: 0.23655555555555557\n",
      "Saved phase plot: c:\\Users\\aryew\\OneDrive\\文档\\Master's Complex Systems & Policy\\Model-Based Decision Making\\Lab-Model-Based-Decision-Making\\Assignment_3\\plots\\ev_phase_plot.png\n",
      "Saved spaghetti plot: c:\\Users\\aryew\\OneDrive\\文档\\Master's Complex Systems & Policy\\Model-Based Decision Making\\Lab-Model-Based-Decision-Making\\Assignment_3\\plots\\ev_spaghetti.png\n",
      "Saved time-evolving density plot: c:\\Users\\aryew\\OneDrive\\文档\\Master's Complex Systems & Policy\\Model-Based Decision Making\\Lab-Model-Based-Decision-Making\\Assignment_3\\plots\\ev_density.png\n",
      "Saved ratio sweep plot: c:\\Users\\aryew\\OneDrive\\文档\\Master's Complex Systems & Policy\\Model-Based Decision Making\\Lab-Model-Based-Decision-Making\\Assignment_3\\plots\\ev_ratio_sweep.png\n"
     ]
    }
   ],
   "source": [
    "# ER\n",
    "\n",
    "def main():\n",
    "    # Defaults aligned with original ev_stag_mesa_model.run_intervention_example\n",
    "    n_trials = 30  # use fewer than 500 for speed while keeping shape\n",
    "    T = 200\n",
    "    strategy_choice_func = \"logit\"\n",
    "    tau = 1.0\n",
    "    max_workers = 1\n",
    "    seed_base = 100\n",
    "\n",
    "    scenario = dict(\n",
    "        # Preserve initial ratio by computing a0 from ratio, matching the original\n",
    "        ratio=2.0,\n",
    "        beta_I=1.5,\n",
    "        b=2.0,\n",
    "        g_I=0.05,\n",
    "        I0=0.1,\n",
    "        network_type=\"ER\",\n",
    "        n_nodes=300,\n",
    "        m=2,\n",
    "        collect=True,\n",
    "        X0_frac=0.40,\n",
    "        init_method=\"random\",\n",
    "        # ER-specific `p` ignored for BA but kept for completeness\n",
    "        p=0.1,\n",
    "    )\n",
    "    subsidy = dict(start=10, end=60, delta_a0=0.4, delta_beta_I=0.0)\n",
    "\n",
    "    baseline_df, subsidy_df, img_path = run_intervention_example(\n",
    "        n_trials=n_trials,\n",
    "        T=T,\n",
    "        scenario_kwargs=scenario,\n",
    "        subsidy_params=subsidy,\n",
    "        max_workers=max_workers,\n",
    "        seed_base=seed_base,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "\n",
    "    print(\"Baseline DF shape:\", baseline_df.shape)\n",
    "    print(\"Subsidy DF shape:\", subsidy_df.shape)\n",
    "    print(\"Saved image:\", img_path)\n",
    "    print(\"Baseline final X_mean:\", float(baseline_df[\"X_mean\"].iloc[-1]))\n",
    "    print(\"Subsidy  final X_mean:\", float(subsidy_df[\"X_mean\"].iloc[-1]))\n",
    "\n",
    "    # Also run the phase plot of X* over (X0, a_I/b) and save it\n",
    "    phase_df = phase_sweep_df(\n",
    "        max_workers=1,\n",
    "        backend=\"thread\",\n",
    "        X0_values=np.linspace(0.0, 1.0, 21),\n",
    "        ratio_values=np.linspace(0.8, 3.5, 31),\n",
    "        batch_size=8,\n",
    "        T=200,\n",
    "        strategy_choice_func=\"logit\",\n",
    "        tau=1.0,\n",
    "    )\n",
    "    phase_path = plot_phase_plot(phase_df)\n",
    "    print(\"Saved phase plot:\", phase_path)\n",
    "\n",
    "    # Spaghetti and time-evolving density plots\n",
    "    # Use a larger trial count for clearer trace/density visuals\n",
    "    n_trials_spaghetti = 100\n",
    "    T_spaghetti = 200\n",
    "\n",
    "    baseline_X, baseline_I, subsidy_X, subsidy_I, baseline_df2, subsidy_df2 = collect_intervention_trials(\n",
    "        n_trials=n_trials_spaghetti,\n",
    "        T=T_spaghetti,\n",
    "        scenario_kwargs=scenario,\n",
    "        subsidy_params=subsidy,\n",
    "        max_workers=max_workers,\n",
    "        seed_base=seed_base,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "    traces_df = traces_to_long_df(baseline_X, subsidy_X)\n",
    "    spaghetti_path = plot_spaghetti(traces_df, max_traces=100, alpha=0.15)\n",
    "    print(\"Saved spaghetti plot:\", spaghetti_path)\n",
    "\n",
    "    density_path = plot_density(traces_df, x_bins=50, time_bins=T_spaghetti)\n",
    "    print(\"Saved time-evolving density plot:\", density_path)\n",
    "\n",
    "    # Ratio sweep computed to DF then plotted\n",
    "    sweep_df = ratio_sweep_df(\n",
    "        X0_frac=scenario.get(\"X0_frac\", 0.40),\n",
    "        ratio_values=np.linspace(0.8, 3.5, 31),\n",
    "        scenario_kwargs=scenario,\n",
    "        T=200,\n",
    "        batch_size=8,\n",
    "        strategy_choice_func=\"logit\",\n",
    "        tau=1.0,\n",
    "    )\n",
    "    sweep_path = plot_ratio_sweep(sweep_df)\n",
    "    print(\"Saved ratio sweep plot:\", sweep_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed1db77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BA\n",
    "\n",
    "def main():\n",
    "    # Defaults aligned with original ev_stag_mesa_model.run_intervention_example\n",
    "    n_trials = 30  # use fewer than 500 for speed while keeping shape\n",
    "    T = 200\n",
    "    strategy_choice_func = \"logit\"\n",
    "    tau = 1.0\n",
    "    max_workers = 1\n",
    "    seed_base = 100\n",
    "\n",
    "    scenario = dict(\n",
    "        # Preserve initial ratio by computing a0 from ratio, matching the original\n",
    "        ratio=2.0,\n",
    "        beta_I=1.5,\n",
    "        b=2.0,\n",
    "        g_I=0.05,\n",
    "        I0=0.1,\n",
    "        network_type=\"BA\",\n",
    "        n_nodes=300,\n",
    "        m=2,\n",
    "        collect=True,\n",
    "        X0_frac=0.40,\n",
    "        init_method=\"random\",\n",
    "        # ER-specific `p` ignored for BA but kept for completeness\n",
    "        p=0.1,\n",
    "    )\n",
    "    subsidy = dict(start=10, end=60, delta_a0=0.4, delta_beta_I=0.0)\n",
    "\n",
    "    baseline_df, subsidy_df, img_path = run_intervention_example(\n",
    "        n_trials=n_trials,\n",
    "        T=T,\n",
    "        scenario_kwargs=scenario,\n",
    "        subsidy_params=subsidy,\n",
    "        max_workers=max_workers,\n",
    "        seed_base=seed_base,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "\n",
    "    print(\"Baseline DF shape:\", baseline_df.shape)\n",
    "    print(\"Subsidy DF shape:\", subsidy_df.shape)\n",
    "    print(\"Saved image:\", img_path)\n",
    "    print(\"Baseline final X_mean:\", float(baseline_df[\"X_mean\"].iloc[-1]))\n",
    "    print(\"Subsidy  final X_mean:\", float(subsidy_df[\"X_mean\"].iloc[-1]))\n",
    "\n",
    "    # Also run the phase plot of X* over (X0, a_I/b) and save it\n",
    "    phase_df = phase_sweep_df(\n",
    "        max_workers=1,\n",
    "        backend=\"thread\",\n",
    "        X0_values=np.linspace(0.0, 1.0, 21),\n",
    "        ratio_values=np.linspace(0.8, 3.5, 31),\n",
    "        batch_size=8,\n",
    "        T=200,\n",
    "        strategy_choice_func=\"logit\",\n",
    "        tau=1.0,\n",
    "    )\n",
    "    phase_path = plot_phase_plot(phase_df)\n",
    "    print(\"Saved phase plot:\", phase_path)\n",
    "\n",
    "    # Spaghetti and time-evolving density plots\n",
    "    # Use a larger trial count for clearer trace/density visuals\n",
    "    n_trials_spaghetti = 100\n",
    "    T_spaghetti = 200\n",
    "\n",
    "    baseline_X, baseline_I, subsidy_X, subsidy_I, baseline_df2, subsidy_df2 = collect_intervention_trials(\n",
    "        n_trials=n_trials_spaghetti,\n",
    "        T=T_spaghetti,\n",
    "        scenario_kwargs=scenario,\n",
    "        subsidy_params=subsidy,\n",
    "        max_workers=max_workers,\n",
    "        seed_base=seed_base,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "    traces_df = traces_to_long_df(baseline_X, subsidy_X)\n",
    "    spaghetti_path = plot_spaghetti(traces_df, max_traces=100, alpha=0.15)\n",
    "    print(\"Saved spaghetti plot:\", spaghetti_path)\n",
    "\n",
    "    density_path = plot_density(traces_df, x_bins=50, time_bins=T_spaghetti)\n",
    "    print(\"Saved time-evolving density plot:\", density_path)\n",
    "\n",
    "    # Ratio sweep computed to DF then plotted\n",
    "    sweep_df = ratio_sweep_df(\n",
    "        X0_frac=scenario.get(\"X0_frac\", 0.40),\n",
    "        ratio_values=np.linspace(0.8, 3.5, 31),\n",
    "        scenario_kwargs=scenario,\n",
    "        T=200,\n",
    "        batch_size=8,\n",
    "        strategy_choice_func=\"logit\",\n",
    "        tau=1.0,\n",
    "    )\n",
    "    sweep_path = plot_ratio_sweep(sweep_df)\n",
    "    print(\"Saved ratio sweep plot:\", sweep_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a09df3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline DF shape: (200, 6)\n",
      "Subsidy DF shape: (200, 6)\n",
      "Saved image: c:\\Users\\aryew\\OneDrive\\文档\\Master's Complex Systems & Policy\\Model-Based Decision Making\\Lab-Model-Based-Decision-Making\\Assignment_3\\plots\\ev_intervention_fanchart.png\n",
      "Baseline final X_mean: 0.9\n",
      "Subsidy  final X_mean: 0.9\n",
      "Saved phase plot: c:\\Users\\aryew\\OneDrive\\文档\\Master's Complex Systems & Policy\\Model-Based Decision Making\\Lab-Model-Based-Decision-Making\\Assignment_3\\plots\\ev_phase_plot.png\n",
      "Saved spaghetti plot: c:\\Users\\aryew\\OneDrive\\文档\\Master's Complex Systems & Policy\\Model-Based Decision Making\\Lab-Model-Based-Decision-Making\\Assignment_3\\plots\\ev_spaghetti.png\n",
      "Saved time-evolving density plot: c:\\Users\\aryew\\OneDrive\\文档\\Master's Complex Systems & Policy\\Model-Based Decision Making\\Lab-Model-Based-Decision-Making\\Assignment_3\\plots\\ev_density.png\n",
      "Saved ratio sweep plot: c:\\Users\\aryew\\OneDrive\\文档\\Master's Complex Systems & Policy\\Model-Based Decision Making\\Lab-Model-Based-Decision-Making\\Assignment_3\\plots\\ev_ratio_sweep.png\n"
     ]
    }
   ],
   "source": [
    "# WS\n",
    "\n",
    "def main():\n",
    "    # Defaults aligned with original ev_stag_mesa_model.run_intervention_example\n",
    "    n_trials = 30  # use fewer than 500 for speed while keeping shape\n",
    "    T = 200\n",
    "    strategy_choice_func = \"logit\"\n",
    "    tau = 1.0\n",
    "    max_workers = 1\n",
    "    seed_base = 100\n",
    "\n",
    "    scenario = dict(\n",
    "        # Preserve initial ratio by computing a0 from ratio, matching the original\n",
    "        ratio=2.0,\n",
    "        beta_I=1.5,\n",
    "        b=2.0,\n",
    "        g_I=0.05,\n",
    "        I0=0.1,\n",
    "        network_type=\"WS\",\n",
    "        n_nodes=300,\n",
    "        m=2,\n",
    "        k=30,\n",
    "        collect=True,\n",
    "        X0_frac=0.40,\n",
    "        init_method=\"random\",\n",
    "        # ER-specific `p` ignored for BA but kept for completeness\n",
    "        p=0.1,\n",
    "    )\n",
    "    subsidy = dict(start=10, end=60, delta_a0=0.4, delta_beta_I=0.0)\n",
    "\n",
    "    baseline_df, subsidy_df, img_path = run_intervention_example(\n",
    "        n_trials=n_trials,\n",
    "        T=T,\n",
    "        scenario_kwargs=scenario,\n",
    "        subsidy_params=subsidy,\n",
    "        max_workers=max_workers,\n",
    "        seed_base=seed_base,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "\n",
    "    print(\"Baseline DF shape:\", baseline_df.shape)\n",
    "    print(\"Subsidy DF shape:\", subsidy_df.shape)\n",
    "    print(\"Saved image:\", img_path)\n",
    "    print(\"Baseline final X_mean:\", float(baseline_df[\"X_mean\"].iloc[-1]))\n",
    "    print(\"Subsidy  final X_mean:\", float(subsidy_df[\"X_mean\"].iloc[-1]))\n",
    "\n",
    "    # Also run the phase plot of X* over (X0, a_I/b) and save it\n",
    "    phase_df = phase_sweep_df(\n",
    "        max_workers=1,\n",
    "        backend=\"thread\",\n",
    "        X0_values=np.linspace(0.0, 1.0, 21),\n",
    "        ratio_values=np.linspace(0.8, 3.5, 31),\n",
    "        batch_size=8,\n",
    "        T=200,\n",
    "        strategy_choice_func=\"logit\",\n",
    "        tau=1.0,\n",
    "    )\n",
    "    phase_path = plot_phase_plot(phase_df)\n",
    "    print(\"Saved phase plot:\", phase_path)\n",
    "\n",
    "    # Spaghetti and time-evolving density plots\n",
    "    # Use a larger trial count for clearer trace/density visuals\n",
    "    n_trials_spaghetti = 100\n",
    "    T_spaghetti = 200\n",
    "\n",
    "    baseline_X, baseline_I, subsidy_X, subsidy_I, baseline_df2, subsidy_df2 = collect_intervention_trials(\n",
    "        n_trials=n_trials_spaghetti,\n",
    "        T=T_spaghetti,\n",
    "        scenario_kwargs=scenario,\n",
    "        subsidy_params=subsidy,\n",
    "        max_workers=max_workers,\n",
    "        seed_base=seed_base,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "    traces_df = traces_to_long_df(baseline_X, subsidy_X)\n",
    "    spaghetti_path = plot_spaghetti(traces_df, max_traces=100, alpha=0.15)\n",
    "    print(\"Saved spaghetti plot:\", spaghetti_path)\n",
    "\n",
    "    density_path = plot_density(traces_df, x_bins=50, time_bins=T_spaghetti)\n",
    "    print(\"Saved time-evolving density plot:\", density_path)\n",
    "\n",
    "    # Ratio sweep computed to DF then plotted\n",
    "    sweep_df = ratio_sweep_df(\n",
    "        X0_frac=scenario.get(\"X0_frac\", 0.40),\n",
    "        ratio_values=np.linspace(0.8, 3.5, 31),\n",
    "        scenario_kwargs=scenario,\n",
    "        T=200,\n",
    "        batch_size=8,\n",
    "        strategy_choice_func=\"logit\",\n",
    "        tau=1.0,\n",
    "    )\n",
    "    sweep_path = plot_ratio_sweep(sweep_df)\n",
    "    print(\"Saved ratio sweep plot:\", sweep_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c47453",
   "metadata": {},
   "source": [
    "# Evaluating & Plotting Adoption Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e0f8c2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aryew\\AppData\\Local\\Temp\\ipykernel_11420\\978460507.py:41: FutureWarning: \n",
      "\n",
      "The `ci` parameter is deprecated. Use `errorbar='sd'` for the same effect.\n",
      "\n",
      "  sns.lineplot(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Helper: convert list of X(t) arrays into tidy DataFrame with network type ---\n",
    "def traces_to_long_df_networks(baseline_X_dict):\n",
    "    \"\"\"\n",
    "    Convert trajectories for multiple networks to tidy DataFrame:\n",
    "    [network_type, trial, time, X]\n",
    "    baseline_X_dict : dict\n",
    "        Keys = network type string, values = list of np.ndarray trajectories\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for net_type, X_list in baseline_X_dict.items():\n",
    "        for trial, X in enumerate(X_list):\n",
    "            for t, x in enumerate(X):\n",
    "                rows.append((net_type, trial, t, float(x)))\n",
    "    return pd.DataFrame(rows, columns=[\"network_type\", \"trial\", \"time\", \"X\"])\n",
    "\n",
    "\n",
    "# --- Helper: compute dX/dt safely ---\n",
    "def compute_dx_dt_safe(traces_df):\n",
    "    dx_dt_list = []\n",
    "    for (net_type, trial_id), df_trial in traces_df.groupby([\"network_type\", \"trial\"]):\n",
    "        df_trial = df_trial.sort_values(\"time\").drop_duplicates(\"time\")\n",
    "        X = df_trial[\"X\"].values\n",
    "        t = df_trial[\"time\"].values\n",
    "        dt = np.diff(t)\n",
    "        dX = np.diff(X)\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            dXdt = np.where(dt != 0, dX/dt, 0)\n",
    "        t_mid = (t[:-1] + t[1:]) / 2\n",
    "        dx_dt_list.append(pd.DataFrame({\n",
    "            \"network_type\": net_type,\n",
    "            \"trial\": trial_id,\n",
    "            \"time\": t_mid,\n",
    "            \"dX_dt\": dXdt\n",
    "        }))\n",
    "    return pd.concat(dx_dt_list, ignore_index=True)\n",
    "\n",
    "\n",
    "# --- Helper: plot adoption speed ---\n",
    "def plot_dx_dt_comparison(dx_dt_df):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.lineplot(\n",
    "        data=dx_dt_df,\n",
    "        x=\"time\",\n",
    "        y=\"dX_dt\",\n",
    "        hue=\"network_type\",\n",
    "        ci=\"sd\",  # shaded area = standard deviation across trials\n",
    "        linewidth=2\n",
    "    )\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"dX/dt - adoption rate\")\n",
    "    plt.title(\"Comparison of adoption speed across networks\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# === Main workflow ===\n",
    "\n",
    "# Example: run your simulation for three networks and collect baseline_X for each\n",
    "# Replace these lines with your actual calls to collect_intervention_trials per network\n",
    "baseline_X_BA, _, _, _, _, _ = collect_intervention_trials(\n",
    "    n_trials=30, T=75, scenario_kwargs={**scenario, \"network_type\":\"BA\"},\n",
    "    subsidy_params=subsidy, max_workers=1, seed_base=100,\n",
    "    strategy_choice_func=\"logit\", tau=1.0\n",
    ")\n",
    "baseline_X_ER, _, _, _, _, _ = collect_intervention_trials(\n",
    "    n_trials=30, T=75, scenario_kwargs={**scenario, \"network_type\":\"ER\", \"p\":0.1},\n",
    "    subsidy_params=subsidy, max_workers=1, seed_base=101,\n",
    "    strategy_choice_func=\"logit\", tau=1.0\n",
    ")\n",
    "baseline_X_WS, _, _, _, _, _ = collect_intervention_trials(\n",
    "    n_trials=30, T=75, scenario_kwargs={**scenario, \"network_type\":\"WS\", \"p\":0.1},\n",
    "    subsidy_params=subsidy, max_workers=1, seed_base=102,\n",
    "    strategy_choice_func=\"logit\", tau=1.0\n",
    ")\n",
    "\n",
    "# Combine into dictionary for conversion\n",
    "baseline_X_dict = {\"BA\": baseline_X_BA, \"ER\": baseline_X_ER, \"WS\": baseline_X_WS}\n",
    "\n",
    "# Convert to tidy DataFrame\n",
    "traces_df = traces_to_long_df_networks(baseline_X_dict)\n",
    "\n",
    "# Compute dx/dt\n",
    "dx_dt_df = compute_dx_dt_safe(traces_df)\n",
    "\n",
    "# Plot adoption speed\n",
    "plot_dx_dt_comparison(dx_dt_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ff08b6",
   "metadata": {},
   "source": [
    "# Probability of Reaching High Adoption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4ac411c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aryew\\AppData\\Local\\Temp\\ipykernel_11420\\3544937114.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  high_adoption_prob_df = final_X_df.groupby(\"network_type\").apply(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  network_type  high_adoption_prob\n",
      "0           BA                 0.3\n",
      "1           ER                 0.2\n",
      "2           WS                 0.9\n"
     ]
    }
   ],
   "source": [
    "def compute_high_adoption_prob(traces_df, threshold=0.9, plot=True):\n",
    "    \"\"\"\n",
    "    Compute probability of reaching high adoption per network.\n",
    "    \"\"\"\n",
    "    final_X_df = traces_df.groupby([\"network_type\", \"trial\"]).agg(final_X=(\"X\", \"last\")).reset_index()\n",
    "    high_adoption_prob_df = final_X_df.groupby(\"network_type\").apply(\n",
    "        lambda df: (df[\"final_X\"] >= threshold).mean()\n",
    "    ).reset_index(name=\"high_adoption_prob\")\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.bar(high_adoption_prob_df[\"network_type\"], high_adoption_prob_df[\"high_adoption_prob\"], color=['blue','green','orange'])\n",
    "        plt.ylabel(f\"Probability of X >= {threshold}\")\n",
    "        plt.xlabel(\"Network type\")\n",
    "        plt.title(\"Probability of reaching high adoption\")\n",
    "        plt.ylim(0, 1)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return high_adoption_prob_df\n",
    "\n",
    "# Usage\n",
    "high_adoption_probs = compute_high_adoption_prob(traces_df, threshold=0.9)\n",
    "print(high_adoption_probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
