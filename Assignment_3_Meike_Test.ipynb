{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12eb4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed packages \n",
    "from mesa import Agent, Model\n",
    "from mesa.time import SimultaneousActivation\n",
    "from mesa.space import NetworkGrid\n",
    "from mesa.datacollection import DataCollector\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Callable, Iterable, List, Dict, Optional, Tuple\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c241833b",
   "metadata": {},
   "source": [
    "# Strategy update rules #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de309ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_strategy_logit(agent, neighbors, a_I, b, tau):\n",
    "    \"\"\"Choose strategy using logit / softmax choice.\n",
    "\n",
    "    Parameters\n",
    "    - agent: the agent choosing a strategy\n",
    "    - neighbors: list of neighbour agents\n",
    "    - a_I: effective coordination payoff given current infrastructure\n",
    "    - b: defection payoff\n",
    "    - tau: temperature parameter for softmax\n",
    "    \"\"\"\n",
    "    # compute expected payoffs for C and D\n",
    "    pi_C = 0.0\n",
    "    pi_D = 0.0\n",
    "    for other in neighbors:\n",
    "        s_j = other.strategy\n",
    "        if s_j == \"C\":\n",
    "            pi_C += a_I\n",
    "            pi_D += b\n",
    "        else:\n",
    "            pi_C += 0.0\n",
    "            pi_D += b\n",
    "\n",
    "    # softmax choice\n",
    "    denom = np.exp(pi_C / tau) + np.exp(pi_D / tau)\n",
    "    P_C = np.exp(pi_C / tau) / denom if denom > 0 else 0.5\n",
    "    return \"C\" if random.random() < P_C else \"D\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e943f0",
   "metadata": {},
   "source": [
    "# The Agent Class #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc0be1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EVAgent(Agent):\n",
    "    \"\"\"Single agent at a graph node.\n",
    "\n",
    "    Attributes\n",
    "    - strategy: \"C\" (adopt EV) or \"D\" (defect / ICE)\n",
    "    - payoff: accumulated payoff from interactions with neighbours\n",
    "    - next_strategy: strategy chosen for the next time step\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, unique_id, model, init_strategy=\"D\"):\n",
    "        super().__init__(unique_id, model)\n",
    "        self.strategy = init_strategy\n",
    "        self.payoff = 0.0\n",
    "        self.next_strategy = init_strategy\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Compute payoff from interactions with neighbours.\n",
    "\n",
    "        Stag Hunt payoff rules:\n",
    "        - C vs C: `a_I` (coordination enhanced by infrastructure)\n",
    "        - C vs D: 0\n",
    "        - D vs C: `b`\n",
    "        - D vs D: `b`\n",
    "        \"\"\"\n",
    "        I = self.model.infrastructure\n",
    "        a0 = self.model.a0\n",
    "        beta_I = self.model.beta_I\n",
    "        b = self.model.b\n",
    "        a_I = a0 + beta_I * I\n",
    "\n",
    "        neighbor_agents = []\n",
    "        for nbr in self.model.G.neighbors(self.pos):\n",
    "            neighbor_agents.extend(self.model.grid.get_cell_list_contents([nbr]))\n",
    "        if not neighbor_agents:\n",
    "            self.payoff = 0.0\n",
    "            return\n",
    "\n",
    "        payoff = 0.0\n",
    "        for other in neighbor_agents:\n",
    "            s_i = self.strategy\n",
    "            s_j = other.strategy\n",
    "            if s_i == \"C\" and s_j == \"C\":\n",
    "                payoff += a_I\n",
    "            elif s_i == \"C\" and s_j == \"D\":\n",
    "                payoff += 0.0\n",
    "            elif s_i == \"D\" and s_j == \"C\":\n",
    "                payoff += b\n",
    "            else:\n",
    "                payoff += b\n",
    "        self.payoff = payoff\n",
    "\n",
    "    ####################################\n",
    "    # Advance method\n",
    "    #\n",
    "    # The advance method updates the agent's strategy based on the selected rule.\n",
    "    #\n",
    "    # Parameters\n",
    "    # - strategy_choice_func: the strategy selection function to use (\"logit\")\n",
    "    ####################################\n",
    "    def advance(self, strategy_choice_func=\"logit\"):\n",
    "        \"\"\"Update next_strategy using the selected rule.\n",
    "\n",
    "        If called without an explicit rule, read `self.model.strategy_choice_func`.\n",
    "        Commit `self.strategy = self.next_strategy` for synchronous updates.\n",
    "        \"\"\"\n",
    "        # Default to logit, and treat any value (\"imitate\" or \"logit\") as logit.\n",
    "        func = strategy_choice_func if strategy_choice_func is not None else getattr(\n",
    "            self.model,\n",
    "            \"strategy_choice_func\",\n",
    "            \"logit\"\n",
    "        )\n",
    "\n",
    "        neighbor_agents = []\n",
    "        for nbr in self.model.G.neighbors(self.pos):\n",
    "            neighbor_agents.extend(self.model.grid.get_cell_list_contents([nbr]))\n",
    "\n",
    "        if func in (\"imitate\", \"logit\"):\n",
    "            # Always use logit as the actual update rule\n",
    "            a_I = self.model.a0 + self.model.beta_I * self.model.infrastructure\n",
    "            self.next_strategy = choose_strategy_logit(\n",
    "                self,\n",
    "                neighbor_agents,\n",
    "                a_I,\n",
    "                self.model.b,\n",
    "                getattr(self.model, \"tau\", 1.0),\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy choice function: {func}\")\n",
    "\n",
    "        # Commit the update for SimultaneousActivation\n",
    "        self.strategy = self.next_strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa036705",
   "metadata": {},
   "source": [
    "# EV Stag Hunt Model Implementation # \n",
    "with Different Network Types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19bf2d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EVStagHuntModel(Model):\n",
    "    \"\"\"Mesa model for EV Stag Hunt on a network.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        initial_ev=10,\n",
    "        a0=2.0,\n",
    "        beta_I=3.0,\n",
    "        b=1.0,\n",
    "        g_I=0.1,\n",
    "        I0=0.05,\n",
    "        seed=None,\n",
    "        network_type=\"random\",   # if nothing is selected, this default -> random network\n",
    "        n_nodes=100,\n",
    "        p=0.05,\n",
    "        m=2,\n",
    "        collect=True,\n",
    "        strategy_choice_func: str = \"imitate\",\n",
    "        tau: float = 1.0,\n",
    "    ):\n",
    "        super().__init__(seed=seed)\n",
    "\n",
    "        # Build graph\n",
    "        # Note: if network_type is not \"BA\" or \"SW\", an Erdős–Rényi random network is used.\n",
    "        if network_type == \"BA\":\n",
    "            # Barabási–Albert scale-free network\n",
    "            G = nx.barabasi_albert_graph(n_nodes, m, seed=seed)\n",
    "        elif network_type == \"SW\":\n",
    "            # Watts–Strogatz small-world network\n",
    "            # m is used as k (number of nearest neighbours), p is rewiring probability\n",
    "            G = nx.watts_strogatz_graph(n_nodes, k=m, p=p, seed=seed)\n",
    "        else:\n",
    "            # Default: Erdős–Rényi random network\n",
    "            G = nx.erdos_renyi_graph(n_nodes, p, seed=seed)\n",
    "\n",
    "        self.G = G\n",
    "        self.grid = NetworkGrid(G)\n",
    "        self.schedule = SimultaneousActivation(self)\n",
    "\n",
    "        # parameters\n",
    "        self.a0 = a0\n",
    "        self.beta_I = beta_I\n",
    "        self.b = b\n",
    "        self.g_I = g_I\n",
    "        self.infrastructure = I0\n",
    "        self.step_count = 0\n",
    "        self.strategy_choice_func = strategy_choice_func\n",
    "        self.tau = tau\n",
    "\n",
    "        # initialize node attribute for agent reference\n",
    "        for n in self.G.nodes:\n",
    "            self.G.nodes[n][\"agent\"] = []\n",
    "\n",
    "        # choose initial EV nodes\n",
    "        total_nodes = self.G.number_of_nodes()\n",
    "        k_ev = max(0, min(initial_ev, total_nodes))\n",
    "        ev_nodes = set(self.random.sample(list(self.G.nodes), k_ev))\n",
    "\n",
    "        # create one agent per node\n",
    "        uid = 0\n",
    "        for node in self.G.nodes:\n",
    "            init_strategy = \"C\" if node in ev_nodes else \"D\"\n",
    "            agent = EVAgent(uid, self, init_strategy)\n",
    "            uid += 1\n",
    "            self.schedule.add(agent)\n",
    "            self.grid.place_agent(agent, node)\n",
    "\n",
    "        self.datacollector = None\n",
    "        if collect:\n",
    "            self.datacollector = DataCollector(\n",
    "                model_reporters={\n",
    "                    \"X\": self.get_adoption_fraction,\n",
    "                    \"I\": lambda m: m.infrastructure,\n",
    "                },\n",
    "                agent_reporters={\"strategy\": \"strategy\", \"payoff\": \"payoff\"},\n",
    "            )\n",
    "\n",
    "    def get_adoption_fraction(self):\n",
    "        agents = self.schedule.agents\n",
    "        if not agents:\n",
    "            return 0.0\n",
    "        return sum(1 for a in agents if a.strategy == \"C\") / len(agents)\n",
    "\n",
    "    #######################\n",
    "    # Model step function\n",
    "    #\n",
    "    # The step function advances the model by one time step.\n",
    "    # It first advances all agents, then computes the adoption fraction and infrastructure level.\n",
    "    # The infrastructure level is updated based on the adoption fraction and the infrastructure growth rate.\n",
    "    # The updated infrastructure level is clipped to the interval [0, 1].\n",
    "    # Finally, if data collection is enabled, the model and agent data are collected.\n",
    "    #######################\n",
    "    def step(self): \n",
    "        self.schedule.step()  # advance all agents\n",
    "        X = self.get_adoption_fraction()  # compute adoption fraction after all agents have advanced\n",
    "        I = self.infrastructure  # infrastructure level before this step\n",
    "        dI = self.g_I * (X - I)  # infrastructure growth rate, impacted by adoption fraction\n",
    "        self.infrastructure = float(min(1.0, max(0.0, I + dI)))  # clip infrastructure level to [0, 1]\n",
    "        if self.datacollector is not None:\n",
    "            self.datacollector.collect(self)  # collect data at the end of each step\n",
    "        self.step_count += 1  # increment step count after data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8360ed74",
   "metadata": {},
   "source": [
    "# Initial Adopter Selection: Random #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a1b4df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# Set initial adopters \n",
    "#\n",
    "# Parameters\n",
    "# - model: the EVStagHuntModel instance\n",
    "# - X0_frac: fraction of agents to initially choose EV adoption\n",
    "# - seed: random seed for reproducibility\n",
    "###########################\n",
    "def set_initial_adopters(model, X0_frac, seed=None):\n",
    "    \"\"\"Set a fraction of agents to EV adopters using random selection.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    agents = model.schedule.agents\n",
    "    n = len(agents)\n",
    "    \n",
    "    # number of adopters\n",
    "    k = int(round(X0_frac * n))\n",
    "    \n",
    "    # reset all to D\n",
    "    for a in agents:\n",
    "        a.strategy = \"D\"\n",
    "    \n",
    "    if k <= 0:\n",
    "        return\n",
    "    \n",
    "    # choose k random agents\n",
    "    idx = rng.choice(n, size=k, replace=False)\n",
    "    for i in idx:\n",
    "        agents[i].strategy = \"C\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21624f81",
   "metadata": {},
   "source": [
    "# Run Network #\n",
    "Runs network and returns final adoption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f2976ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Ratio sweep helpers (computation-only)\n",
    "# -----------------------------\n",
    "#########################\n",
    "#\n",
    "# Run a single network trial\n",
    "# \n",
    "# Parameters\n",
    "# - X0_frac: fraction of agents to initially choose EV adoption\n",
    "# - ratio: payoff ratio between EV and DC agents (a0 = ratio*b - beta_I*I0)\n",
    "# - I0: initial infrastructure level\n",
    "# - beta_I: cost of EV adoption relative to DC (beta_I*I0)\n",
    "# - b: payoff of EV (b)\n",
    "# - g_I: infrastructure growth rate (g_I)\n",
    "# - T: number of time steps to run\n",
    "# - network_type: type of network to generate (\"random\", \"BA\", or \"SW\")\n",
    "# - n_nodes: number of nodes in the network\n",
    "# - p: probability of edge creation in random network / rewiring prob in SW\n",
    "# - m: number of edges to attach from a new node to existing nodes in BA network /\n",
    "#      k (number of nearest neighbours) in SW network\n",
    "# - seed: random seed for reproducibility\n",
    "# - tol: tolerance for convergence check (default: 1e-3)\n",
    "# - patience: number of steps to wait for convergence (default: 30)\n",
    "def run_network_trial(\n",
    "    X0_frac: float,\n",
    "    ratio: float,\n",
    "    *,\n",
    "    I0: float = 0.05,\n",
    "    beta_I: float = 2.0,\n",
    "    b: float = 1.0,\n",
    "    g_I: float = 0.05,\n",
    "    T: int = 200,\n",
    "    network_type: str = \"random\", #choose network type here\n",
    "    n_nodes: int = 120,\n",
    "    p: float = 0.05,\n",
    "    m: int = 2,\n",
    "    seed: int | None = None,\n",
    "    tol: float = 1e-3,\n",
    "    patience: int = 30,\n",
    "    collect: bool = False,\n",
    "    strategy_choice_func: str = \"logit\",  \n",
    "    tau: float = 1.0,\n",
    ") -> float:\n",
    "    \"\"\"Run a single realisation and return final adoption fraction.\n",
    "\n",
    "    Preserves the intended initial payoff ratio via a0 = ratio*b - beta_I*I0.\n",
    "    Includes basic stability-based early stopping.\n",
    "    \"\"\"\n",
    "    initial_ev = int(round(X0_frac * n_nodes))\n",
    "    a0 = ratio * b - beta_I * I0\n",
    "\n",
    "    model = EVStagHuntModel(\n",
    "        initial_ev=initial_ev,\n",
    "        a0=a0,\n",
    "        beta_I=beta_I,\n",
    "        b=b,\n",
    "        g_I=g_I,\n",
    "        I0=I0,\n",
    "        seed=seed,\n",
    "        network_type=network_type,\n",
    "        n_nodes=n_nodes,\n",
    "        p=p,\n",
    "        m=m,\n",
    "        collect=collect,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "\n",
    "    stable_steps = 0\n",
    "    prev_X = None\n",
    "    prev_I = None\n",
    "    for _ in range(T):\n",
    "        model.step()\n",
    "        X = model.get_adoption_fraction()\n",
    "        I = model.infrastructure\n",
    "        if prev_X is not None and prev_I is not None:\n",
    "            if abs(X - prev_X) < tol and abs(I - prev_I) < tol:\n",
    "                stable_steps += 1\n",
    "            else:\n",
    "                stable_steps = 0\n",
    "        prev_X, prev_I = X, I\n",
    "        if X in (0.0, 1.0) and stable_steps >= 10:\n",
    "            break\n",
    "        if stable_steps >= patience:\n",
    "            break\n",
    "\n",
    "    return model.get_adoption_fraction()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54d3f70",
   "metadata": {},
   "source": [
    "# Final adoption vs payoff ratio #\n",
    "This function runs multiple simulations for each payoff ratio, computes the final EV adoption in each run, and returns the mean adoption level for every ratio. This allows us to estimate how EV uptake depends on incentive strength and to detect tipping points in adoption dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a50463ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "# Compute final mean adoption fraction vs ratio\n",
    "# \n",
    "##########################\n",
    "def final_mean_adoption_vs_ratio(\n",
    "    X0_frac: float,\n",
    "    ratio_values: Iterable[float],\n",
    "    *,\n",
    "    I0: float = 0.05,\n",
    "    beta_I: float = 2.0,\n",
    "    b: float = 1.0,\n",
    "    g_I: float = 0.05,\n",
    "    T: int = 200,\n",
    "    network_type: str = \"random\",  # \"random\", \"BA\", or \"SW\"\n",
    "    n_nodes: int = 120,\n",
    "    p: float = 0.05,\n",
    "    m: int = 2,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    strategy_choice_func: str = \"logit\",  # default changed from \"imitate\" to \"logit\"\n",
    "    tau: float = 1.0,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute mean final adoption across a sweep of ratio values.\n",
    "\n",
    "    For each ratio, average over `batch_size` trials with jittered `I0` and seeds.\n",
    "    Returns a numpy array of means aligned with `ratio_values` order.\n",
    "    \"\"\"\n",
    "    ratios = list(ratio_values)\n",
    "    means: List[float] = []\n",
    "    for ratio in ratios:\n",
    "        finals: List[float] = []\n",
    "        for _ in range(batch_size):\n",
    "            I0_j = float(np.clip(np.random.normal(loc=I0, scale=init_noise_I), 0.0, 1.0))\n",
    "            seed_j = np.random.randint(0, 2**31 - 1)\n",
    "            x_star = run_network_trial(\n",
    "                X0_frac,\n",
    "                ratio,\n",
    "                I0=I0_j,\n",
    "                beta_I=beta_I,\n",
    "                b=b,\n",
    "                g_I=g_I,\n",
    "                T=T,\n",
    "                network_type=network_type,\n",
    "                n_nodes=n_nodes,\n",
    "                p=p,\n",
    "                m=m,\n",
    "                seed=seed_j,\n",
    "                collect=False,\n",
    "                strategy_choice_func=strategy_choice_func,\n",
    "                tau=tau,\n",
    "            )\n",
    "            finals.append(x_star)\n",
    "        means.append(float(np.mean(finals)))\n",
    "    return np.asarray(means, dtype=float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a161f5",
   "metadata": {},
   "source": [
    "# Generating Heatmap Rows #\n",
    "Adoption vs Initial Conditions for a Fixed Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaebe8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "# Compute final mean adoption fraction vs ratio\n",
    "# \n",
    "##########################\n",
    "def final_mean_adoption_vs_ratio(\n",
    "    X0_frac: float,\n",
    "    ratio_values: Iterable[float],\n",
    "    *,\n",
    "    I0: float = 0.05,\n",
    "    beta_I: float = 2.0,\n",
    "    b: float = 1.0,\n",
    "    g_I: float = 0.05,\n",
    "    T: int = 200,\n",
    "    network_type: str = \"random\",   # supports \"random\", \"BA\", \"SW\"\n",
    "    n_nodes: int = 120,\n",
    "    p: float = 0.05,\n",
    "    m: int = 2,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    strategy_choice_func: str = \"logit\",   # <-- changed from \"imitate\"\n",
    "    tau: float = 1.0,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute mean final EV adoption across a sweep of payoff ratios.\n",
    "\n",
    "    For each ratio in ratio_values:\n",
    "        - Run `batch_size` independent simulations\n",
    "        - Each simulation randomly perturbs initial infrastructure I0 (noise)\n",
    "        - Each uses a random seed for agent decisions and network processes\n",
    "        - Record the final adoption fraction\n",
    "    Return a numpy array of mean adoption values (same ordering as ratio_values).\n",
    "    \"\"\"\n",
    "    ratios = list(ratio_values)\n",
    "    means: List[float] = []\n",
    "\n",
    "    for ratio in ratios:\n",
    "        finals: List[float] = []\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            # Add small noise to initial infrastructure for robustness\n",
    "            I0_j = float(np.clip(np.random.normal(loc=I0, scale=init_noise_I), 0.0, 1.0))\n",
    "\n",
    "            # Fresh random seed for each run\n",
    "            seed_j = np.random.randint(0, 2**31 - 1)\n",
    "\n",
    "            # Run a single simulation\n",
    "            x_star = run_network_trial(\n",
    "                X0_frac,\n",
    "                ratio,\n",
    "                I0=I0_j,\n",
    "                beta_I=beta_I,\n",
    "                b=b,\n",
    "                g_I=g_I,\n",
    "                T=T,\n",
    "                network_type=network_type,\n",
    "                n_nodes=n_nodes,\n",
    "                p=p,\n",
    "                m=m,\n",
    "                seed=seed_j,\n",
    "                collect=False,\n",
    "                strategy_choice_func=strategy_choice_func,\n",
    "                tau=tau,\n",
    "            )\n",
    "            finals.append(x_star)\n",
    "\n",
    "        # Average final adoption across runs\n",
    "        means.append(float(np.mean(finals)))\n",
    "\n",
    "    return np.asarray(means, dtype=float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899b11a7",
   "metadata": {},
   "source": [
    "# Whole Heatmap #\n",
    "Adoption vs Initial Conditions for a Fixed Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "392cd27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "# Compute heatmap matrix for phase sweep\n",
    "# \n",
    "##########################\n",
    "def phase_sweep_X0_vs_ratio(\n",
    "    X0_values: Iterable[float],\n",
    "    ratio_values: Iterable[float],\n",
    "    *,\n",
    "    I0: float = 0.05,\n",
    "    beta_I: float = 2.0,\n",
    "    b: float = 1.0,\n",
    "    g_I: float = 0.05,\n",
    "    T: int = 250,\n",
    "    network_type: str = \"BA\",   # \"random\", \"BA\", or \"SW\"\n",
    "    n_nodes: int = 120,\n",
    "    p: float = 0.05,\n",
    "    m: int = 2,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    "    max_workers: int | None = None,\n",
    "    backend: str = \"process\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute a heatmap matrix of mean final adoption X* over (X0, ratio).\n",
    "\n",
    "    Returns an array of shape (len(ratio_values), len(X0_values)) aligned with\n",
    "    the provided orders. Rows correspond to ratios; columns to X0 values.\n",
    "    \"\"\"\n",
    "    X0_values = list(X0_values)\n",
    "    ratio_values = list(ratio_values)\n",
    "    X_final = np.zeros((len(ratio_values), len(X0_values)), dtype=float)\n",
    "\n",
    "    # Prepare tasks per ratio\n",
    "    tasks: List[Dict] = []\n",
    "    for ratio in ratio_values:\n",
    "        tasks.append({\n",
    "            \"ratio\": ratio,\n",
    "            \"X0_values\": X0_values,\n",
    "            \"I0\": I0,\n",
    "            \"beta_I\": beta_I,\n",
    "            \"b\": b,\n",
    "            \"g_I\": g_I,\n",
    "            \"T\": T,\n",
    "            \"network_type\": network_type,\n",
    "            \"n_nodes\": n_nodes,\n",
    "            \"p\": p,\n",
    "            \"m\": m,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"init_noise_I\": init_noise_I,\n",
    "            \"strategy_choice_func\": strategy_choice_func,\n",
    "            \"tau\": tau,\n",
    "        })\n",
    "\n",
    "    if max_workers is None:\n",
    "        try:\n",
    "            max_workers = os.cpu_count() or 1\n",
    "        except Exception:\n",
    "            max_workers = 1\n",
    "\n",
    "    Executor = ProcessPoolExecutor if backend == \"process\" and max_workers > 1 else ThreadPoolExecutor\n",
    "\n",
    "    if max_workers > 1:\n",
    "        with Executor(max_workers=max_workers) as ex:\n",
    "            futures = [ex.submit(_row_for_ratio_task, args) for args in tasks]\n",
    "            for i, fut in enumerate(futures):\n",
    "                row = fut.result()\n",
    "                X_final[i, :] = row\n",
    "    else:\n",
    "        for i, args in enumerate(tasks):\n",
    "            row = _row_for_ratio_task(args)\n",
    "            X_final[i, :] = row\n",
    "\n",
    "    return X_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc1c5f8",
   "metadata": {},
   "source": [
    "## Plotting Functions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c5a865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b644be7f",
   "metadata": {},
   "source": [
    "Helper: Automatic File Path for Saving Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e75a748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _default_plot_path(filename: str) -> str:\n",
    "    plots_dir = os.path.join(os.getcwd(), \"plots\")\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    return os.path.join(plots_dir, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aea6f74",
   "metadata": {},
   "source": [
    "Fan chart plot for baseline vs policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8aa6d476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fanchart(traces_df: pd.DataFrame, out_path: Optional[str] = None) -> str:\n",
    "    \"\"\"Plot fan charts (quantile bands) for baseline vs subsidy using traces DF.\n",
    "\n",
    "    traces_df columns: ['group', 'trial', 'time', 'X'] where group in {'baseline','subsidy'}.\n",
    "    \"\"\"\n",
    "    if traces_df.empty:\n",
    "        raise ValueError(\"traces_df is empty\")\n",
    "\n",
    "    groups = [\"baseline\", \"subsidy\"]\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(11, 8), constrained_layout=True)\n",
    "\n",
    "    for j, group in enumerate(groups):\n",
    "        gdf = traces_df[traces_df[\"group\"] == group]\n",
    "\n",
    "        # Compute quantiles by time across trials\n",
    "        q = gdf.groupby(\"time\")[\"X\"].quantile([0.10, 0.25, 0.75, 0.90]).unstack(level=1)\n",
    "        mean = gdf.groupby(\"time\")[\"X\"].mean()\n",
    "        t = mean.index.to_numpy()\n",
    "\n",
    "        ax = axes[0, j]\n",
    "        ax.fill_between(t, q[0.10], q[0.90], color=(\"steelblue\" if group == \"baseline\" else \"darkorange\"), alpha=0.15, label=\"10–90%\")\n",
    "        ax.fill_between(t, q[0.25], q[0.75], color=(\"steelblue\" if group == \"baseline\" else \"darkorange\"), alpha=0.30, label=\"25–75%\")\n",
    "\n",
    "        # Overlay some traces for context (sample up to 100 trials)\n",
    "        trial_ids = gdf[\"trial\"].unique()\n",
    "        rng = np.random.default_rng(123)\n",
    "        sample = rng.choice(trial_ids, size=min(100, len(trial_ids)), replace=False)\n",
    "        for tr in sample:\n",
    "            tr_df = gdf[gdf[\"trial\"] == tr]\n",
    "            ax.plot(tr_df[\"time\"], tr_df[\"X\"], color=(\"steelblue\" if group == \"baseline\" else \"darkorange\"), alpha=0.1, linewidth=0.8)\n",
    "\n",
    "        ax.plot(t, mean, color=(\"steelblue\" if group == \"baseline\" else \"darkorange\"), linewidth=2, label=\"mean\")\n",
    "        ax.set_title(f\"{group.capitalize()} adoption\")\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.set_ylabel(\"X(t)\")\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.legend(loc=\"lower right\")\n",
    "\n",
    "        # Final X(T) histogram\n",
    "        t_max = int(gdf[\"time\"].max())\n",
    "        final_vals = gdf[gdf[\"time\"] == t_max].groupby(\"trial\")[\"X\"].mean().to_numpy()\n",
    "        axes[1, j].hist(final_vals, bins=20, color=(\"steelblue\" if group == \"baseline\" else \"darkorange\"), alpha=0.8)\n",
    "        axes[1, j].set_title(f\"{group.capitalize()} final X(T)\")\n",
    "        axes[1, j].set_xlabel(\"X(T)\")\n",
    "        axes[1, j].set_ylabel(\"Count\")\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = _default_plot_path(\"ev_intervention_fanchart.png\")\n",
    "    fig.savefig(out_path, dpi=140)\n",
    "    plt.close(fig)\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78be906b",
   "metadata": {},
   "source": [
    "Spaghetti Plot of Individual Adoption Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2af0d785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spaghetti(traces_df: pd.DataFrame, *, max_traces: int = 100, alpha: float = 0.15, out_path: Optional[str] = None) -> str:\n",
    "    \"\"\"Spaghetti plot from traces DF for baseline vs subsidy.\"\"\"\n",
    "    groups = [\"baseline\", \"subsidy\"]\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(11, 4.5), constrained_layout=True)\n",
    "    rng = np.random.default_rng(123)\n",
    "\n",
    "    for j, group in enumerate(groups):\n",
    "        gdf = traces_df[traces_df[\"group\"] == group]\n",
    "        trial_ids = gdf[\"trial\"].unique()\n",
    "        sample = rng.choice(trial_ids, size=min(max_traces, len(trial_ids)), replace=False)\n",
    "        ax = axes[j]\n",
    "        for tr in sample:\n",
    "            tr_df = gdf[gdf[\"trial\"] == tr]\n",
    "            ax.plot(tr_df[\"time\"], tr_df[\"X\"], color=(\"steelblue\" if group == \"baseline\" else \"darkorange\"), alpha=alpha, linewidth=0.8)\n",
    "        ax.set_title(f\"{group.capitalize()} traces\")\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.set_ylabel(\"X(t)\")\n",
    "        ax.set_ylim(0, 1)\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = _default_plot_path(\"ev_spaghetti.png\")\n",
    "    fig.savefig(out_path, dpi=140)\n",
    "    plt.close(fig)\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba10de5a",
   "metadata": {},
   "source": [
    "Density Plot of Adoption Dynamics (Baseline vs Subsidy) over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f28eb039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_density(traces_df: pd.DataFrame, *, x_bins: int = 50, time_bins: Optional[int] = None, out_path: Optional[str] = None) -> str:\n",
    "    \"\"\"Time-evolving density plot (2D histogram) from traces DF.\"\"\"\n",
    "    groups = [\"baseline\", \"subsidy\"]\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4.8), constrained_layout=True)\n",
    "\n",
    "    for j, group in enumerate(groups):\n",
    "        gdf = traces_df[traces_df[\"group\"] == group]\n",
    "        T = int(gdf[\"time\"].max()) + 1\n",
    "        if time_bins is None:\n",
    "            bins_time = T\n",
    "        else:\n",
    "            bins_time = time_bins\n",
    "        hb = axes[j].hist2d(gdf[\"time\"].to_numpy(), gdf[\"X\"].to_numpy(), bins=[bins_time, x_bins], range=[[0, T - 1], [0.0, 1.0]], cmap=\"magma\")\n",
    "        axes[j].set_title(f\"{group.capitalize()} density: time vs X(t)\")\n",
    "        axes[j].set_xlabel(\"Time\")\n",
    "        axes[j].set_ylabel(\"X(t)\")\n",
    "        fig.colorbar(hb[3], ax=axes[j], label=\"count\")\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = _default_plot_path(\"ev_density.png\")\n",
    "    fig.savefig(out_path, dpi=140)\n",
    "    plt.close(fig)\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc842ca",
   "metadata": {},
   "source": [
    "Density of Adoption Trajectories Over Time (Baseline vs Subsidy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b10eb8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_density(traces_df: pd.DataFrame, *, x_bins: int = 50, time_bins: Optional[int] = None, out_path: Optional[str] = None) -> str:\n",
    "    \"\"\"Time-evolving density plot (2D histogram) from traces DF.\"\"\"\n",
    "    groups = [\"baseline\", \"subsidy\"]\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4.8), constrained_layout=True)\n",
    "\n",
    "    for j, group in enumerate(groups):\n",
    "        gdf = traces_df[traces_df[\"group\"] == group]\n",
    "        T = int(gdf[\"time\"].max()) + 1\n",
    "        if time_bins is None:\n",
    "            bins_time = T\n",
    "        else:\n",
    "            bins_time = time_bins\n",
    "        hb = axes[j].hist2d(gdf[\"time\"].to_numpy(), gdf[\"X\"].to_numpy(), bins=[bins_time, x_bins], range=[[0, T - 1], [0.0, 1.0]], cmap=\"magma\")\n",
    "        axes[j].set_title(f\"{group.capitalize()} density: time vs X(t)\")\n",
    "        axes[j].set_xlabel(\"Time\")\n",
    "        axes[j].set_ylabel(\"X(t)\")\n",
    "        fig.colorbar(hb[3], ax=axes[j], label=\"count\")\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = _default_plot_path(\"ev_density.png\")\n",
    "    fig.savefig(out_path, dpi=140)\n",
    "    plt.close(fig)\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d42f29",
   "metadata": {},
   "source": [
    "Phase Plot: Long-Run Adoption as a Function of X0 and Payoff Ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35d4909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_phase_plot(phase_df: pd.DataFrame, out_path: Optional[str] = None) -> str:\n",
    "    \"\"\"Plot heatmap from tidy DataFrame with columns ['X0','ratio','X_final'].\"\"\"\n",
    "    # Pivot to matrix for imshow\n",
    "    pivot = phase_df.pivot(index=\"ratio\", columns=\"X0\", values=\"X_final\").sort_index().sort_index(axis=1)\n",
    "    ratios = pivot.index.to_numpy()\n",
    "    X0s = pivot.columns.to_numpy()\n",
    "\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    im = plt.imshow(\n",
    "        pivot.to_numpy(),\n",
    "        origin=\"lower\",\n",
    "        extent=[X0s[0], X0s[-1], ratios[0], ratios[-1]],\n",
    "        aspect=\"auto\",\n",
    "        vmin=0.0,\n",
    "        vmax=1.0,\n",
    "        cmap=\"plasma\",\n",
    "    )\n",
    "    plt.colorbar(im, label=\"Final adopters X*\")\n",
    "    plt.xlabel(\"X0 (initial adoption)\")\n",
    "    plt.ylabel(\"a_I / b (initial payoff ratio)\")\n",
    "    plt.title(\"Network phase plot: X* over X0 and a_I/b\")\n",
    "\n",
    "    # Overlay threshold X = 1/ratio\n",
    "    X_thresh = 1.0 / ratios\n",
    "    X_thresh_clipped = np.clip(X_thresh, 0.0, 1.0)\n",
    "    plt.plot(X_thresh_clipped, ratios, color=\"white\", linestyle=\"--\", linewidth=1.5, label=\"X = b / a_I (initial)\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = _default_plot_path(\"ev_phase_plot.png\")\n",
    "    plt.savefig(out_path, dpi=140, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0828da",
   "metadata": {},
   "source": [
    "# Subsidy Policy #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6df794f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Experiment utilities for the EV Stag Hunt model.\n",
    "\n",
    "Contains policy factory functions, trial runners, multiprocessing helpers,\n",
    "and standalone plotting routines. Depends on `ev_core` for the model.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from typing import Callable, Dict, Optional, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "\n",
    "# -----------------------------\n",
    "# Policy factories (subsidy)\n",
    "# -----------------------------\n",
    "\n",
    "def policy_subsidy_factory(start: int, end: int, delta_a0: float = 0.3, delta_beta_I: float = 0.0) -> Callable:\n",
    "    \"\"\"Create a policy that temporarily boosts coordination payoffs.\n",
    "\n",
    "    Raises `a0` and/or `beta_I` during `[start, end)` and reverts after.\n",
    "    Returns a closure `policy(model, step)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def policy(model, step):\n",
    "        if not hasattr(policy, \"base_a0\"):\n",
    "            policy.base_a0 = model.a0\n",
    "        if not hasattr(policy, \"base_beta_I\"):\n",
    "            policy.base_beta_I = model.beta_I\n",
    "\n",
    "        if start <= step < end:\n",
    "            model.a0 = policy.base_a0 + delta_a0\n",
    "            model.beta_I = policy.base_beta_I + delta_beta_I\n",
    "        else:\n",
    "            model.a0 = policy.base_a0\n",
    "            model.beta_I = policy.base_beta_I\n",
    "\n",
    "    return policy\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Trial runner\n",
    "# -----------------------------\n",
    "\n",
    "def run_timeseries_trial(\n",
    "    T: int = 200,\n",
    "    scenario_kwargs: Optional[Dict] = None,\n",
    "    seed: Optional[int] = None,\n",
    "    policy: Optional[Callable] = None,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    ") -> Tuple[np.ndarray, np.ndarray, pd.DataFrame]:\n",
    "    \"\"\"Run one simulation; return X(t), I(t), and model vars dataframe.\"\"\"\n",
    "\n",
    "    scenario = {\n",
    "        \"a0\": 2.0,\n",
    "        \"ratio\": None,\n",
    "        \"beta_I\": 3.0,\n",
    "        \"b\": 1.0,\n",
    "        \"g_I\": 0.1,\n",
    "        \"I0\": 0.05,\n",
    "        \"network_type\": \"random\",\n",
    "        \"n_nodes\": 100,\n",
    "        \"p\": 0.05,\n",
    "        \"m\": 2,\n",
    "        \"collect\": True,\n",
    "        \"X0_frac\": 0.0,\n",
    "        \"init_method\": \"random\",\n",
    "    }\n",
    "    if scenario_kwargs:\n",
    "        scenario.update(scenario_kwargs)\n",
    "\n",
    "    # Compute a0 from ratio if provided\n",
    "    a0_for_model = scenario[\"a0\"]\n",
    "    if scenario.get(\"ratio\") is not None:\n",
    "        a0_for_model = (\n",
    "            float(scenario[\"ratio\"]) * float(scenario[\"b\"])\n",
    "            - float(scenario[\"beta_I\"]) * float(scenario[\"I0\"])\n",
    "        )\n",
    "    model = EVStagHuntModel(\n",
    "        initial_ev=0,                     \n",
    "        a0=a0_for_model,\n",
    "        beta_I=scenario[\"beta_I\"],\n",
    "        b=scenario[\"b\"],\n",
    "        g_I=scenario[\"g_I\"],\n",
    "        I0=scenario[\"I0\"],\n",
    "        seed=seed,\n",
    "        network_type=scenario[\"network_type\"],\n",
    "        n_nodes=scenario[\"n_nodes\"],\n",
    "        p=scenario[\"p\"],\n",
    "        m=scenario[\"m\"],\n",
    "        collect=True,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "\n",
    "    # Set initial adopters AFTER model creation\n",
    "    if scenario.get(\"X0_frac\", 0.0) > 0.0:\n",
    "        set_initial_adopters(\n",
    "            model,\n",
    "            scenario[\"X0_frac\"],\n",
    "            method=scenario.get(\"init_method\", \"random\"),\n",
    "            seed=seed,\n",
    "        )\n",
    "\n",
    "    for t in range(T):\n",
    "        if policy is not None:\n",
    "            policy(model, t)\n",
    "        model.step()\n",
    "\n",
    "    df = model.datacollector.get_model_vars_dataframe().copy()\n",
    "    return df[\"X\"].to_numpy(), df[\"I\"].to_numpy(), df\n",
    "\n",
    "\n",
    "def _timeseries_trial_worker(args_dict: Dict) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Worker for parallel trials that reconstructs closures for policies.\"\"\"\n",
    "    T = args_dict[\"T\"]\n",
    "    scenario_kwargs = args_dict.get(\"scenario_kwargs\", {})\n",
    "    seed = args_dict.get(\"seed\", None)\n",
    "    policy_spec = args_dict.get(\"policy\", None)\n",
    "    strategy_choice_func = args_dict.get(\"strategy_choice_func\", \"logit\")\n",
    "    tau = args_dict.get(\"tau\", 1.0)\n",
    "\n",
    "    policy = None\n",
    "    if isinstance(policy_spec, dict):\n",
    "        ptype = policy_spec.get(\"type\")\n",
    "        if ptype == \"subsidy\":\n",
    "            policy = policy_subsidy_factory(**policy_spec[\"params\"])\n",
    "        # no infrastructure policy anymore\n",
    "\n",
    "    X, I, _df = run_timeseries_trial(\n",
    "        T=T,\n",
    "        scenario_kwargs=scenario_kwargs,\n",
    "        seed=seed,\n",
    "        policy=policy,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "    return X, I\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Experiment: Intervention trials + plotting\n",
    "# -----------------------------\n",
    "\n",
    "def collect_intervention_trials(\n",
    "    n_trials: int = 10,\n",
    "    T: int = 200,\n",
    "    scenario_kwargs: Optional[Dict] = None,\n",
    "    subsidy_params: Optional[Dict] = None,\n",
    "    max_workers: int = 1,\n",
    "    seed_base: int = 42,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    ") -> Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray], pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Run baseline and subsidy trials; return raw trajectories and summary dataframes.\"\"\"\n",
    "\n",
    "    scenario = scenario_kwargs or {}\n",
    "    subsidy = subsidy_params or {\"start\": 30, \"end\": 80, \"delta_a0\": 0.3, \"delta_beta_I\": 0.0}\n",
    "\n",
    "    baseline_args = []\n",
    "    subsidy_args = []\n",
    "    for i in range(n_trials):\n",
    "        seed = seed_base + i\n",
    "        baseline_args.append(\n",
    "            {\n",
    "                \"T\": T,\n",
    "                \"scenario_kwargs\": scenario,\n",
    "                \"seed\": seed,\n",
    "                \"policy\": None,\n",
    "                \"strategy_choice_func\": strategy_choice_func,\n",
    "                \"tau\": tau,\n",
    "            }\n",
    "        )\n",
    "        subsidy_args.append(\n",
    "            {\n",
    "                \"T\": T,\n",
    "                \"scenario_kwargs\": scenario,\n",
    "                \"seed\": seed,\n",
    "                \"policy\": {\"type\": \"subsidy\", \"params\": subsidy},\n",
    "                \"strategy_choice_func\": strategy_choice_func,\n",
    "                \"tau\": tau,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    baseline_X, baseline_I = [], []\n",
    "    subsidy_X, subsidy_I = [], []\n",
    "\n",
    "    # Run sequentially or concurrently\n",
    "    Executor = ThreadPoolExecutor if max_workers == 1 else ProcessPoolExecutor\n",
    "    with Executor(max_workers=max_workers) as ex:\n",
    "        baseline_futs = [ex.submit(_timeseries_trial_worker, args) for args in baseline_args]\n",
    "        subsidy_futs = [ex.submit(_timeseries_trial_worker, args) for args in subsidy_args]\n",
    "        for fut in as_completed(baseline_futs):\n",
    "            X, I = fut.result()\n",
    "            baseline_X.append(X)\n",
    "            baseline_I.append(I)\n",
    "        for fut in as_completed(subsidy_futs):\n",
    "            X, I = fut.result()\n",
    "            subsidy_X.append(X)\n",
    "            subsidy_I.append(I)\n",
    "\n",
    "    # Align order by seed (as_completed may scramble)\n",
    "    baseline_X = sorted(baseline_X, key=lambda arr: tuple(arr))\n",
    "    subsidy_X = sorted(subsidy_X, key=lambda arr: tuple(arr))\n",
    "\n",
    "    # Summary stats\n",
    "    def summarize(X_list: List[np.ndarray]) -> pd.DataFrame:\n",
    "        mat = np.vstack(X_list)\n",
    "        df = pd.DataFrame({\n",
    "            \"X_mean\": mat.mean(axis=0),\n",
    "            \"X_med\": np.median(mat, axis=0),\n",
    "            \"X_q10\": np.quantile(mat, 0.10, axis=0),\n",
    "            \"X_q25\": np.quantile(mat, 0.25, axis=0),\n",
    "            \"X_q75\": np.quantile(mat, 0.75, axis=0),\n",
    "            \"X_q90\": np.quantile(mat, 0.90, axis=0),\n",
    "        })\n",
    "        return df\n",
    "\n",
    "    baseline_df = summarize(baseline_X)\n",
    "    subsidy_df = summarize(subsidy_X)\n",
    "\n",
    "    return baseline_X, baseline_I, subsidy_X, subsidy_I, baseline_df, subsidy_df\n",
    "\n",
    "\n",
    "def traces_to_long_df(baseline_X: List[np.ndarray], subsidy_X: List[np.ndarray]) -> pd.DataFrame:\n",
    "    \"\"\"Convert trajectory lists to a tidy DataFrame: [group, trial, time, X].\"\"\"\n",
    "    rows = []\n",
    "    for trial, X in enumerate(baseline_X):\n",
    "        for t, x in enumerate(X):\n",
    "            rows.append((\"baseline\", trial, t, float(x)))\n",
    "    for trial, X in enumerate(subsidy_X):\n",
    "        for t, x in enumerate(X):\n",
    "            rows.append((\"subsidy\", trial, t, float(x)))\n",
    "    return pd.DataFrame(rows, columns=[\"group\", \"trial\", \"time\", \"X\"])\n",
    "\n",
    "\n",
    "def ratio_sweep_df(\n",
    "    X0_frac: float = 0.40,\n",
    "    ratio_values: Optional[np.ndarray] = None,\n",
    "    scenario_kwargs: Optional[Dict] = None,\n",
    "    T: int = 250,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute X* vs ratio and return as a DataFrame.\"\"\"\n",
    "    scenario = {\n",
    "        \"beta_I\": 2.0,\n",
    "        \"b\": 1.0,\n",
    "        \"g_I\": 0.05,\n",
    "        \"I0\": 0.05,\n",
    "        \"network_type\": \"BA\",\n",
    "        \"n_nodes\": 120,\n",
    "        \"p\": 0.05,\n",
    "        \"m\": 2,\n",
    "    }\n",
    "    if scenario_kwargs:\n",
    "        scenario.update(scenario_kwargs)\n",
    "\n",
    "    if ratio_values is None:\n",
    "        ratio_values = np.linspace(0.8, 3.5, 41)\n",
    "\n",
    "    X_means = final_mean_adoption_vs_ratio(\n",
    "        X0_frac,\n",
    "        ratio_values,\n",
    "        I0=scenario[\"I0\"],\n",
    "        beta_I=scenario[\"beta_I\"],\n",
    "        b=scenario[\"b\"],\n",
    "        g_I=scenario[\"g_I\"],\n",
    "        T=T,\n",
    "        network_type=scenario[\"network_type\"],\n",
    "        n_nodes=scenario[\"n_nodes\"],\n",
    "        p=scenario[\"p\"],\n",
    "        m=scenario[\"m\"],\n",
    "        batch_size=batch_size,\n",
    "        init_noise_I=init_noise_I,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "\n",
    "    return pd.DataFrame({\"ratio\": ratio_values, \"X_mean\": X_means})\n",
    "\n",
    "\n",
    "def phase_sweep_df(\n",
    "    max_workers: int | None = None,\n",
    "    backend: str = \"process\",\n",
    "    X0_values: Optional[np.ndarray] = None,\n",
    "    ratio_values: Optional[np.ndarray] = None,\n",
    "    scenario_kwargs: Optional[Dict] = None,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    T: int = 250,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute tidy DataFrame of X* over (X0, ratio).\"\"\"\n",
    "    if X0_values is None:\n",
    "        X0_values = np.linspace(0.0, 1.0, 21)\n",
    "    if ratio_values is None:\n",
    "        ratio_values = np.linspace(0.8, 3.5, 41)\n",
    "\n",
    "    scenario = {\n",
    "        \"I0\": 0.05,\n",
    "        \"beta_I\": 2.0,\n",
    "        \"b\": 1.0,\n",
    "        \"g_I\": 0.05,\n",
    "        \"network_type\": \"BA\",\n",
    "        \"n_nodes\": 120,\n",
    "        \"p\": 0.05,\n",
    "        \"m\": 2,\n",
    "    }\n",
    "    if scenario_kwargs:\n",
    "        scenario.update(scenario_kwargs)\n",
    "\n",
    "    X_final = phase_sweep_X0_vs_ratio(\n",
    "        X0_values,\n",
    "        ratio_values,\n",
    "        I0=scenario[\"I0\"],\n",
    "        beta_I=scenario[\"beta_I\"],\n",
    "        b=scenario[\"b\"],\n",
    "        g_I=scenario[\"g_I\"],\n",
    "        T=T,\n",
    "        network_type=scenario[\"network_type\"],\n",
    "        n_nodes=scenario[\"n_nodes\"],\n",
    "        p=scenario[\"p\"],\n",
    "        m=scenario[\"m\"],\n",
    "        batch_size=batch_size,\n",
    "        init_noise_I=init_noise_I,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "        max_workers=max_workers or 1,\n",
    "        backend=backend,\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "    for i, X0 in enumerate(X0_values):\n",
    "        for j, ratio in enumerate(ratio_values):\n",
    "            rows.append((float(X0), float(ratio), float(X_final[j, i])))\n",
    "    return pd.DataFrame(rows, columns=[\"X0\", \"ratio\", \"X_final\"])\n",
    "\n",
    "\n",
    "def run_ratio_sweep_plot(\n",
    "    X0_frac: float = 0.40,\n",
    "    ratio_values: Optional[np.ndarray] = None,\n",
    "    scenario_kwargs: Optional[Dict] = None,\n",
    "    T: int = 250,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    "    out_path: Optional[str] = None,\n",
    ") -> str:\n",
    "    \"\"\"Sweep ratio values and plot final adoption X* vs a_I/b for a fixed X0.\n",
    "\n",
    "    Calls the core computation helper and saves a simple line plot.\n",
    "    Returns the path to the saved image.\n",
    "    \"\"\"\n",
    "    scenario = {\n",
    "        \"beta_I\": 2.0,\n",
    "        \"b\": 1.0,\n",
    "        \"g_I\": 0.05,\n",
    "        \"I0\": 0.05,\n",
    "        \"network_type\": \"BA\",\n",
    "        \"n_nodes\": 120,\n",
    "        \"p\": 0.05,\n",
    "        \"m\": 2,\n",
    "    }\n",
    "    if scenario_kwargs:\n",
    "        scenario.update(scenario_kwargs)\n",
    "\n",
    "    if ratio_values is None:\n",
    "        ratio_values = np.linspace(0.8, 3.5, 41)\n",
    "\n",
    "    X_means = final_mean_adoption_vs_ratio(\n",
    "        X0_frac,\n",
    "        ratio_values,\n",
    "        I0=scenario[\"I0\"],\n",
    "        beta_I=scenario[\"beta_I\"],\n",
    "        b=scenario[\"b\"],\n",
    "        g_I=scenario[\"g_I\"],\n",
    "        T=T,\n",
    "        network_type=scenario[\"network_type\"],\n",
    "        n_nodes=scenario[\"n_nodes\"],\n",
    "        p=scenario[\"p\"],\n",
    "        m=scenario[\"m\"],\n",
    "        batch_size=batch_size,\n",
    "        init_noise_I=init_noise_I,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    ax.plot(ratio_values, X_means, color=\"C0\", lw=2)\n",
    "    ax.set_xlabel(\"a_I / b (ratio)\")\n",
    "    ax.set_ylabel(\"Final adoption X*\")\n",
    "    ax.set_title(f\"X* vs ratio for X0={X0_frac:.2f}\")\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "    ax.grid(True, alpha=0.25)\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = os.path.join(os.getcwd(), \"ev_ratio_sweep.png\")\n",
    "    fig.savefig(out_path, dpi=140, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def run_phase_plot_X0_vs_ratio_network(\n",
    "    max_workers: int | None = None,\n",
    "    backend: str = \"process\",\n",
    "    X0_values: Optional[np.ndarray] = None,\n",
    "    ratio_values: Optional[np.ndarray] = None,\n",
    "    scenario_kwargs: Optional[Dict] = None,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    T: int = 250,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    "    out_path: Optional[str] = None,\n",
    ") -> str:\n",
    "    \"\"\"Produce a heatmap of X* over (X0, a_I/b) using core sweep helper.\n",
    "\n",
    "    Saves a figure similar to the original model script and returns the path.\n",
    "    \"\"\"\n",
    "    # Defaults aligned with the original phase plot\n",
    "    if X0_values is None:\n",
    "        X0_values = np.linspace(0.0, 1.0, 21)\n",
    "    if ratio_values is None:\n",
    "        ratio_values = np.linspace(0.8, 3.5, 41)\n",
    "\n",
    "    scenario = {\n",
    "        \"I0\": 0.05,\n",
    "        \"beta_I\": 2.0,\n",
    "        \"b\": 1.0,\n",
    "        \"g_I\": 0.05,\n",
    "        \"network_type\": \"BA\",\n",
    "        \"n_nodes\": 120,\n",
    "        \"p\": 0.05,\n",
    "        \"m\": 2,\n",
    "    }\n",
    "    if scenario_kwargs:\n",
    "        scenario.update(scenario_kwargs)\n",
    "\n",
    "    X_final = phase_sweep_X0_vs_ratio(\n",
    "        X0_values,\n",
    "        ratio_values,\n",
    "        I0=scenario[\"I0\"],\n",
    "        beta_I=scenario[\"beta_I\"],\n",
    "        b=scenario[\"b\"],\n",
    "        g_I=scenario[\"g_I\"],\n",
    "        T=T,\n",
    "        network_type=scenario[\"network_type\"],\n",
    "        n_nodes=scenario[\"n_nodes\"],\n",
    "        p=scenario[\"p\"],\n",
    "        m=scenario[\"m\"],\n",
    "        batch_size=batch_size,\n",
    "        init_noise_I=init_noise_I,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "        max_workers=max_workers or 1,\n",
    "        backend=backend,\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    im = plt.imshow(\n",
    "        X_final,\n",
    "        origin=\"lower\",\n",
    "        extent=[X0_values[0], X0_values[-1], ratio_values[0], ratio_values[-1]],\n",
    "        aspect=\"auto\",\n",
    "        vmin=0.0,\n",
    "        vmax=1.0,\n",
    "        cmap=\"plasma\",\n",
    "    )\n",
    "    plt.colorbar(im, label=\"Final adopters X*\")\n",
    "    plt.xlabel(\"X0 (initial adoption)\")\n",
    "    plt.ylabel(\"a_I / b (initial payoff ratio)\")\n",
    "    plt.title(\"Network phase plot: X* over X0 and a_I/b\")\n",
    "\n",
    "    # Overlay initial threshold X = b/a_I => X = 1/ratio\n",
    "    X_thresh = 1.0 / ratio_values\n",
    "    X_thresh_clipped = np.clip(X_thresh, 0.0, 1.0)\n",
    "    plt.plot(\n",
    "        X_thresh_clipped,\n",
    "        ratio_values,\n",
    "        color=\"white\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=1.5,\n",
    "        label=\"X = b / a_I (initial)\",\n",
    "    )\n",
    "\n",
    "    plt.legend(loc=\"upper right\")\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = os.path.join(os.getcwd(), \"ev_phase_plot.png\")\n",
    "    plt.savefig(out_path, dpi=140, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def run_intervention_example(\n",
    "    n_trials: int = 10,\n",
    "    T: int = 200,\n",
    "    scenario_kwargs: Optional[Dict] = None,\n",
    "    subsidy_params: Optional[Dict] = None,\n",
    "    max_workers: int = 1,\n",
    "    seed_base: int = 42,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, str]:\n",
    "    \"\"\"Convenience: collect trials, plot, and return summary + image path.\"\"\"\n",
    "\n",
    "    baseline_X, baseline_I, subsidy_X, subsidy_I, baseline_df, subsidy_df = collect_intervention_trials(\n",
    "        n_trials=n_trials,\n",
    "        T=T,\n",
    "        scenario_kwargs=scenario_kwargs,\n",
    "        subsidy_params=subsidy_params,\n",
    "        max_workers=max_workers,\n",
    "        seed_base=seed_base,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "    # Use DataFrame-based plotting to ensure outputs go to plots/\n",
    "    traces_df = traces_to_long_df(baseline_X, subsidy_X)\n",
    "    img_path = plot_fanchart(traces_df)\n",
    "    return baseline_df, subsidy_df, img_path\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# CLI Entrypoint (optional)\n",
    "# -----------------------------\n",
    "\n",
    "def main():\n",
    "    # Defaults aligned with original ev_stag_mesa_model.run_intervention_example\n",
    "    n_trials = 30  # use fewer than 500 for speed while keeping shape\n",
    "    T = 200\n",
    "    strategy_choice_func = \"logit\"\n",
    "    tau = 1.0\n",
    "    max_workers = 1\n",
    "    seed_base = 100\n",
    "\n",
    "    scenario = dict(\n",
    "        # Preserve initial ratio by computing a0 from ratio, matching the original\n",
    "        ratio=2.3,\n",
    "        beta_I=2.0,\n",
    "        b=1.0,\n",
    "        g_I=0.10,\n",
    "        I0=0.05,\n",
    "        network_type=\"BA\",\n",
    "        n_nodes=300,\n",
    "        m=2,\n",
    "        collect=True,\n",
    "        X0_frac=0.40,\n",
    "        init_method=\"random\",\n",
    "        # ER-specific `p` ignored for BA but kept for completeness\n",
    "        p=0.05,\n",
    "    )\n",
    "    subsidy = dict(start=10, end=60, delta_a0=0.4, delta_beta_I=0.0)\n",
    "\n",
    "    baseline_df, subsidy_df, img_path = run_intervention_example(\n",
    "        n_trials=n_trials,\n",
    "        T=T,\n",
    "        scenario_kwargs=scenario,\n",
    "        subsidy_params=subsidy,\n",
    "        max_workers=max_workers,\n",
    "        seed_base=seed_base,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "\n",
    "    print(\"Baseline DF shape:\", baseline_df.shape)\n",
    "    print(\"Subsidy DF shape:\", subsidy_df.shape)\n",
    "    print(\"Saved image:\", img_path)\n",
    "    print(\"Baseline final X_mean:\", float(baseline_df[\"X_mean\"].iloc[-1]))\n",
    "    print(\"Subsidy  final X_mean:\", float(subsidy_df[\"X_mean\"].iloc[-1]))\n",
    "\n",
    "    # Also run the phase plot of X* over (X0, a_I/b) and save it\n",
    "    phase_df = phase_sweep_df(\n",
    "        max_workers=1,\n",
    "        backend=\"thread\",\n",
    "        X0_values=np.linspace(0.0, 1.0, 21),\n",
    "        ratio_values=np.linspace(0.8, 3.5, 31),\n",
    "        batch_size=8,\n",
    "        T=200,\n",
    "        strategy_choice_func=\"logit\",\n",
    "        tau=1.0,\n",
    "    )\n",
    "    phase_path = plot_phase_plot(phase_df)\n",
    "    print(\"Saved phase plot:\", phase_path)\n",
    "\n",
    "    # Spaghetti and time-evolving density plots\n",
    "    n_trials_spaghetti = 100\n",
    "    T_spaghetti = 200\n",
    "\n",
    "    baseline_X, baseline_I, subsidy_X, subsidy_I, baseline_df2, subsidy_df2 = collect_intervention_trials(\n",
    "        n_trials=n_trials_spaghetti,\n",
    "        T=T_spaghetti,\n",
    "        scenario_kwargs=scenario,\n",
    "        subsidy_params=subsidy,\n",
    "        max_workers=max_workers,\n",
    "        seed_base=seed_base,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "    traces_df = traces_to_long_df(baseline_X, subsidy_X)\n",
    "    spaghetti_path = plot_spaghetti(traces_df, max_traces=100, alpha=0.15)\n",
    "    print(\"Saved spaghetti plot:\", spaghetti_path)\n",
    "\n",
    "    density_path = plot_density(traces_df, x_bins=50, time_bins=T_spaghetti)\n",
    "    print(\"Saved time-evolving density plot:\", density_path)\n",
    "\n",
    "    # Ratio sweep computed to DF then plotted\n",
    "    sweep_df = ratio_sweep_df(\n",
    "        X0_frac=scenario.get(\"X0_frac\", 0.40),\n",
    "        ratio_values=np.linspace(0.8, 3.5, 31),\n",
    "        scenario_kwargs=scenario,\n",
    "        T=200,\n",
    "        batch_size=8,\n",
    "        strategy_choice_func=\"logit\",\n",
    "        tau=1.0,\n",
    "    )\n",
    "    sweep_path = plot_ratio_sweep(sweep_df)\n",
    "    print(\"Saved ratio sweep plot:\", sweep_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51a97c3",
   "metadata": {},
   "source": [
    "# Experiment runner #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808bd10b",
   "metadata": {},
   "source": [
    "Heatmap: Final doption vs X0,I0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83bd3e24",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 76\u001b[39m\n\u001b[32m     73\u001b[39m X0_vals = np.linspace(\u001b[32m0.0\u001b[39m, \u001b[32m1.0\u001b[39m, \u001b[32m31\u001b[39m)\n\u001b[32m     74\u001b[39m I0_vals = np.linspace(\u001b[32m0.0\u001b[39m, \u001b[32m1.0\u001b[39m, \u001b[32m31\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m X0v, I0v, H = \u001b[43mheatmap_Xstar_X0_I0\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX0_vals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43mI0_vals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta_I\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43mb\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43mg_I\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43mT\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnetwork_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBA\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# change to \"random\" or \"SW\" if you want\u001b[39;49;00m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_nodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43mm\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\n\u001b[32m     90\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m plot_heatmap_X0_I0(X0v, I0v, H, title=\u001b[33m\"\u001b[39m\u001b[33mBA: Final adoption X* vs (X0, I0)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     95\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMin X*:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(H.min()), \u001b[33m\"\u001b[39m\u001b[33mMax X*:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(H.max()))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mheatmap_Xstar_X0_I0\u001b[39m\u001b[34m(X0_values, I0_values, ratio, beta_I, b, g_I, T, batch_size, network_type, n_nodes, p, m, tau)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[32m     29\u001b[39m     seed = np.random.randint(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m**\u001b[32m31\u001b[39m - \u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     x_star = \u001b[43mrun_network_trial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX0_frac\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mratio\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mratio\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43mI0\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mI0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta_I\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbeta_I\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43mb\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43mg_I\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mg_I\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43mT\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnetwork_type\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnetwork_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_nodes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_nodes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[43mp\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[43mm\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrategy_choice_func\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtau\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     finals.append(x_star)\n\u001b[32m     48\u001b[39m H[i, j] = \u001b[38;5;28mfloat\u001b[39m(np.mean(finals))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mrun_network_trial\u001b[39m\u001b[34m(X0_frac, ratio, I0, beta_I, b, g_I, T, network_type, n_nodes, p, m, seed, tol, patience, collect, strategy_choice_func, tau)\u001b[39m\n\u001b[32m     71\u001b[39m prev_I = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(T):\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m     X = model.get_adoption_fraction()\n\u001b[32m     75\u001b[39m     I = model.infrastructure\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36mEVStagHuntModel.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m): \n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mschedule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# advance all agents\u001b[39;00m\n\u001b[32m     95\u001b[39m     X = \u001b[38;5;28mself\u001b[39m.get_adoption_fraction()  \u001b[38;5;66;03m# compute adoption fraction after all agents have advanced\u001b[39;00m\n\u001b[32m     96\u001b[39m     I = \u001b[38;5;28mself\u001b[39m.infrastructure  \u001b[38;5;66;03m# infrastructure level before this step\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bartw\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\mesa\\time.py:120\u001b[39m, in \u001b[36mBaseScheduler._wrapped_step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapped_step\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    119\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Wrapper for the step method to include time and step updating.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_original_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28mself\u001b[39m.model._advance_time()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bartw\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\mesa\\time.py:200\u001b[39m, in \u001b[36mSimultaneousActivation.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    199\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Step all agents, then advance them.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_each\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstep\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m     \u001b[38;5;66;03m# do_each recomputes the agent_keys from scratch whenever it is called.\u001b[39;00m\n\u001b[32m    202\u001b[39m     \u001b[38;5;66;03m# It can handle the case when some agents might have been removed in\u001b[39;00m\n\u001b[32m    203\u001b[39m     \u001b[38;5;66;03m# the previous loop.\u001b[39;00m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28mself\u001b[39m.do_each(\u001b[33m\"\u001b[39m\u001b[33madvance\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bartw\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\mesa\\time.py:152\u001b[39m, in \u001b[36mBaseScheduler.do_each\u001b[39m\u001b[34m(self, method, shuffle)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[32m    151\u001b[39m     \u001b[38;5;28mself\u001b[39m._agents.shuffle(inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_agents\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bartw\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\mesa\\agent.py:246\u001b[39m, in \u001b[36mAgentSet.do\u001b[39m\u001b[34m(self, method_name, return_results, *args, **kwargs)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    233\u001b[39m \u001b[33;03mInvoke a method on each agent in the AgentSet.\u001b[39;00m\n\u001b[32m    234\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    242\u001b[39m \u001b[33;03m    AgentSet | list[Any]: The results of the method calls if return_results is True, otherwise the AgentSet itself.\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    244\u001b[39m \u001b[38;5;66;03m# we iterate over the actual weakref keys and check if weakref is alive before calling the method\u001b[39;00m\n\u001b[32m    245\u001b[39m res = [\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    247\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m agentref \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agents.keyrefs()\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (agent := agentref()) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    249\u001b[39m ]\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res \u001b[38;5;28;01mif\u001b[39;00m return_results \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mEVAgent.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     31\u001b[39m neighbor_agents = []\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m nbr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.G.neighbors(\u001b[38;5;28mself\u001b[39m.pos):\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     neighbor_agents.extend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_cell_list_contents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnbr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m neighbor_agents:\n\u001b[32m     35\u001b[39m     \u001b[38;5;28mself\u001b[39m.payoff = \u001b[32m0.0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bartw\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\mesa\\space.py:1578\u001b[39m, in \u001b[36mNetworkGrid.get_cell_list_contents\u001b[39m\u001b[34m(self, cell_list)\u001b[39m\n\u001b[32m   1575\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns a bool of the contents of a cell.\"\"\"\u001b[39;00m\n\u001b[32m   1576\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.G.nodes[node_id][\u001b[33m\"\u001b[39m\u001b[33magent\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[38;5;28mself\u001b[39m.default_val()\n\u001b[32m-> \u001b[39m\u001b[32m1578\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_cell_list_contents\u001b[39m(\u001b[38;5;28mself\u001b[39m, cell_list: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]) -> \u001b[38;5;28mlist\u001b[39m[Agent]:\n\u001b[32m   1579\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns a list of the agents contained in the nodes identified\u001b[39;00m\n\u001b[32m   1580\u001b[39m \u001b[33;03m    in `cell_list`; nodes with empty content are excluded.\u001b[39;00m\n\u001b[32m   1581\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1582\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.iter_cell_list_contents(cell_list))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def heatmap_Xstar_X0_I0(\n",
    "    X0_values,\n",
    "    I0_values,\n",
    "    *,\n",
    "    ratio=2.3,\n",
    "    beta_I=2.0,\n",
    "    b=1.0,\n",
    "    g_I=0.10,\n",
    "    T=200,\n",
    "    batch_size=5,\n",
    "    network_type=\"BA\",\n",
    "    n_nodes=200,\n",
    "    p=0.05,\n",
    "    m=2,\n",
    "    tau=1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute mean final adoption X* over a grid of (X0, I0).\n",
    "    Returns: X0_vals, I0_vals, H where H[i, j] corresponds to I0[i], X0[j].\n",
    "    \"\"\"\n",
    "    X0_values = list(X0_values)\n",
    "    I0_values = list(I0_values)\n",
    "    H = np.zeros((len(I0_values), len(X0_values)), dtype=float)\n",
    "\n",
    "    for i, I0 in enumerate(I0_values):\n",
    "        for j, X0 in enumerate(X0_values):\n",
    "            finals = []\n",
    "            for _ in range(batch_size):\n",
    "                seed = np.random.randint(0, 2**31 - 1)\n",
    "                x_star = run_network_trial(\n",
    "                    X0_frac=float(X0),\n",
    "                    ratio=float(ratio),\n",
    "                    I0=float(I0),\n",
    "                    beta_I=float(beta_I),\n",
    "                    b=float(b),\n",
    "                    g_I=float(g_I),\n",
    "                    T=int(T),\n",
    "                    network_type=str(network_type),\n",
    "                    n_nodes=int(n_nodes),\n",
    "                    p=float(p),\n",
    "                    m=int(m),\n",
    "                    seed=int(seed),\n",
    "                    collect=False,\n",
    "                    strategy_choice_func=\"logit\",\n",
    "                    tau=float(tau),\n",
    "                )\n",
    "                finals.append(x_star)\n",
    "            H[i, j] = float(np.mean(finals))\n",
    "\n",
    "    return np.asarray(X0_values, dtype=float), np.asarray(I0_values, dtype=float), H\n",
    "\n",
    "\n",
    "def plot_heatmap_X0_I0(X0_vals, I0_vals, H, title=\"Final adoption X* vs (X0, I0)\"):\n",
    "    \"\"\"Plot heatmap with X0 on x-axis, I0 on y-axis.\"\"\"\n",
    "    plt.figure(figsize=(7, 4.5))\n",
    "    im = plt.imshow(\n",
    "        H,\n",
    "        origin=\"lower\",\n",
    "        extent=[X0_vals[0], X0_vals[-1], I0_vals[0], I0_vals[-1]],\n",
    "        aspect=\"auto\",\n",
    "        vmin=0.0,\n",
    "        vmax=1.0,\n",
    "        cmap=\"plasma\",\n",
    "    )\n",
    "    plt.colorbar(im, label=\"Final adoption X*\")\n",
    "    plt.xlabel(\"X0 (initial adoption)\")\n",
    "    plt.ylabel(\"I0 (initial infrastructure)\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Change this for faster running\n",
    "X0_vals = np.linspace(0.0, 1.0, 11)\n",
    "I0_vals = np.linspace(0.0, 1.0, 11)\n",
    "\n",
    "X0v, I0v, H = heatmap_Xstar_X0_I0(\n",
    "    X0_vals,\n",
    "    I0_vals,\n",
    "    ratio=2.3,\n",
    "    beta_I=2.0,\n",
    "    b=1.0,\n",
    "    g_I=0.10,\n",
    "    T=200,\n",
    "    batch_size=5,\n",
    "    network_type=\"BA\",  # change to \"random\" or \"SW\" if you want\n",
    "    n_nodes=200,\n",
    "    p=0.05,\n",
    "    m=2,\n",
    "    tau=1.0\n",
    ")\n",
    "\n",
    "plot_heatmap_X0_I0(X0v, I0v, H, title=\"BA: Final adoption X* vs (X0, I0)\")\n",
    "\n",
    "\n",
    "print(\"Min X*:\", float(H.min()), \"Max X*:\", float(H.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4038c060",
   "metadata": {},
   "source": [
    "Phase plot trajectory: I(t) vs X(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5bd6454b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "set_initial_adopters() got an unexpected keyword argument 'method'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     19\u001b[39m     plt.show()\n\u001b[32m     21\u001b[39m scenario = \u001b[38;5;28mdict\u001b[39m(ratio=\u001b[32m2.3\u001b[39m, beta_I=\u001b[32m2.0\u001b[39m, b=\u001b[32m1.0\u001b[39m, g_I=\u001b[32m0.10\u001b[39m, I0=\u001b[32m0.05\u001b[39m,\n\u001b[32m     22\u001b[39m                 network_type=\u001b[33m\"\u001b[39m\u001b[33mBA\u001b[39m\u001b[33m\"\u001b[39m, n_nodes=\u001b[32m300\u001b[39m, m=\u001b[32m2\u001b[39m, p=\u001b[32m0.05\u001b[39m,\n\u001b[32m     23\u001b[39m                 X0_frac=\u001b[32m0.10\u001b[39m, init_method=\u001b[33m\"\u001b[39m\u001b[33mrandom\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m X, I, _ = \u001b[43mrun_timeseries_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m250\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscenario_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscenario\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m                               \u001b[49m\u001b[43mstrategy_choice_func\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m plot_phase_trajectory_from_trial(X, I, title=\u001b[33m\"\u001b[39m\u001b[33mBA, low X0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m scenario[\u001b[33m\"\u001b[39m\u001b[33mX0_frac\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[32m0.50\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mrun_timeseries_trial\u001b[39m\u001b[34m(T, scenario_kwargs, seed, policy, strategy_choice_func, tau)\u001b[39m\n\u001b[32m     37\u001b[39m model = EVStagHuntModel(\n\u001b[32m     38\u001b[39m     a0=a0_for_model,\n\u001b[32m     39\u001b[39m     beta_I=scenario[\u001b[33m\"\u001b[39m\u001b[33mbeta_I\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m     tau=tau,\n\u001b[32m     51\u001b[39m )\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m scenario.get(\u001b[33m\"\u001b[39m\u001b[33mX0_frac\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0.0\u001b[39m) > \u001b[32m0.0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     \u001b[43mset_initial_adopters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscenario\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX0_frac\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscenario\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minit_method\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrandom\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(T):\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m policy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mTypeError\u001b[39m: set_initial_adopters() got an unexpected keyword argument 'method'"
     ]
    }
   ],
   "source": [
    "def plot_phase_trajectory_from_trial(X, I, title=\"Phase trajectory (I vs X)\"):\n",
    "    t = np.arange(len(X))\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(11, 4), constrained_layout=True)\n",
    "\n",
    "    axes[0].plot(t, X, label=\"X(t)\")\n",
    "    axes[0].plot(t, I, label=\"I(t)\")\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    axes[0].set_xlabel(\"Time\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].plot(X, I)\n",
    "    axes[1].set_xlim(0, 1)\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    axes[1].set_xlabel(\"X(t)\")\n",
    "    axes[1].set_ylabel(\"I(t)\")\n",
    "    axes[1].set_title(\"Phase plane\")\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "scenario = dict(ratio=2.3, beta_I=2.0, b=1.0, g_I=0.10, I0=0.05,\n",
    "                network_type=\"BA\", n_nodes=300, m=2, p=0.05,\n",
    "                X0_frac=0.10, init_method=\"random\")\n",
    "X, I, _ = run_timeseries_trial(T=250, scenario_kwargs=scenario, seed=1,\n",
    "                               strategy_choice_func=\"logit\", tau=1.0)\n",
    "plot_phase_trajectory_from_trial(X, I, title=\"BA, low X0\")\n",
    "\n",
    "scenario[\"X0_frac\"] = 0.50\n",
    "X2, I2, _ = run_timeseries_trial(T=250, scenario_kwargs=scenario, seed=1,\n",
    "                                 strategy_choice_func=\"logit\", tau=1.0)\n",
    "plot_phase_trajectory_from_trial(X2, I2, title=\"BA, high X0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e69d617",
   "metadata": {},
   "source": [
    "βI sensitivity curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "975d832a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 650x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def beta_sensitivity_curve(\n",
    "    beta_values, *, X0_frac=0.30, I0=0.05, ratio=2.3,\n",
    "    b=1.0, g_I=0.10, T=250, batch_size=10,\n",
    "    network_type=\"BA\", n_nodes=225, p=0.05, m=2, tau=1.0\n",
    "):\n",
    "    rows = []\n",
    "    for beta_I in beta_values:\n",
    "        finals = []\n",
    "        for _ in range(batch_size):\n",
    "            seed = np.random.randint(0, 2**31 - 1)\n",
    "            finals.append(run_network_trial(\n",
    "                X0_frac=X0_frac, ratio=ratio, I0=I0,\n",
    "                beta_I=beta_I, b=b, g_I=g_I, T=T,\n",
    "                network_type=network_type, n_nodes=n_nodes, p=p, m=m,\n",
    "                seed=seed, collect=False, strategy_choice_func=\"logit\", tau=tau\n",
    "            ))\n",
    "        rows.append((beta_I, float(np.mean(finals))))\n",
    "    return pd.DataFrame(rows, columns=[\"beta_I\", \"X_star_mean\"])\n",
    "\n",
    "df_beta = beta_sensitivity_curve(beta_values=[0.5, 1.0, 2.0, 3.0, 4.0])\n",
    "plt.figure(figsize=(6.5, 4))\n",
    "plt.plot(df_beta[\"beta_I\"], df_beta[\"X_star_mean\"], marker=\"o\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel(\"βI\")\n",
    "plt.ylabel(\"Mean final adoption X*\")\n",
    "plt.title(\"Sensitivity to infrastructure feedback βI\")\n",
    "plt.grid(alpha=0.25)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b169b7f9",
   "metadata": {},
   "source": [
    "BA vs ER vs SW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5625d467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_threshold(X_series, thresh=0.9):\n",
    "    X_series = np.asarray(X_series)\n",
    "    idx = np.where(X_series >= thresh)[0]\n",
    "    return int(idx[0]) if len(idx) else None\n",
    "\n",
    "def largest_ev_cluster_fraction(model) -> float:\n",
    "    \"\"\"Largest connected component size among EV adopters at the end.\"\"\"\n",
    "    ev_nodes = []\n",
    "    for node in model.G.nodes:\n",
    "        agents = model.grid.get_cell_list_contents([node])\n",
    "        if agents and agents[0].strategy == \"C\":\n",
    "            ev_nodes.append(node)\n",
    "    if len(ev_nodes) == 0:\n",
    "        return 0.0\n",
    "    sub = model.G.subgraph(ev_nodes)\n",
    "    comps = list(nx.connected_components(sub))\n",
    "    if not comps:\n",
    "        return 0.0\n",
    "    return max(len(c) for c in comps) / model.G.number_of_nodes()\n",
    "\n",
    "def run_single_topology_trial(\n",
    "    *,\n",
    "    network_type: str,\n",
    "    X0_frac: float,\n",
    "    ratio: float,\n",
    "    I0: float,\n",
    "    beta_I: float,\n",
    "    b: float,\n",
    "    g_I: float,\n",
    "    T: int,\n",
    "    n_nodes: int,\n",
    "    p: float,\n",
    "    m: int,\n",
    "    seed: int,\n",
    "    tau: float = 1.0,\n",
    "    high_thresh: float = 0.9,\n",
    "):\n",
    "    initial_ev = int(round(X0_frac * n_nodes))\n",
    "    a0 = ratio * b - beta_I * I0\n",
    "\n",
    "    model = EVStagHuntModel(\n",
    "        initial_ev=initial_ev,\n",
    "        a0=a0,\n",
    "        beta_I=beta_I,\n",
    "        b=b,\n",
    "        g_I=g_I,\n",
    "        I0=I0,\n",
    "        seed=seed,\n",
    "        network_type=network_type,\n",
    "        n_nodes=n_nodes,\n",
    "        p=p,\n",
    "        m=m,\n",
    "        collect=True,                 # we want time series\n",
    "        strategy_choice_func=\"logit\",\n",
    "        tau=tau,\n",
    "    )\n",
    "\n",
    "    X_series = []\n",
    "    for _ in range(T):\n",
    "        model.step()\n",
    "        X_series.append(model.get_adoption_fraction())\n",
    "\n",
    "    X_star = float(X_series[-1])\n",
    "    hit_high = 1.0 if X_star >= high_thresh else 0.0\n",
    "    t_hit = time_to_threshold(X_series, thresh=high_thresh)\n",
    "    t_hit_val = -1 if t_hit is None else t_hit\n",
    "    cluster = largest_ev_cluster_fraction(model)\n",
    "\n",
    "    return X_star, hit_high, t_hit_val, cluster\n",
    "\n",
    "def topology_benchmark_df(\n",
    "    network_types=(\"BA\", \"random\", \"SW\"),\n",
    "    *,\n",
    "    X0_frac=0.30,\n",
    "    ratio=2.3,\n",
    "    I0=0.05,\n",
    "    beta_I=2.0,\n",
    "    b=1.0,\n",
    "    g_I=0.10,\n",
    "    T=250,\n",
    "    n_trials=50,\n",
    "    n_nodes=300,\n",
    "    p=0.05,\n",
    "    m=2,\n",
    "    tau=1.0,\n",
    "    high_thresh=0.9,\n",
    "):\n",
    "    rows = []\n",
    "    for net in network_types:\n",
    "        for k in range(n_trials):\n",
    "            seed = 10000 + k\n",
    "            X_star, hit_high, t_hit, cluster = run_single_topology_trial(\n",
    "                network_type=net,\n",
    "                X0_frac=X0_frac,\n",
    "                ratio=ratio,\n",
    "                I0=I0,\n",
    "                beta_I=beta_I,\n",
    "                b=b,\n",
    "                g_I=g_I,\n",
    "                T=T,\n",
    "                n_nodes=n_nodes,\n",
    "                p=p,\n",
    "                m=m,\n",
    "                seed=seed,\n",
    "                tau=tau,\n",
    "                high_thresh=high_thresh,\n",
    "            )\n",
    "            rows.append((net, X_star, hit_high, t_hit, cluster))\n",
    "    return pd.DataFrame(rows, columns=[\"network\", \"X_star\", \"hit_high\", \"t_hit\", \"cluster_frac\"])\n",
    "\n",
    "def plot_topology_metrics(df: pd.DataFrame, high_thresh=0.9):\n",
    "    # Speed: mean time to reach high adoption, ignoring failures (t_hit=-1)\n",
    "    speed = df[df[\"t_hit\"] >= 0].groupby(\"network\")[\"t_hit\"].mean()\n",
    "    # Probability: fraction reaching high equilibrium\n",
    "    prob = df.groupby(\"network\")[\"hit_high\"].mean()\n",
    "    # Clusters: mean largest-cluster fraction\n",
    "    clus = df.groupby(\"network\")[\"cluster_frac\"].mean()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(14, 4), constrained_layout=True)\n",
    "\n",
    "    axes[0].bar(speed.index, speed.values)\n",
    "    axes[0].set_title(f\"Speed (mean time to X≥{high_thresh})\")\n",
    "    axes[0].set_ylabel(\"time steps\")\n",
    "\n",
    "    axes[1].bar(prob.index, prob.values)\n",
    "    axes[1].set_title(\"Probability of high adoption\")\n",
    "    axes[1].set_ylabel(\"P(X* high)\")\n",
    "    axes[1].set_ylim(0, 1)\n",
    "\n",
    "    axes[2].bar(clus.index, clus.values)\n",
    "    axes[2].set_title(\"Cluster formation\")\n",
    "    axes[2].set_ylabel(\"Largest EV cluster / N\")\n",
    "    axes[2].set_ylim(0, 1)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# ---- RUN IT ----\n",
    "df_top = topology_benchmark_df(\n",
    "    network_types=(\"BA\", \"random\", \"SW\"),\n",
    "    X0_frac=0.30,\n",
    "    ratio=2.3,\n",
    "    I0=0.05,\n",
    "    beta_I=2.0,\n",
    "    T=250,\n",
    "    n_trials=40,      # increase if you want smoother stats\n",
    "    n_nodes=300,\n",
    "    p=0.05,\n",
    "    m=2,\n",
    "    tau=1.0\n",
    ")\n",
    "\n",
    "display(df_top.groupby(\"network\")[[\"X_star\", \"hit_high\", \"t_hit\", \"cluster_frac\"]].mean())\n",
    "plot_topology_metrics(df_top, high_thresh=0.9)\n",
    "\n",
    "def tipping_sensitivity_by_network(\n",
    "    network_types=(\"BA\",\"random\",\"SW\"),\n",
    "    X0_values=None,\n",
    "    *,\n",
    "    ratio=2.3,\n",
    "    I0=0.05,\n",
    "    beta_I=2.0,\n",
    "    b=1.0,\n",
    "    g_I=0.10,\n",
    "    T=250,\n",
    "    n_trials=25,\n",
    "    n_nodes=300,\n",
    "    p=0.05,\n",
    "    m=2,\n",
    "    tau=1.0,\n",
    "    high_thresh=0.9,\n",
    "):\n",
    "    if X0_values is None:\n",
    "        X0_values = np.linspace(0.0, 1.0, 11)\n",
    "\n",
    "    rows = []\n",
    "    for net in network_types:\n",
    "        for X0 in X0_values:\n",
    "            hits = []\n",
    "            for k in range(n_trials):\n",
    "                seed = np.random.randint(0, 2**31 - 1)\n",
    "                X_star, hit_high, t_hit, cluster = run_single_topology_trial(\n",
    "                    network_type=net,\n",
    "                    X0_frac=float(X0),\n",
    "                    ratio=ratio,\n",
    "                    I0=I0,\n",
    "                    beta_I=beta_I,\n",
    "                    b=b,\n",
    "                    g_I=g_I,\n",
    "                    T=T,\n",
    "                    n_nodes=n_nodes,\n",
    "                    p=p,\n",
    "                    m=m,\n",
    "                    seed=seed,\n",
    "                    tau=tau,\n",
    "                    high_thresh=high_thresh,\n",
    "                )\n",
    "                hits.append(hit_high)\n",
    "            rows.append((net, float(X0), float(np.mean(hits))))\n",
    "    return pd.DataFrame(rows, columns=[\"network\",\"X0\",\"P_high\"])\n",
    "\n",
    "df_tip = tipping_sensitivity_by_network(\n",
    "    network_types=(\"BA\",\"random\",\"SW\"),\n",
    "    X0_values=np.linspace(0,1,11),\n",
    "    n_trials=20,\n",
    "    T=250,\n",
    "    n_nodes=300,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(7,4.5))\n",
    "for net, g in df_tip.groupby(\"network\"):\n",
    "    plt.plot(g[\"X0\"], g[\"P_high\"], marker=\"o\", label=net)\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel(\"X0 (initial adoption)\")\n",
    "plt.ylabel(\"P(high-adoption equilibrium)\")\n",
    "plt.title(\"Tipping sensitivity by topology\")\n",
    "plt.grid(alpha=0.25)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
